apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: fastbi-main-workflow-
spec:
  templates:
    - name: exit-handler
      inputs: {}
      outputs: {}
      metadata: {}
      steps:
        - - name: check-all-task-status
            template: check-task-status
            arguments:
              parameters:
                - name: workflow_status
                  value: '{{workflow.failures}}'
                - name: argo_main_address
                  value: '{{workflow.parameters.argo_main_address}}'
          - name: k8s-pvc-cleanup
            template: k8s-pvc-cleanup
            arguments: {}
            when: >-
              {{workflow.status}} != Succeeded
    - name: main
      inputs:
        parameters:
          - name: repo_url
          - name: revision
          - name: commit_short_sha
          - name: commit_before_sha
          - name: git_username
          - name: git_token
      outputs: {}
      metadata: {}
      steps:
        - - name: checkout
            template: checkout
            arguments:
              parameters:
                - name: repo_url
                  value: '{{inputs.parameters.repo_url}}'
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: git_username
                  value: '{{inputs.parameters.git_username}}'
                - name: git_token
                  value: '{{inputs.parameters.git_token}}'
        - - name: detect-changes
            template: detect-changes
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: commit_before_sha
                  value: '{{inputs.parameters.commit_before_sha}}'
        - - name: data-orchestration-variables-import
            template: data-orchestration-variables-import
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: repo_url
                  value: '{{inputs.parameters.repo_url}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
              artifacts:
                - name: repo-artifact
                  from: '{{steps.checkout.outputs.artifacts.repo-artifact}}'
                - name: changed-files-artifact
                  from: '{{steps.detect-changes.outputs.artifacts.changed-files-artifact}}'
        - - name: compile-dbt-manifest
            template: compile-dbt-manifest
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
              artifacts:
                - name: repo-artifact
                  from: '{{steps.checkout.outputs.artifacts.repo-artifact}}'
                - name: changed-files-artifact
                  from: '{{steps.detect-changes.outputs.artifacts.changed-files-artifact}}'
          - name: compile-dag
            template: compile-dag
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
              artifacts:
                - name: compiled-repo-artifact
                  from: '{{steps.data-orchestration-variables-import.outputs.artifacts.compiled-repo-artifact}}'
                - name: changed-files-artifact
                  from: '{{steps.detect-changes.outputs.artifacts.changed-files-artifact}}'
                - name: airflow_secret_key
                  from: '{{steps.data-orchestration-variables-import.outputs.artifacts.airflow_secret_key}}'
                - name: airflow_dag_id
                  from: '{{steps.data-orchestration-variables-import.outputs.artifacts.airflow_dag_id}}'
        - - name: refresh-dbt-incremental-modules
            template: refresh-dbt-incremental-modules
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
                - name: commit_before_sha
                  value: '{{inputs.parameters.commit_before_sha}}'
              artifacts:
                - name: repo-artifact
                  from: '{{steps.checkout.outputs.artifacts.repo-artifact}}'
                - name: changed-files-artifact
                  from: '{{steps.detect-changes.outputs.artifacts.changed-files-artifact}}'
        - - name: data-model-da-dependencies-import
            template: data-model-da-dependencies-import
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
                - name: repo_url
                  value: '{{inputs.parameters.repo_url}}'
              artifacts:
                - name: repo-artifact
                  from: '{{steps.checkout.outputs.artifacts.repo-artifact}}'
                - name: changed-files-artifact
                  from: '{{steps.detect-changes.outputs.artifacts.changed-files-artifact}}'
          - name: data-model-data-governance-metadata-import
            template: data-model-data-governance-metadata-import
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
              artifacts:
                - name: repo-artifact
                  from: '{{steps.checkout.outputs.artifacts.repo-artifact}}'
                - name: changed-files-artifact
                  from: '{{steps.detect-changes.outputs.artifacts.changed-files-artifact}}'
          - name: data-model-data-catalog-deployment
            template: data-model-data-catalog-deployment
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
                - name: repo_url
                  value: '{{inputs.parameters.repo_url}}'
              artifacts:
                - name: repo-artifact
                  from: '{{steps.checkout.outputs.artifacts.repo-artifact}}'
                - name: changed-files-artifact
                  from: '{{steps.detect-changes.outputs.artifacts.changed-files-artifact}}'
          - name: data-model-data-quality-deployment
            template: data-model-data-quality-deployment
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
                - name: repo_url
                  value: '{{inputs.parameters.repo_url}}'
              artifacts:
                - name: repo-artifact
                  from: '{{steps.checkout.outputs.artifacts.repo-artifact}}'
                - name: changed-files-artifact
                  from: '{{steps.detect-changes.outputs.artifacts.changed-files-artifact}}'
        - - name: k8s-pvc-cleanup
            template: k8s-pvc-cleanup
            arguments: {}
            when: >-
              {{workflow.status}} != Succeeded || {{workflow.status}} ==
              Succeeded
    - name: checkout
      inputs:
        parameters:
          - name: repo_url
          - name: revision
          - name: git_username
          - name: git_token
      outputs:
        artifacts:
          - name: repo-artifact
            path: /workdir/src
      metadata: {}
      container:
        name: ''
        image: alpine/git
        command:
          - sh
          - '-c'
        args:
          - >
            #!/bin/sh


            # ===========================================
            # 🚀 Repository Checkout Script
            # ===========================================


            echo "==========================================="
            echo "🚀 [INFO] Starting repository checkout process"
            echo "==========================================="

            # ===========================================
            # 📋 [CONFIG] System Setup
            # ===========================================

            echo "⚙️ [PROCESS] Installing required packages..."

            apk add --no-cache jq || { echo "❌ [ERROR] Failed to install jq package"; exit 1; }

            # ===========================================
            # 🔍 [CHECK] Environment Validation
            # ===========================================

            echo "🔍 [CHECK] Validating repository configuration..."

            REPO_URL="{{inputs.parameters.repo_url}}"
            REPO_URL_NO_PROTOCOL="${REPO_URL#https://}"

            echo "📋 [CONFIG] Repository URL configured: ${REPO_URL%/*}/[REPO_NAME]"

            # ===========================================
            # ⚙️ [PROCESS] Git Configuration
            # ===========================================

            echo "⚙️ [PROCESS] Configuring git settings..."

            git config --global advice.detachedHead false || { echo "❌ [ERROR] Failed to configure global git settings"; exit 1; }

            # ===========================================
            # 🚀 [INFO] Repository Operations
            # ===========================================

            echo "🚀 [INFO] Cloning repository..."

            git clone https://{{inputs.parameters.git_username}}:{{inputs.parameters.git_token}}@${REPO_URL_NO_PROTOCOL} /workdir/src || { echo "❌ [ERROR] Failed to clone repository"; exit 1; }

            echo "⚙️ [PROCESS] Navigating to repository directory..."

            cd /workdir/src || { echo "❌ [ERROR] Failed to navigate to repository directory"; exit 1; }

            echo "⚙️ [PROCESS] Configuring local git settings..."

            git config advice.detachedHead false || { echo "❌ [ERROR] Failed to configure local git settings"; exit 1; }

            echo "⚙️ [PROCESS] Checking out specified revision..."

            git checkout {{inputs.parameters.revision}} || { echo "❌ [ERROR] Failed to checkout revision {{inputs.parameters.revision}}"; exit 1; }

            # ===========================================
            # ✅ [SUCCESS] Completion
            # ===========================================

            echo "==========================================="

            echo "✅ [SUCCESS] Repository cloned and checked out successfully"
            
            echo "📋 [INFO] Working directory: /workdir/src"
            
            echo "📋 [INFO] Revision: {{inputs.parameters.revision}}"
            
            echo "==========================================="
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
    - name: detect-changes
      inputs:
        parameters:
          - name: revision
          - name: commit_before_sha

      outputs:
        artifacts:
          - name: changed-files-artifact
            path: /workdir/changed_folders.txt
      metadata: {}
      container:
        name: ''
        image: alpine/git
        command:
          - sh
          - '-c'
        args:
          - |
            #!/bin/sh

            # ===========================================
            # Git Change Detection Script
            # Detects changed folders between Git branches
            # ===========================================


            echo "🚀 [INFO] Starting Git change detection process"
            echo "=========================================="

            # ===========================================
            # Environment Setup
            # ===========================================

            echo "📋 [CONFIG] Setting up working directory"
            cd /workdir/src || { echo "❌ [ERROR] Failed to change to /workdir/src directory"; exit 1; }

            # ===========================================
            # Git Operations
            # ===========================================

            echo "⚙️ [PROCESS] Fetching latest changes from remote repository"
            git fetch --all || { echo "❌ [ERROR] Failed to fetch from remote repository"; exit 1; }

            echo "⚙️ [PROCESS] Analyzing changes between branches"
            echo "🔍 [CHECK] Comparing {{inputs.parameters.commit_before_sha}} with {{inputs.parameters.revision}}"

            git diff-tree --no-commit-id --name-only -r {{inputs.parameters.commit_before_sha}} {{inputs.parameters.revision}} \
              | cut -d'/' -f1 \
              | sort \
              | uniq > /workdir/changed_folders.txt || { 
                echo "❌ [ERROR] Failed to identify changed folders"
                exit 1
              }

            # ===========================================
            # Filter Hidden Folders
            # ===========================================

            echo "⚙️ [PROCESS] Filtering out hidden folders and files"

            > /workdir/temp.txt
            while read -r line; do
              # Remove leading whitespace (if any)
              line=$(echo "$line" | sed 's/^[ \t]*//')
              # Get the first character
              first_char=$(echo "$line" | cut -c1)
              if [ "$first_char" != "." ]; then
                echo "$line" >> /workdir/temp.txt
              fi
            done < /workdir/changed_folders.txt

            mv /workdir/temp.txt /workdir/changed_folders.txt

            # ===========================================
            # Fallback Configuration
            # ===========================================

            if [ ! -s /workdir/changed_folders.txt ]; then
              echo "⚠️ [WARNING] No changed folders detected, using default fallback"
              echo "README.md" > /workdir/changed_folders.txt
            fi

            # ===========================================
            # Results Summary
            # ===========================================

            echo "✅ [SUCCESS] Change detection completed successfully"
            echo "📋 Folders changed: $(cat /workdir/changed_folders.txt)"
            echo "📋 [CONFIG] Changed folders identified:"
            echo "=========================================="

            # Display count of changed folders instead of full content for security
            folder_count=$(wc -l < /workdir/changed_folders.txt)
            echo "🔍 [CHECK] Total changed folders detected: $folder_count"

            echo "✅ [SUCCESS] Git change detection process completed"
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
    - name: data-orchestration-variables-import
      inputs:
        parameters:
          - name: revision
          - name: repo_url
          - name: commit_short_sha
        artifacts:
          - name: repo-artifact
            path: /workdir/src
          - name: changed-files-artifact
            path: /workdir/changed_folders.txt
      outputs:
        artifacts:
          - name: airflow_secret_key
            path: /workdir/airflow_secrets.yaml
          - name: airflow_dag_id
            path: /workdir/airflow_dags.yaml
          - name: compiled-repo-artifact
            path: /workdir/src
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_orchestrator_core_image_name}}:{{workflow.parameters.data_orchestrator_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash

            echo "==========================================="
            echo "🚀 [INFO] DBT Project Data Orchestration Variables Import"
            echo "📋 [CONFIG] Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="

            cd /workdir/src

            # Create YAML files for the next steps
            echo "⚙️ [PROCESS] Initializing YAML configuration files..."
            cat > /workdir/airflow_secrets.yaml << EOF
            secrets:
                platform: {}
            EOF

            cat > /workdir/airflow_dags.yaml << EOF
            dags: []
            EOF

            echo "✅ [SUCCESS] YAML configuration files initialized"

            # Function to create and upload dynamic DAG variables
            create_dynamic_dag_variables() {
                local base_secret_file="$1"
                local dynamic_config_file="dynamic_dag_config.yml"
                
                echo "⚙️ [PROCESS] Processing dynamic DAG configurations..."
                
                if [ ! -f "$dynamic_config_file" ]; then
                    echo "❌ [ERROR] Dynamic config file not found: $dynamic_config_file"
                    return 1
                fi

                # Get all DAG configurations
                mapfile -t dag_name_list < <(yq -r '(.dags | keys)[]' "$dynamic_config_file")

                if [ ${#dag_name_list[@]} -eq 0 ]; then
                    echo "❌ [ERROR] No DAG configurations found in '$dynamic_config_file'"
                    return 1
                fi

                echo "📋 [CONFIG] Found ${#dag_name_list[@]} DAG configurations to process"

                # Initialize airflow_secrets.yaml if it doesn't exist
                if [ ! -f "/workdir/airflow_secrets.yaml" ]; then
                    echo "⚙️ [PROCESS] Initializing airflow_secrets.yaml..."
                    echo "secrets:" > /workdir/airflow_secrets.yaml
                    echo "  platform:" >> /workdir/airflow_secrets.yaml
                    echo "    airflow: []" >> /workdir/airflow_secrets.yaml
                fi

                # Process each DAG
                for dag_name_from_list in "${dag_name_list[@]}"; do
                    if [ -z "$dag_name_from_list" ]; then
                        echo "⚠️ [WARNING] Skipping empty DAG name found in configuration"
                        continue
                    fi

                    echo "⚙️ [PROCESS] Processing DAG: $dag_name_from_list"
                    
                    # 1. Get the original base key from base_file_path
                    local original_base_key=$(yq -r 'keys | .[0]' "$base_secret_file")
                    if [ -z "$original_base_key" ] || [ "$original_base_key" == "null" ]; then
                        echo "❌ [ERROR] Could not determine original base key from $base_secret_file"
                        continue
                    fi

                    # 2. Construct the new root key with DAG name
                    local new_root_key="${original_base_key}_${dag_name_from_list^^}"

                    # 3. Create the initial structure in final_output_for_dag by assigning new key and deleting old.
                    local temp_file="${base_secret_file%.*}_${dag_name_from_list}.yml"
                    yq ".\"${new_root_key}\" = .\"${original_base_key}\" | del(.\"${original_base_key}\")" "$base_secret_file" > "$temp_file" || {
                        echo "❌ [ERROR] Failed to create initial output file for DAG: $dag_name_from_list"
                        continue
                    }

                    # 4. Get the specific override configurations for the current DAG from dynamic_config_file_path
                    local overrides_for_dag_path=".dags.${dag_name_from_list}"
                    local temp_dag_overrides_file="temp_overrides_${dag_name_from_list}.yml"
                    
                    # Check if overrides exist for this DAG. Use yq -e to exit with error if path not found.
                    if ! yq -e "${overrides_for_dag_path}" "$dynamic_config_file" > "$temp_dag_overrides_file" 2>/dev/null || [ ! -s "$temp_dag_overrides_file" ]; then
                        echo "⚠️ [WARNING] No overrides found for DAG: $dag_name_from_list - using base values"
                        rm -f "$temp_dag_overrides_file"
                    else
                        # 5. Merge these overrides into the new_root_key structure in final_output_for_dag
                        yq -y ".\"${new_root_key}\" = (.\"${new_root_key}\" * $(cat "$temp_dag_overrides_file"))" "$temp_file" > "${temp_file}.tmp" || {
                            echo "❌ [ERROR] Failed to merge overrides for DAG: $dag_name_from_list"
                            rm -f "$temp_dag_overrides_file"
                            continue
                        }
                        mv "${temp_file}.tmp" "$temp_file"
                        rm -f "$temp_dag_overrides_file" 
                    fi

                    # 6. Specifically update the DAG_ID inside the merged structure
                    local current_dag_id_in_merged=$(yq -r ".\"${new_root_key}\".DAG_ID" "$temp_file")
                    if [ -z "$current_dag_id_in_merged" ] || [ "$current_dag_id_in_merged" == "null" ]; then
                        echo "⚠️ [WARNING] DAG_ID not found in merged configuration for: $dag_name_from_list"
                    else
                        yq -y ".\"${new_root_key}\".DAG_ID = \"${current_dag_id_in_merged}\"" "$temp_file" > "${temp_file}.tmp" && mv "${temp_file}.tmp" "$temp_file"
                        echo "✅ [SUCCESS] Configuration created for DAG: $dag_name_from_list"
                    fi

                    # Add DAG configuration to YAML file (moved here after temp_file is created)
                    local dag_config="{\"project\": \"$DBT_PROJECT_NAME\", \"dag_id\": \"$current_dag_id_in_merged\", \"variables_file\": \"$temp_file\"}"
                    local temp_yaml_file=$(mktemp)
                    yq -y ".dags += [$dag_config]" /workdir/airflow_dags.yaml > "$temp_yaml_file"
                    mv "$temp_yaml_file" /workdir/airflow_dags.yaml

                    # Add secret key to YAML file
                    local temp_secrets_file=$(mktemp)
                    yq -y ".secrets.platform.\"${PLATFORM,,}\" = (.secrets.platform.\"${PLATFORM,,}\" // []) + [\"${new_root_key}\"]" /workdir/airflow_secrets.yaml > "$temp_secrets_file"
                    mv "$temp_secrets_file" /workdir/airflow_secrets.yaml

                    # Airflow specific part - import variables
                    export AIRFLOW_SECRET_FILE_NAME="$temp_file"
                    export PLATFORM=$(yq -r ".\"${new_root_key}\".PLATFORM" "$temp_file")
                    export NAMESPACE=$(yq -r ".\"${new_root_key}\".NAMESPACE" "$temp_file")
                    export OPERATOR=$(yq -r ".\"${new_root_key}\".OPERATOR" "$temp_file")
                    
                    # Import variables for this DAG
                    echo "⚙️ [PROCESS] Importing variables for DAG: $dag_name_from_list"
                    python /usr/app/tsb-data-orchestrator-core/create_variables_airflow_bauth.op.py 2>&1 | tee -a /workdir/create_variables_airflow_bauth.log || true
                done
                
                # Restore original AIRFLOW_SECRET_FILE_NAME
                export AIRFLOW_SECRET_FILE_NAME="$base_secret_file"
                echo "✅ [SUCCESS] Dynamic DAG processing completed"
            }

            # Scan changed folders for DBT compilation
            echo "==========================================="
            echo "🔍 [CHECK] Processing changed folders..."
            echo "==========================================="

            while read folder; do
                if [ -d "$folder" ]; then
                    echo "⚙️ [PROCESS] Collecting variables from: $folder"
                    cd "$folder"
                    
                    # Set warehouse credentials
                    echo "🔍 [CHECK] Validating data warehouse credentials..."
                    
                    if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                        if [[ -z "$GCP_SA_SECRET" ]]; then
                            echo "❌ [ERROR] Google BigQuery credentials missing: Service Account JSON not provided"
                            exit 1
                        else
                            if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                                echo "⚙️ [PROCESS] Configuring Google Cloud authentication..."
                                echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                                gcloud auth activate-service-account --key-file /secrets/sa.json
                                gcloud config set project $GCP_PROJECT_NAME
                                export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                                echo "✅ [SUCCESS] Google Cloud authentication configured"
                            fi
                        fi
                    fi
                    
                    if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                        if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                            echo "❌ [ERROR] Snowflake credentials missing: Private Key and Passphrase not provided"
                            exit 1
                        else
                            if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                                echo "⚙️ [PROCESS] Configuring Snowflake authentication..."
                                mkdir -p /snowsql/secrets/
                                echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8
                                chmod 600 /snowsql/secrets/rsa_key.p8
                                export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                                echo "✅ [SUCCESS] Snowflake authentication configured"
                            fi
                        fi
                    fi
                    
                    if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                        if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                            echo "❌ [ERROR] Redshift credentials missing: required parameters not provided"
                            exit 1
                        fi
                    fi
                    
                    if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                        if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                            echo "❌ [ERROR] Fabric credentials missing: required parameters not provided"
                            exit 1
                        fi
                    fi
                    
                    echo "✅ [SUCCESS] Data warehouse credentials validated"
                    
                    echo "🔍 [CHECK] Validating required environment variables..."
                    if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                        echo "❌ [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                        exit 1
                    else  
                        echo "✅ [SUCCESS] All required variables are defined"
                    fi
                    
                    echo "⚙️ [PROCESS] Importing required Airflow Secrets from: $AIRFLOW_SECRET_FILE_NAME"
                    
                    # Update GITLINK_SECRET
                    echo "📋 [CONFIG] Updating GITLINK_SECRET..."
                    gitlink_secret=$(grep 'GITLINK_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                    if [ -z "$gitlink_secret" ]; then
                        echo "⚙️ [PROCESS] Adding GITLINK_SECRET variable..."
                        domain_and_path=$(echo {{ inputs.parameters.repo_url }} | sed -e 's/https:\/\///')
                        domain=$(echo ${domain_and_path} | cut -d "/" -f 1)
                        path=$(echo ${domain_and_path} | cut -d "/" -f 2-)
                        git_url="https://${PRIVATE_ACCESS_NAME}:${PRIVATE_ACCESS_TOKEN}@${domain}/${path}.git"
                        sed -i '$ a\  GITLINK_SECRET: '''${git_url}'''' $AIRFLOW_SECRET_FILE_NAME
                        echo "✅ [SUCCESS] GITLINK_SECRET added"
                    else
                        echo "✅ [SUCCESS] GITLINK_SECRET already exists"
                    fi
                    
                    # Update DBT_WORKLOAD_API_CONNECTION_ID for API operator
                    operator=$(grep 'OPERATOR:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                    dag_id=$(grep 'DAG_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                    if [[ "$operator" == "api" ]]; then
                        echo "📋 [CONFIG] Updating DBT_WORKLOAD_API_CONNECTION_ID..."
                        dbt_workload_connection_id=$(grep 'DBT_WORKLOAD_API_CONNECTION_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                        if [ -z "$dbt_workload_connection_id" ]; then
                            echo "⚙️ [PROCESS] Adding DBT_WORKLOAD_API_CONNECTION_ID variable..."
                            sed -i '$ a\  DBT_WORKLOAD_API_CONNECTION_ID: '''${dag_id,,}'''' $AIRFLOW_SECRET_FILE_NAME
                        else
                            sed -i 's/DBT_WORKLOAD_API_CONNECTION_ID.*/DBT_WORKLOAD_API_CONNECTION_ID: '''${dag_id,,}'''/' $AIRFLOW_SECRET_FILE_NAME
                        fi
                        echo "✅ [SUCCESS] DBT_WORKLOAD_API_CONNECTION_ID updated"
                    fi
                    
                    # Update SECRET_PACKAGE_REPO_TOKEN_NAME
                    echo "📋 [CONFIG] Updating SECRET_PACKAGE_REPO_TOKEN_NAME..."
                    package_token_name=$(grep 'SECRET_PACKAGE_REPO_TOKEN_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                    if [ -z "$package_token_name" ]; then
                        echo "⚙️ [PROCESS] Adding SECRET_PACKAGE_REPO_TOKEN_NAME variable..."
                        sed -i '$ a\  SECRET_PACKAGE_REPO_TOKEN_NAME: '''${SECRET_PACKAGE_REPO_TOKEN_NAME}'''' $AIRFLOW_SECRET_FILE_NAME
                    else
                        sed -i 's/SECRET_PACKAGE_REPO_TOKEN_NAME.*/SECRET_PACKAGE_REPO_TOKEN_NAME: '''${SECRET_PACKAGE_REPO_TOKEN_NAME}'''/' $AIRFLOW_SECRET_FILE_NAME
                    fi
                    echo "✅ [SUCCESS] SECRET_PACKAGE_REPO_TOKEN_NAME updated"
                    
                    # Update SECRET_DBT_PACKAGE_REPO_TOKEN
                    echo "📋 [CONFIG] Updating SECRET_DBT_PACKAGE_REPO_TOKEN..."
                    package_token=$(grep 'SECRET_DBT_PACKAGE_REPO_TOKEN:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                    if [ -z "$package_token" ]; then
                        echo "⚙️ [PROCESS] Adding SECRET_DBT_PACKAGE_REPO_TOKEN variable..."
                        sed -i '$ a\  SECRET_DBT_PACKAGE_REPO_TOKEN: '''${SECRET_DBT_PACKAGE_REPO_TOKEN}'''' $AIRFLOW_SECRET_FILE_NAME
                    else
                        sed -i 's/SECRET_DBT_PACKAGE_REPO_TOKEN.*/SECRET_DBT_PACKAGE_REPO_TOKEN: '''${SECRET_DBT_PACKAGE_REPO_TOKEN}'''/' $AIRFLOW_SECRET_FILE_NAME
                    fi
                    echo "✅ [SUCCESS] SECRET_DBT_PACKAGE_REPO_TOKEN updated"
                    
                    # Add Datahub Variables if enabled
                    if [ "$(grep 'DATAHUB_ENABLED:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" | tr -cd '[:alpha:]')" == "true" ] || [ "$(grep 'DATAHUB_ENABLED:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" | tr -cd '[:alpha:]')" == "True" ]; then
                        echo "📋 [CONFIG] Datahub is enabled - updating Datahub variables..."
                        
                        # Update DATAHUB_GMS_TOKEN
                        datahub_gms_token=$(grep 'DATAHUB_GMS_TOKEN:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                        if [ -z "$datahub_gms_token" ]; then
                            echo "⚙️ [PROCESS] Adding DATAHUB_GMS_TOKEN variable..."
                            sed -i '$ a\  DATAHUB_GMS_TOKEN: '''${DATAHUB_GMS_TOKEN}'''' $AIRFLOW_SECRET_FILE_NAME
                        else
                            sed -i 's/DATAHUB_GMS_TOKEN.*/DATAHUB_GMS_TOKEN: '''${DATAHUB_GMS_TOKEN}'''/' $AIRFLOW_SECRET_FILE_NAME
                        fi
                        
                        # Update DATAHUB_GMS_URL
                        datahub_gms_url=$(grep 'DATAHUB_GMS_URL:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                        if [ -z "$datahub_gms_url" ]; then
                            echo "⚙️ [PROCESS] Adding DATAHUB_GMS_URL variable..."
                            sed -i '$ a\  DATAHUB_GMS_URL: '''${DATAHUB_GMS_URL}'''' $AIRFLOW_SECRET_FILE_NAME
                        else
                            sed -i 's/DATAHUB_GMS_URL.*/DATAHUB_GMS_URL: '''${DATAHUB_GMS_URL}'''/' $AIRFLOW_SECRET_FILE_NAME
                        fi
                        echo "✅ [SUCCESS] Datahub variables updated"
                    fi
                    
                    # Add GKE Operator Dependencies
                    echo "📋 [CONFIG] Updating GKE Operator dependencies..."
                    operator=$(grep 'OPERATOR:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                    if [[ "$operator" == "gke" ]]; then
                        sa_secret=$(grep 'SA_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                        if [ -z "$sa_secret" ]; then
                            echo "⚙️ [PROCESS] Adding SA_SECRET variable for GKE operator..."
                            sed -i '$ a\  SA_SECRET: '''${GCP_SA_SECRET}'''' $AIRFLOW_SECRET_FILE_NAME
                        else
                            sed -i 's/SA_SECRET.*/SA_SECRET: '''${GCP_SA_SECRET}'''/' $AIRFLOW_SECRET_FILE_NAME
                        fi
                        echo "✅ [SUCCESS] GKE operator dependencies updated"
                    fi
                    
                    # Import variables from Airflow Secret yml file
                    echo "⚙️ [PROCESS] Importing variables from Airflow Secret file..."
                    export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" )
                    export DAG_OPERATOR=$(grep 'OPERATOR:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                    export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                    export DBT_REPO_NAME=$(grep 'DBT_PROJECT_DIRECTORY:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                    export GITLINK_SECRET=$(grep 'GITLINK_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                    export SECRET_DBT_PACKAGE_REPO_TOKEN=$(grep 'SECRET_DBT_PACKAGE_REPO_TOKEN:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                    export SECRET_PACKAGE_REPO_TOKEN_NAME=$(grep 'SECRET_PACKAGE_REPO_TOKEN_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                    export DYNAMIC_DAG=$(grep 'DYNAMIC_DAG:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                    echo "✅ [SUCCESS] Variables imported from secret file"

                    touch /workdir/create_variables_airflow_bauth.log

                    echo "🔍 [CHECK] Platform validation..."
                    if [ "${PLATFORM,,}" == "composer" ]; then
                        echo "📋 [CONFIG] Platform: Google Cloud Composer"
                        echo "🔍 [CHECK] Validating Composer environment variables..."
                        if [[ -z $AIRFLOW_URL || -z $AIRFLOW_SECRET_FILE_NAME || -z $PLATFORM || -z $project_id || -z $location || -z $composer_environment_name || -z $service_account_email ]]; then
                            echo "❌ [ERROR] One or more Composer variables are undefined"
                            exit 1
                        else  
                            echo "✅ [SUCCESS] All Composer variables are defined"
                            
                            # Deploy DBT API Service if Operator is API
                            if [[ "$DAG_OPERATOR" == "api" ]]; then
                                echo "⚙️ [PROCESS] Deploying DBT API Service for Composer..."
                                export HTTPS_ENABLED="true"
                                export PHASE_ENVIRONMENT="prod"
                                export WORKER_NUM="10"
                                export MAX_REQUESTS="45"
                                export CPU_REQUEST="2000m"
                                export MEMORY_REQUEST="4Gi"
                                export APP_VERSION="{{inputs.parameters.commit_short_sha}}"
                                
                                /bin/bash -c "/usr/app/tsb-data-orchestrator-core/deploy_dbt_api_service.sh" 2>&1 | tee -a /workdir/deploy_dbt_api_service.log || true
                                
                                # Source the environment file to get the variables
                                source "${DBT_PROJECT_NAME}_${APP_VERSION:-deploy}_env.sh"
                                
                                echo "📋 [CONFIG] Updating DBT_WORKLOAD_API_CONNECTION_ID..."
                                dbt_workload_connection_id=$(grep 'DBT_WORKLOAD_API_CONNECTION_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                                if [ -z "$dbt_workload_connection_id" ]; then
                                    echo "⚙️ [PROCESS] Adding DBT_WORKLOAD_API_CONNECTION_ID variable..."
                                    sed -i "$ a\  DBT_WORKLOAD_API_CONNECTION_ID: '$DBT_WORKLOAD_API_CONNECTION_ID'" $AIRFLOW_SECRET_FILE_NAME
                                else
                                    sed -i "s/DBT_WORKLOAD_API_CONNECTION_ID.*/DBT_WORKLOAD_API_CONNECTION_ID: '$DBT_WORKLOAD_API_CONNECTION_ID'/" $AIRFLOW_SECRET_FILE_NAME
                                fi
                                
                                echo "📋 [CONFIG] Updating DBT_WORKLOAD_API_RELEASE..."
                                dbt_workload_release=$(grep 'DBT_WORKLOAD_API_RELEASE:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                                if [ -z "$dbt_workload_release" ]; then
                                    echo "⚙️ [PROCESS] Adding DBT_WORKLOAD_API_RELEASE variable..."
                                    sed -i "$ a\  DBT_WORKLOAD_API_RELEASE: '$DBT_WORKLOAD_API_RELEASE'" $AIRFLOW_SECRET_FILE_NAME
                                fi
                                echo "✅ [SUCCESS] DBT API Service deployed and configured"
                            fi
                            
                            echo "⚙️ [PROCESS] Importing secrets to Airflow vault..."
                            if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                                echo "📋 [CONFIG] Dynamic DAG is enabled - creating multiple DAG configurations..."
                                create_dynamic_dag_variables "$AIRFLOW_SECRET_FILE_NAME"
                            else
                                echo "📋 [CONFIG] Using standard single DAG configuration..."
                                python /usr/app/tsb-data-orchestrator-core/create_variables_airflow_bauth.op.py 2>&1 | tee -a /workdir/create_variables_airflow_bauth.log || true
                                
                                # Add to YAML files using temporary files
                                dag_config="{
                                    \"project\": \"$DBT_PROJECT_NAME\",
                                    \"dag_id\": \"$dag_id\",
                                    \"variables_file\": \"$AIRFLOW_SECRET_FILE_NAME\"
                                }"
                                temp_file=$(mktemp)
                                yq -y ".dags += [$dag_config]" /workdir/airflow_dags.yaml > "$temp_file"
                                mv "$temp_file" /workdir/airflow_dags.yaml
                                
                                # Add secret key to YAML file using temporary file
                                airflow_secret_key=$(grep -Eo '^[a-zA-Z0-9_]+:' $AIRFLOW_SECRET_FILE_NAME | head -n 1 | tr -d ':')
                                temp_file=$(mktemp)
                                yq -y ".secrets.platform.\"${PLATFORM,,}\" = (.secrets.platform.\"${PLATFORM,,}\" // []) + [\"${airflow_secret_key}\"]" /workdir/airflow_secrets.yaml > "$temp_file"
                                mv "$temp_file" /workdir/airflow_secrets.yaml
                            fi
                            echo "✅ [SUCCESS] Secrets imported to Composer Airflow vault"
                        fi 
                    elif [ "${PLATFORM,,}" == "airflow" ]; then
                        echo "📋 [CONFIG] Platform: Standalone Airflow"
                        echo "🔍 [CHECK] Validating Airflow environment variables..."
                        if [[ -z $AIRFLOW_URL || -z $AIRFLOW_SECRET_FILE_NAME || -z $PLATFORM || -z $AIRFLOW_USER || -z $AIRFLOW_PASSWORD ]]; then
                            echo "❌ [ERROR] One or more Airflow variables are undefined"
                            exit 1
                        else  
                            echo "✅ [SUCCESS] All Airflow variables are defined"
                            
                            # Deploy DBT API Service if Operator is API
                            if [[ "$DAG_OPERATOR" == "api" ]]; then
                                echo "⚙️ [PROCESS] Deploying DBT API Service for Airflow..."
                                export HTTPS_ENABLED="false"
                                export PHASE_ENVIRONMENT="prod"
                                export WORKER_NUM="10"
                                export MAX_REQUESTS="45"
                                export CPU_REQUEST="2000m"
                                export MEMORY_REQUEST="4Gi"
                                export APP_VERSION="{{inputs.parameters.commit_short_sha}}"
                                
                                /bin/bash -c "/usr/app/tsb-data-orchestrator-core/deploy_dbt_api_service.sh" 2>&1 | tee -a /workdir/deploy_dbt_api_service.log || true
                                
                                # Source the environment file to get the variables
                                source "${DBT_PROJECT_NAME}_${APP_VERSION:-deploy}_env.sh"
                                
                                echo "📋 [CONFIG] Updating DBT_WORKLOAD_API_CONNECTION_ID..."
                                dbt_workload_connection_id=$(grep 'DBT_WORKLOAD_API_CONNECTION_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                                if [ -z "$dbt_workload_connection_id" ]; then
                                    echo "⚙️ [PROCESS] Adding DBT_WORKLOAD_API_CONNECTION_ID variable..."
                                    sed -i "$ a\  DBT_WORKLOAD_API_CONNECTION_ID: '$DBT_WORKLOAD_API_CONNECTION_ID'" $AIRFLOW_SECRET_FILE_NAME
                                else
                                    sed -i "s/DBT_WORKLOAD_API_CONNECTION_ID.*/DBT_WORKLOAD_API_CONNECTION_ID: '$DBT_WORKLOAD_API_CONNECTION_ID'/" $AIRFLOW_SECRET_FILE_NAME
                                fi
                                
                                echo "📋 [CONFIG] Updating DBT_WORKLOAD_API_RELEASE..."
                                dbt_workload_release=$(grep 'DBT_WORKLOAD_API_RELEASE:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                                if [ -z "$dbt_workload_release" ]; then
                                    echo "⚙️ [PROCESS] Adding DBT_WORKLOAD_API_RELEASE variable..."
                                    sed -i "$ a\  DBT_WORKLOAD_API_RELEASE: '$DBT_WORKLOAD_API_RELEASE'" $AIRFLOW_SECRET_FILE_NAME
                                fi
                                echo "✅ [SUCCESS] DBT API Service deployed and configured"
                            fi
                            
                            echo "⚙️ [PROCESS] Importing secrets to Airflow vault..."
                            if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                                echo "📋 [CONFIG] Dynamic DAG is enabled - creating multiple DAG configurations..."
                                create_dynamic_dag_variables "$AIRFLOW_SECRET_FILE_NAME"
                            else
                                echo "📋 [CONFIG] Using standard single DAG configuration..."
                                python /usr/app/tsb-data-orchestrator-core/create_variables_airflow_bauth.op.py 2>&1 | tee -a /workdir/create_variables_airflow_bauth.log || true
                                
                                # Add to YAML files using temporary files
                                dag_config="{
                                    \"project\": \"$DBT_PROJECT_NAME\",
                                    \"dag_id\": \"$dag_id\",
                                    \"variables_file\": \"$AIRFLOW_SECRET_FILE_NAME\"
                                }"
                                temp_file=$(mktemp)
                                yq -y ".dags += [$dag_config]" /workdir/airflow_dags.yaml > "$temp_file"
                                mv "$temp_file" /workdir/airflow_dags.yaml
                                
                                # Add secret key to YAML file using temporary file
                                airflow_secret_key=$(grep -Eo '^[a-zA-Z0-9_]+:' $AIRFLOW_SECRET_FILE_NAME | head -n 1 | tr -d ':')
                                temp_file=$(mktemp)
                                yq -y ".secrets.platform.\"${PLATFORM,,}\" = (.secrets.platform.\"${PLATFORM,,}\" // []) + [\"${airflow_secret_key}\"]" /workdir/airflow_secrets.yaml > "$temp_file"
                                mv "$temp_file" /workdir/airflow_secrets.yaml
                            fi
                            echo "✅ [SUCCESS] Secrets imported to Airflow vault"
                        fi
                    else
                        echo "❌ [ERROR] Unknown platform: $PLATFORM"
                        exit 1
                    fi
                    cd ..
                else
                    echo "⚠️ [WARNING] Skipping non-directory: $folder"
                fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "🔍 [CHECK] Validating execution results..."
            echo "==========================================="

            error_found=0
            warning_count=0

            if [ ! -f "/workdir/deploy_dbt_api_service.log" ]; then
                echo "📋 [CONFIG] No DBT API Service deployment log found - proceeding successfully"
            else
                echo "🔍 [CHECK] Analyzing DBT API Service deployment log..."
                while read -r line; do
                    [[ -z "$line" ]] && continue

                    if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                        ((warning_count++))
                        continue
                    fi

                    if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                        echo "❌ [ERROR] Error found in DBT API Service deployment: $line"
                        error_found=1
                    fi
                done < /workdir/deploy_dbt_api_service.log
            fi

            if [ ! -f "/workdir/create_variables_airflow_bauth.log" ]; then
                echo "📋 [CONFIG] No variable import log found - proceeding successfully"
            else
                echo "🔍 [CHECK] Analyzing variable import log..."
                while read -r line; do
                    [[ -z "$line" ]] && continue

                    if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                        ((warning_count++))
                        continue
                    fi

                    if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                        echo "❌ [ERROR] Error found in variable import procedure: $line"
                        error_found=1
                    fi
                done < /workdir/create_variables_airflow_bauth.log

                if [ $error_found -eq 1 ]; then
                    echo "❌ [ERROR] DBT Project DAG Variable import procedure failed"
                    exit 1
                else
                    echo "✅ [SUCCESS] DBT Project DAG Variable import procedure completed successfully"
                fi
            fi

            echo "==========================================="
            echo "✅ [SUCCESS] DBT Project Data Orchestration Variables Import Completed"
            echo "📋 [CONFIG] Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="
        env:
          - name: CUSTOMER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: DOMAIN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DOMAIN
          - name: DBT_API_SERVER_CONTROLLER_IMAGE_TAG
            value: "v0.0.7.1"
          - name: DBT_API_SERVER_CONTROLLER_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DBT_API_SERVER_CONTROLLER_SECRET
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: PRIVATE_ACCESS_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_NAME
          - name: PRIVATE_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_TOKEN
          - name: SECRET_PACKAGE_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_PACKAGE_REPO_TOKEN_NAME
          - name: SECRET_DBT_PACKAGE_REPO_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_DBT_PACKAGE_REPO_TOKEN
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_LOCATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_LOCATION
                optional: true
          - name: COMPOSER_ENVIRONMENT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: COMPOSER_ENVIRONMENT_NAME
                optional: true
          - name: GCP_SA_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_EMAIL
                optional: true
          - name: DATAHUB_GMS_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATAHUB_GMS_URL
                optional: true
          - name: DATAHUB_GMS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATAHUB_GMS_TOKEN
                optional: true
          - name: GCP_LOCATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_LOCATION
                optional: true
          - name: GCP_LOCATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_LOCATION
                optional: true
          - name: COMPOSER_ENVIRONMENT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: COMPOSER_ENVIRONMENT_NAME
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir-temp
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: compile-dbt-manifest
      inputs:
        parameters:
          - name: revision
          - name: commit_short_sha
        artifacts:
          - name: repo-artifact
            path: /workdir/src
          - name: changed-files-artifact
            path: /workdir/changed_folders.txt
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.dbt_workflow_core_image_name}}:{{workflow.parameters.dbt_workflow_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "🚀 [INFO] DBT Project Manifest Compilation"
            echo "📋 [CONFIG] Target: Main Branch"
            echo "🔗 [CONFIG] Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="

            cd /workdir/src

            # Scan changed folders for DBT compilation
            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "⚙️ [PROCESS] Processing DBT Project: $folder"
                echo "==========================================="
                
                cd "$folder"
                
                # ===========================================
                # 🔐 CREDENTIALS CONFIGURATION
                # ===========================================
                echo "🔍 [CHECK] Configuring data warehouse credentials..."
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "❌ [ERROR] Google BigQuery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "🔐 [CONFIG] Setting up Google Cloud authentication"
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json || { echo "❌ [ERROR] Failed to decode service account secret"; exit 1; }
                      gcloud auth activate-service-account --key-file /secrets/sa.json || { echo "❌ [ERROR] Failed to activate service account"; exit 1; }
                      gcloud config set project $GCP_PROJECT_NAME || { echo "❌ [ERROR] Failed to set GCP project"; exit 1; }
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "✅ [SUCCESS] Google Cloud authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "❌ [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "🔐 [CONFIG] Setting up Snowflake authentication"
                      mkdir -p /snowsql/secrets/ || { echo "❌ [ERROR] Failed to create Snowflake secrets directory"; exit 1; }
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8 || { echo "❌ [ERROR] Failed to write Snowflake private key"; exit 1; }
                      chmod 600 /snowsql/secrets/rsa_key.p8 || { echo "❌ [ERROR] Failed to set private key permissions"; exit 1; }
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "✅ [SUCCESS] Snowflake authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "❌ [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  else
                    echo "✅ [SUCCESS] Redshift credentials validated"
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "❌ [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  else
                    echo "✅ [SUCCESS] Fabric credentials validated"
                  fi
                fi
                
                # ===========================================
                # 📋 ENVIRONMENT VALIDATION
                # ===========================================
                echo "🔍 [CHECK] Validating required environment variables..."
                
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "❌ [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                else  
                  echo "✅ [SUCCESS] All required variables are defined"
                  echo "📋 [CONFIG] Importing variables from Airflow secret file"
                  
                  manifest_name=$(grep 'MANIFEST_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || { echo "❌ [ERROR] Failed to extract manifest name"; exit 1; }
                  export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" ) || { echo "❌ [ERROR] Failed to extract platform"; exit 1; }
                  export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "❌ [ERROR] Failed to extract DBT project name"; exit 1; }
                  export operator=$(grep 'OPERATOR:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || { echo "❌ [ERROR] Failed to extract operator"; exit 1; }
                  
                  echo "📋 [CONFIG] Platform: ${PLATFORM}"
                  echo "📋 [CONFIG] DBT Project: ${DBT_PROJECT_NAME}"
                  echo "📋 [CONFIG] Operator: ${operator}"
                fi
                
                # ===========================================
                # ⚙️ DBT COMPILATION
                # ===========================================
                echo "⚙️ [PROCESS] Starting DBT project compilation..."
                
                touch /workdir/compile_dbt_project_manifest.log || { echo "❌ [ERROR] Failed to create log file"; exit 1; }
                
                # Prepare the files for bash operator
                if [[ "$operator" == "bash" ]]; then
                  echo "📋 [CONFIG] Preparing files for bash operator"
                  mkdir -p /tmp/dbt_project_bash_operator || { echo "❌ [ERROR] Failed to create temporary directory"; exit 1; }
                  cp -r "../$folder" "/tmp/dbt_project_bash_operator/$folder" || { echo "❌ [ERROR] Failed to copy files to temporary directory"; exit 1; }
                fi
                
                echo "⚙️ [PROCESS] Running DBT dependencies..."
                dbt deps --quiet 2>&1 | tee -a /workdir/compile_dbt_project_manifest.log || true
                
                echo "⚙️ [PROCESS] Compiling DBT project manifest..."
                dbt compile --exclude test_type:generic test_type:singular package:re_data 2>&1 | tee -a /workdir/compile_dbt_project_manifest.log || true
                
                echo "✅ [SUCCESS] DBT compilation completed"
                
                # ===========================================
                # 📤 MANIFEST DEPLOYMENT
                # ===========================================
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "==========================================="
                  echo "☁️ [DEPLOY] Google Cloud Composer Platform"
                  echo "==========================================="
                  
                  echo "🔍 [CHECK] Validating Composer environment variables..."
                  if [[ -z $GCP_BUCKET_NAME ]]; then
                    echo "❌ [ERROR] GCP_BUCKET_NAME is undefined"
                    exit 1
                  else  
                    echo "✅ [SUCCESS] All Composer variables are defined"
                    echo "📤 [UPLOAD] Uploading manifest to Composer GCS bucket"
                    
                    gsutil cp ./target/manifest.json gs://$GCP_BUCKET_NAME/dags/${DBT_PROJECT_NAME}/${manifest_name}.json || { echo "❌ [ERROR] Failed to upload manifest to GCS"; exit 1; }
                    echo "✅ [SUCCESS] Manifest uploaded to Composer GCS bucket"
                    
                    # Copy files to final destination for bash operator 
                    if [[ "$operator" == "bash" ]]; then
                      echo "📤 [UPLOAD] Copying DBT files to GCS for bash operator"
                      gsutil cp "/tmp/dbt_project_bash_operator/$folder/"* "gs://$GCP_BUCKET_NAME/dags/dbt/${DBT_PROJECT_NAME}" || { echo "❌ [ERROR] Failed to copy DBT files to GCS"; exit 1; }
                      echo "✅ [SUCCESS] DBT files copied to GCS"
                    fi
                  fi
                  
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "==========================================="
                  echo "🔄 [DEPLOY] Apache Airflow Platform"
                  echo "==========================================="
                  
                  echo "🔍 [CHECK] Validating Airflow environment variables..."
                  if [[ -z ${GIT_USER_EMAIL} || -z ${GIT_USER_NAME} || -z ${DAG_REPO_PUSH} ]]; then
                    echo "❌ [ERROR] One or more Airflow variables are undefined"
                    exit 1
                  else
                    echo "✅ [SUCCESS] All Airflow variables are defined"

                    echo "⚙️ [PROCESS] Setting up SSH keys..."
                    export GIT_SSH_COMMAND="ssh -i /root/.ssh/id_rsa -o StrictHostKeyChecking=no"
                    
                    echo "📋 [CONFIG] Configuring Git for Airflow deployment"
                    git config --global user.email "${GIT_USER_EMAIL}" || { echo "❌ [ERROR] Failed to set Git user email"; exit 1; }
                    git config --global user.name "${GIT_USER_NAME}" || { echo "❌ [ERROR] Failed to set Git user name"; exit 1; }
                    
                    REPO_DIR=../../airflow-dags
                    
                    # Clean up existing repository if present
                    if [ ! -d "$REPO_DIR" ]; then
                      echo "📋 [CONFIG] Airflow DAGs repository directory does not exist - proceeding with clone"
                    else
                      echo "🧹 [CLEANUP] Removing existing Airflow DAGs repository"
                      rm -fr $REPO_DIR || { echo "❌ [ERROR] Failed to remove existing repository"; exit 1; }
                    fi
                    
                    echo "📥 [CLONE] Cloning Airflow DAGs repository"
                    git clone $DAG_REPO_PUSH $REPO_DIR || { echo "❌ [ERROR] Failed to clone Airflow DAGs repository"; exit 1; }
                    
                    # Create directory structure
                    if [ ! -d "$REPO_DIR/dags/" ]; then
                      echo "📁 [CREATE] Creating dags directory"
                      mkdir "$REPO_DIR/dags/" || { echo "❌ [ERROR] Failed to create dags directory"; exit 1; }
                    fi
                    
                    if [ ! -d "$REPO_DIR/dags/${DBT_PROJECT_NAME}/" ]; then
                      echo "📁 [CREATE] Creating DBT project directory"
                      mkdir "$REPO_DIR/dags/${DBT_PROJECT_NAME}/" || { echo "❌ [ERROR] Failed to create DBT project directory"; exit 1; }
                    fi
                    
                    echo "📤 [UPLOAD] Uploading manifest to Airflow DAG repository"
                    cp ./target/manifest.json $REPO_DIR/dags/${DBT_PROJECT_NAME}/${manifest_name}.json || { echo "❌ [ERROR] Failed to copy manifest to repository"; exit 1; }
                    
                    # Copy files to final destination for bash operator 
                    if [[ "$operator" == "bash" ]]; then
                      echo "📤 [UPLOAD] Copying DBT files for bash operator"
                      mkdir -p "$REPO_DIR/dags/dbt/${DBT_PROJECT_NAME}" || { echo "❌ [ERROR] Failed to create DBT files directory"; exit 1; }
                      cp -r "/tmp/dbt_project_bash_operator/$folder/"* "$REPO_DIR/dags/dbt/${DBT_PROJECT_NAME}" || { echo "❌ [ERROR] Failed to copy DBT files to repository"; exit 1; }
                    fi
                    
                    echo "✅ [SUCCESS] Files uploaded to Airflow repository"
                    
                    echo "📝 [COMMIT] Committing changes to repository"
                    git -C "$REPO_DIR" add -A || { echo "❌ [ERROR] Failed to stage changes"; exit 1; }
                    git -C "$REPO_DIR" commit -am"Update DBT Project Manifest files" || { echo "❌ [ERROR] Failed to commit changes"; exit 1; }
                    git -C "$REPO_DIR" remote set-url --push origin "${DAG_REPO_PUSH}" || { echo "❌ [ERROR] Failed to set remote URL"; exit 1; }
                    git -C "$REPO_DIR" push || { echo "⚠️ [WARNING] Failed to push changes - continuing"; true; }
                    
                    echo "✅ [SUCCESS] Changes committed and pushed to Airflow repository"
                  fi
                  
                else
                  echo "❌ [ERROR] Unknown platform: ${PLATFORM}"
                  exit 1
                fi
                
                cd ..
              else
                echo "⏭️ [SKIP] Working on root - skipping $folder"
              fi
            done < /workdir/changed_folders.txt

            # ===========================================
            # 🔍 COMPILATION VALIDATION
            # ===========================================
            echo "==========================================="
            echo "🔍 [VALIDATION] Checking compilation results"
            echo "==========================================="

            error_found=0

            if [ ! -f "/workdir/compile_dbt_project_manifest.log" ]; then
              echo "✅ [SUCCESS] No changes detected - manifest compilation log file does not exist"
              echo "✅ [SUCCESS] Proceeding successfully"
            else
              echo "🔍 [CHECK] Analyzing compilation log for errors..."
              
              while read -r line; do
                if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                  echo "❌ [ERROR] Error found in DBT manifest compilation: $line"
                  error_found=1
                fi
              done < /workdir/compile_dbt_project_manifest.log

              if [ $error_found -eq 1 ]; then
                echo "❌ [ERROR] DBT manifest compilation failed - please check the logs for details"
                exit 1
              else
                echo "✅ [SUCCESS] DBT manifest compilation completed successfully"
              fi
            fi

            echo "==========================================="
            echo "🎉 [COMPLETE] DBT Project Manifest Compilation Finished"
            echo "📋 [SUMMARY] All operations completed successfully"
            echo "==========================================="
        env:
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: PRIVATE_ACCESS_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_NAME
          - name: PRIVATE_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_TOKEN
          - name: SECRET_PACKAGE_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_PACKAGE_REPO_TOKEN_NAME
          - name: SECRET_DBT_PACKAGE_REPO_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_DBT_PACKAGE_REPO_TOKEN
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_LOCATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_LOCATION
                optional: true
          - name: COMPOSER_ENVIRONMENT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: COMPOSER_ENVIRONMENT_NAME
                optional: true
          - name: GCP_SA_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_EMAIL
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir-temp
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
          - name: ssh-keys
            mountPath: /root/.ssh/
    - name: compile-dag
      inputs:
        parameters:
          - name: revision
          - name: commit_short_sha
        artifacts:
          - name: compiled-repo-artifact
            path: /workdir/src
          - name: changed-files-artifact
            path: /workdir/changed_folders.txt
          - name: airflow_secret_key
            path: /workdir/airflow_secrets.yaml
          - name: airflow_dag_id
            path: /workdir/airflow_dags.yaml
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_orchestrator_core_image_name}}:{{workflow.parameters.data_orchestrator_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash
            # Exit on error, treat unset variables as errors, and return value of pipeline is the status of the last command


            echo "==========================================="
            echo "🚀 [INFO] DBT Project Manifest Compilation"
            echo "📋 [CONFIG] Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="

            cd /workdir/src

            # Validate required configuration files
            echo "🔍 [CHECK] Validating configuration files..."
            if [ ! -f "/workdir/airflow_dags.yaml" ] || [ ! -f "/workdir/airflow_secrets.yaml" ]; then
              echo "❌ [ERROR] Required configuration files missing"
              exit 1
            fi
            echo "✅ [SUCCESS] Configuration files validated"

            # Scan changed folders for DBT compilation
            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "⚙️ [PROCESS] Processing folder: $folder"
                echo "==========================================="
                
                cd "$folder"
                
                # Initialize an array to track created DAG files for this folder
                created_dag_files=()
                
                # Configure warehouse credentials
                echo "🔐 [CONFIG] Setting up data warehouse credentials..."
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "❌ [ERROR] Google BigQuery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                      gcloud auth activate-service-account --key-file /secrets/sa.json
                      gcloud config set project $GCP_PROJECT_NAME
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "✅ [SUCCESS] BigQuery credentials configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "❌ [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      mkdir -p /snowsql/secrets/
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8
                      chmod 600 /snowsql/secrets/rsa_key.p8
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "✅ [SUCCESS] Snowflake credentials configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "❌ [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  fi
                  echo "✅ [SUCCESS] Redshift credentials validated"
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "❌ [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  fi
                  echo "✅ [SUCCESS] Fabric credentials validated"
                fi
                
                echo "🔍 [CHECK] Validating required environment variables..."
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "❌ [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                else  
                  echo "✅ [SUCCESS] All required variables are defined"
                  echo "📋 [CONFIG] Importing variables from Airflow Secret yml file"
                  
                  # Validate variables file exists
                  if [ ! -f "$AIRFLOW_SECRET_FILE_NAME" ]; then
                    echo "❌ [ERROR] Variables file $AIRFLOW_SECRET_FILE_NAME not found"
                    exit 1
                  fi

                  echo "✅ [SUCCESS] Variables file validated"
                  
                  export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" )
                  export source_commit_id={{ inputs.parameters.commit_short_sha }}
                  export dbt_project_name=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export DYNAMIC_DAG=$(grep 'DYNAMIC_DAG:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                fi

                echo "⚙️ [PROCESS] Processing dbt project: $dbt_project_name"
                
                touch /workdir/create_dag_from_template.log

                # Function to create and upload dynamic DAGs
                create_dynamic_dags() {
                  local base_secret_file="$1"
                  local current_project="$dbt_project_name"
                  local dynamic_config_file="dynamic_dag_config.yml"
                  
                  echo "🔍 [CHECK] Validating dynamic DAG configuration..."
                  if [ ! -f "$dynamic_config_file" ]; then
                    echo "❌ [ERROR] $dynamic_config_file not found"
                    return 1
                  fi

                  echo "📋 [CONFIG] Reading DAG configurations from airflow_dags.yaml"

                  # Use yq to extract DAG configurations for the current project
                  local dag_configs=$(yq -r '.dags[] | select(.project == "'$current_project'") | [.dag_id, .variables_file] | @tsv' /workdir/airflow_dags.yaml)
                  
                  if [ -z "$dag_configs" ]; then
                    echo "❌ [ERROR] No DAG configurations found for project $current_project"
                    return 1
                  fi
                  
                  echo "✅ [SUCCESS] Found DAG configurations for dynamic processing"
                    
                  # Process each DAG configuration
                  while IFS=$'\t' read -r dag_id variables_file; do
                    if [ -z "$dag_id" ] || [ -z "$variables_file" ]; then
                      echo "⚠️ [WARNING] Skipping invalid DAG configuration"
                      continue
                    fi
                    
                    echo "⚙️ [PROCESS] Creating DAG: $dag_id"
                    
                    # Verify variables file exists
                    if [ ! -f "$variables_file" ]; then
                      echo "❌ [ERROR] Variables file $variables_file not found"
                      continue
                    fi
                    
                    echo "✅ [SUCCESS] Variables file validated for DAG: $dag_id"
                    
                    # Create DAG file using parameters instead of environment variables
                    echo "⚙️ [PROCESS] Generating DAG file for $dag_id"
                    python /usr/app/tsb-data-orchestrator-core/create_dag_from_template.py --variables-file "$variables_file" --dag-id "$dag_id" 2>&1 | tee -a /workdir/create_dag_from_template.log || true
                    
                    # Move DAG file to dags directory with proper name
                    local dag_file="${dag_id}_dag.py"
                    
                    if [ -f "$dag_file" ]; then
                      echo "✅ [SUCCESS] Moving DAG file to /workdir/dags/${dag_id}.py"
                      mv "$dag_file" "/workdir/dags/${dag_id}.py"
                      # Track the created DAG file
                      created_dag_files+=("/workdir/dags/${dag_id}.py")
                    else
                      echo "⚠️ [WARNING] DAG file $dag_file was not created, attempting fallback..."
                      # Try to find any DAG file and use it
                      found_dag_file=$(find . -name "*dag*.py" | head -1)
                      if [ -n "$found_dag_file" ]; then
                        echo "✅ [SUCCESS] Found fallback DAG file: $found_dag_file"
                        mv "$found_dag_file" "/workdir/dags/${dag_id}.py"
                        created_dag_files+=("/workdir/dags/${dag_id}.py")
                      else
                        echo "❌ [ERROR] No DAG file found for $dag_id"
                        continue
                      fi
                    fi
                  done <<< "$dag_configs"
                }

                # Function to create and upload standard DAG
                create_standard_dag() {
                  local base_secret_file="$1"
                  local current_project="$dbt_project_name"
                  
                  echo "📋 [CONFIG] Using standard single DAG configuration"
                  # Get the DAG configuration from airflow_dags.yaml
                  local dag_config=$(yq -r '.dags[] | select(.project == "'$current_project'") | [.dag_id, .variables_file] | @tsv' /workdir/airflow_dags.yaml)
                  
                  if [ -z "$dag_config" ]; then
                    echo "❌ [ERROR] Could not find DAG configuration for project $current_project in airflow_dags.yaml"
                    return 1
                  fi
                  
                  IFS=$'\t' read -r dag_id variables_file <<< "$dag_config"
                  
                  echo "📋 [CONFIG] DAG configuration:"
                  echo "   DAG ID: $dag_id"
                  echo "   Variables file: $variables_file"
                  
                  # Verify variables file exists
                  if [ ! -f "$variables_file" ]; then
                    echo "❌ [ERROR] Variables file $variables_file not found"
                    return 1
                  fi
                  
                  # Set environment variables
                  export AIRFLOW_SECRET_FILE_NAME="$variables_file"
                  export DAG_ID="$dag_id"
                  export DBT_PROJECT_DAG_ID="$dag_id"
                  
                  echo "⚙️ [PROCESS] Generating standard DAG file"
                  python /usr/app/tsb-data-orchestrator-core/create_dag_from_template.py 2>&1 | tee -a /workdir/create_dag_from_template.log || true
                  
                  # Move DAG file to dags directory with proper name
                  local dag_file="${dag_id}_dag.py"

                  if [ -f "$dag_file" ]; then
                    echo "✅ [SUCCESS] Moving DAG file to /workdir/dags/${dag_id}.py"
                    mv "$dag_file" "/workdir/dags/${dag_id}.py"
                    # Track the created DAG file
                    created_dag_files+=("/workdir/dags/${dag_id}.py")
                  else
                    echo "⚠️ [WARNING] DAG file $dag_file was not created, attempting fallback..."
                    # Try to find any DAG file and use it
                    found_dag_file=$(find . -name "*dag*.py" | head -1)
                    if [ -n "$found_dag_file" ]; then
                      echo "✅ [SUCCESS] Found fallback DAG file: $found_dag_file"
                      mv "$found_dag_file" "/workdir/dags/${dag_id}.py"
                      created_dag_files+=("/workdir/dags/${dag_id}.py")
                    else
                      echo "❌ [ERROR] No DAG file found for standard configuration"
                      return 1
                    fi
                  fi
                }

                # Create directory for DAG files if it doesn't exist
                mkdir -p /workdir/dags
                
                echo "⚙️ [PROCESS] Compiling DBT Project DAG"
                if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                  echo "📋 [CONFIG] Dynamic DAG is enabled - creating multiple DAG files"
                  create_dynamic_dags "$AIRFLOW_SECRET_FILE_NAME"
                else
                  echo "📋 [CONFIG] Using standard single DAG configuration"
                  create_standard_dag "$AIRFLOW_SECRET_FILE_NAME"
                fi

                echo "✅ [SUCCESS] DAG for dbt project $dbt_project_name was created"
                echo "==========================================="
                echo "📤 [UPLOAD] Starting DAG deployment process"
                echo "==========================================="
                
                echo "🔍 [CHECK] Platform configuration: $PLATFORM"
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "📋 [CONFIG] Platform: Google Cloud Composer"
                  echo "🔍 [CHECK] Validating Composer environment variables"
                  if [[ -z $GCP_BUCKET_NAME ]]; then
                    echo "❌ [ERROR] GCP_BUCKET_NAME is undefined"
                    exit 1
                  else  
                    echo "✅ [SUCCESS] All Composer variables are defined"
                    echo "📤 [UPLOAD] Uploading DAG files to Composer GCS Bucket"
                    if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                      if [ ${#created_dag_files[@]} -gt 0 ]; then
                        echo "⚙️ [PROCESS] Uploading dynamic DAGs to GCS bucket"
                        for dag_file in "${created_dag_files[@]}"; do
                          gsutil -m cp "$dag_file" "gs://$GCP_BUCKET_NAME/dags/$folder/"
                          echo "${PLATFORM,,}=gs://$GCP_BUCKET_NAME/dags/$folder/$(basename $dag_file)" >> /workdir/compile_dag_path.log
                        done
                        echo "✅ [SUCCESS] Dynamic DAGs uploaded to GCS"
                      else
                        echo "❌ [ERROR] No DAG files found for upload"
                        exit 1
                      fi
                    else
                      if [ ${#created_dag_files[@]} -gt 0 ]; then
                        gsutil -m cp "${created_dag_files[0]}" "gs://$GCP_BUCKET_NAME/dags/$folder/"
                        echo "${PLATFORM,,}=gs://$GCP_BUCKET_NAME/dags/$folder/$(basename ${created_dag_files[0]})" >> /workdir/compile_dag_path.log
                        echo "✅ [SUCCESS] Standard DAG uploaded to GCS"
                      else
                        echo "❌ [ERROR] No DAG files found for upload"
                        exit 1
                      fi
                    fi
                    echo "✅ [SUCCESS] Composer deployment completed"
                  fi
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "📋 [CONFIG] Platform: Apache Airflow"
                  echo "🔍 [CHECK] Validating Airflow environment variables"
                  if [[ -z ${GIT_USER_EMAIL} || -z ${GIT_USER_NAME} || -z ${DAG_REPO_PUSH} ]]; then
                    echo "❌ [ERROR] One or more Airflow variables are undefined"
                    exit 1
                  else
                    echo "✅ [SUCCESS] All Airflow variables are defined"

                    echo "⚙️ [PROCESS] Setting up SSH keys..."
                    export GIT_SSH_COMMAND="ssh -i /root/.ssh/id_rsa -o StrictHostKeyChecking=no"

                    echo "📋 [CONFIG] Configuring Git for DAG repository"
                    git config --global user.email "${GIT_USER_EMAIL}"
                    git config --global user.name "${GIT_USER_NAME}"
                    
                    REPO_DIR=../../airflow-dags
                    if [ ! -d "$REPO_DIR" ]; then
                      echo "📋 [CONFIG] Repository directory does not exist - will create"
                    else
                      echo "⚠️ [WARNING] Repository directory exists - removing for fresh clone"
                      rm -fr $REPO_DIR
                    fi
                    
                    echo "⚙️ [PROCESS] Cloning Airflow DAGs repository"
                    git clone $DAG_REPO_PUSH $REPO_DIR
                    
                    if [ ! -d "$REPO_DIR/dags/" ]; then
                      echo "📋 [CONFIG] Creating dags directory in repository"
                      mkdir "$REPO_DIR/dags/"
                    else
                      echo "✅ [SUCCESS] Dags directory exists in repository"
                    fi
                    
                    echo "📤 [UPLOAD] Uploading DAG files to Airflow DAG repository"
                    if [ ! -d "$REPO_DIR/dags/$folder/" ]; then
                      echo "📋 [CONFIG] Creating project directory: $REPO_DIR/dags/$folder/"
                      mkdir $REPO_DIR/dags/$folder/
                    else
                      echo "✅ [SUCCESS] Project directory exists"
                    fi
                    
                    if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                      if [ ${#created_dag_files[@]} -gt 0 ]; then
                        echo "⚙️ [PROCESS] Copying dynamic DAGs to repository"
                        for dag_file in "${created_dag_files[@]}"; do
                          cp "$dag_file" $REPO_DIR/dags/$folder/
                          echo "${PLATFORM,,}=$REPO_DIR/dags/$folder/$(basename $dag_file)" >> /workdir/compile_dag_path.log
                        done
                        echo "✅ [SUCCESS] Dynamic DAGs copied to repository"
                      else
                        echo "❌ [ERROR] No DAG files found for upload"
                        exit 1
                      fi
                    else
                      if [ ${#created_dag_files[@]} -gt 0 ]; then
                        cp "${created_dag_files[0]}" $REPO_DIR/dags/$folder/
                        echo "${PLATFORM,,}=$REPO_DIR/dags/$folder/$(basename ${created_dag_files[0]})" >> /workdir/compile_dag_path.log
                        echo "✅ [SUCCESS] Standard DAG copied to repository"
                      else
                        echo "❌ [ERROR] No DAG files found for upload"
                        exit 1
                      fi
                    fi
                    
                    echo "⚙️ [PROCESS] Committing and pushing changes to repository"
                    git -C "$REPO_DIR" add -A
                    git -C "$REPO_DIR" commit -am"Update DBT Project DAG files"
                    git -C "$REPO_DIR" remote set-url --push origin "${DAG_REPO_PUSH}" 
                    git -C "$REPO_DIR" push || true
                    echo "✅ [SUCCESS] Airflow deployment completed"
                  fi
                else
                  echo "❌ [ERROR] Unknown platform: $PLATFORM"
                  exit 1
                fi
                cd ..
              else
                echo "⚠️ [WARNING] Skipping root directory: $folder"
              fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "🔍 [CHECK] Validating DAG compilation results"
            echo "==========================================="

            error_found=0
            warning_count=0

            if [ ! -f "/workdir/create_dag_from_template.log" ]; then
              echo "✅ [SUCCESS] No changes detected - DAG compilation log file does not exist. Proceeding successfully."
            else
              while read -r line; do
                [[ -z "$line" ]] && continue

                if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                  ((warning_count++))
                  continue
                fi

                if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                  echo "❌ [ERROR] Error found in DAG compilation log: $line"
                  error_found=1
                fi
              done < /workdir/create_dag_from_template.log

              if [ $error_found -eq 1 ]; then
                echo "❌ [ERROR] DAG compilation procedure failed. Please check the logs for details."
                exit 1
              else
                echo "✅ [SUCCESS] DAG compilation validation completed"
              fi
            fi

            echo "==========================================="
            echo "✅ [SUCCESS] DBT Project DAG Compilation completed"
            echo "📋 [SUMMARY] Deployment paths logged to compile_dag_path.log"
            echo "==========================================="
        env:
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: PRIVATE_ACCESS_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_NAME
          - name: PRIVATE_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_TOKEN
          - name: SECRET_PACKAGE_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_PACKAGE_REPO_TOKEN_NAME
          - name: SECRET_DBT_PACKAGE_REPO_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_DBT_PACKAGE_REPO_TOKEN
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_LOCATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_LOCATION
                optional: true
          - name: COMPOSER_ENVIRONMENT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: COMPOSER_ENVIRONMENT_NAME
                optional: true
          - name: GCP_SA_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_EMAIL
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir-temp
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
          - name: ssh-keys
            mountPath: /root/.ssh/
    - name: refresh-dbt-incremental-modules
      inputs:
        parameters:
          - name: revision
          - name: commit_short_sha
          - name: commit_before_sha
        artifacts:
          - name: repo-artifact
            path: /workdir/src
          - name: changed-files-artifact
            path: /workdir/changed_folders.txt
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.dbt_workflow_core_image_name}}:{{workflow.parameters.dbt_workflow_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "🚀 [INFO] DBT Project Incremental Modules Refresh"
            echo "📋 [CONFIG] Environment: Main Branch"
            echo "🔗 [CONFIG] Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="

            cd /workdir/src || { echo "❌ [ERROR] Failed to change to /workdir/src directory"; exit 1; }

            # Scan changed folders for DBT compilation
            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "⚙️ [PROCESS] Processing folder: $folder"
                echo "==========================================="
                
                cd "$folder" || { echo "❌ [ERROR] Failed to change to directory: $folder"; exit 1; }
                
                echo "📋 [CONFIG] Setting up warehouse credentials..."
                
                # Set warehouse credentials
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  echo "🔍 [CHECK] Configuring BigQuery credentials..."
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "❌ [ERROR] Google BigQuery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "⚙️ [PROCESS] Activating BigQuery service account..."
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json || { echo "❌ [ERROR] Failed to decode service account secret"; exit 1; }
                      gcloud auth activate-service-account --key-file /secrets/sa.json || { echo "❌ [ERROR] Failed to activate service account"; exit 1; }
                      gcloud config set project "$GCP_PROJECT_NAME" || { echo "❌ [ERROR] Failed to set GCP project"; exit 1; }
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "✅ [SUCCESS] BigQuery credentials configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  echo "🔍 [CHECK] Configuring Snowflake credentials..."
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "❌ [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "⚙️ [PROCESS] Setting up Snowflake private key..."
                      mkdir -p /snowsql/secrets/ || { echo "❌ [ERROR] Failed to create Snowflake secrets directory"; exit 1; }
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8 || { echo "❌ [ERROR] Failed to write Snowflake private key"; exit 1; }
                      chmod 600 /snowsql/secrets/rsa_key.p8 || { echo "❌ [ERROR] Failed to set private key permissions"; exit 1; }
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE="$SNOWFLAKE_PASSPHRASE"
                      echo "✅ [SUCCESS] Snowflake credentials configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  echo "🔍 [CHECK] Validating Redshift credentials..."
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "❌ [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  else
                    echo "✅ [SUCCESS] Redshift credentials validated"
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  echo "🔍 [CHECK] Validating Fabric credentials..."
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "❌ [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  else
                    echo "✅ [SUCCESS] Fabric credentials validated"
                  fi
                fi
                
                echo "🔍 [CHECK] Validating required environment variables..."
                if [[ -z "$AIRFLOW_SECRET_FILE_NAME" ]]; then
                  echo "❌ [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                else
                  echo "✅ [SUCCESS] All required variables are defined"
                  echo "⚙️ [PROCESS] Importing variables from Airflow secret file..."
                  export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "❌ [ERROR] Failed to extract DBT_PROJECT_NAME"; exit 1; }
                fi
                
                touch /workdir/refresh_dbt_incremental_modules.log || { echo "❌ [ERROR] Failed to create log file"; exit 1; }
                
                # Function to check and modify packages.yml if needed
                check_and_modify_packages() {
                  local packages_file="packages.yml"
                  
                  echo "🔍 [CHECK] Checking for re-data package in $packages_file"
                  
                  # Check if re-data package exists in packages.yml
                  if grep -q "re-data/re_data" "$packages_file"; then
                    echo "⚠️ [WARNING] Found re-data package, removing it..."
                    
                    # Create a temporary file without the re-data package
                    yq 'del(.packages[] | select(.package == "re-data/re_data"))' "$packages_file" > "${packages_file}.tmp" || { echo "❌ [ERROR] Failed to create temporary packages file"; exit 1; }
                    
                    # Verify the temporary file was created and has content
                    if [ -s "${packages_file}.tmp" ]; then
                      mv "${packages_file}.tmp" "$packages_file" || { echo "❌ [ERROR] Failed to replace packages file"; exit 1; }
                      echo "✅ [SUCCESS] Successfully removed re-data package from $packages_file"
                    else
                      echo "❌ [ERROR] Failed to create temporary file without re-data package"
                      exit 1
                    fi
                    
                    # Force recompilation by removing manifest and packages
                    echo "⚙️ [PROCESS] Cleaning dbt project..."
                    dbt clean --quiet 2>/dev/null || true
                    rm -rf ./dbt_packages/ 2>/dev/null || true
                    echo "✅ [SUCCESS] DBT project cleanup completed"
                  else
                    echo "📋 [CONFIG] No re-data package found in $packages_file"
                  fi
                }
                
                # Remove Data_Quality packages
                echo "==========================================="
                echo "⚙️ [PROCESS] Data Quality packages removal"
                echo "==========================================="
                
                if [ -f "packages.yml" ]; then
                  check_and_modify_packages
                else
                  echo "📋 [CONFIG] No packages.yml file found, skipping package modifications"
                fi
                
                # Copy incremental refresh script to project directory
                echo "⚙️ [PROCESS] Copying incremental refresh script..."
                cp /usr/app/dbt/model_incremental_refresh.sh . || { echo "❌ [ERROR] Failed to copy incremental refresh script"; exit 1; }
                
                echo "==========================================="
                echo "🚀 [INFO] Starting incremental refresh procedure"
                echo "📋 [CONFIG] Previous commit: {{inputs.parameters.commit_before_sha}}"
                echo "==========================================="
                
                /bin/bash model_incremental_refresh.sh --previous-commit "{{inputs.parameters.commit_before_sha}}" 2>&1 | tee -a /workdir/refresh_dbt_incremental_modules.log || { echo "❌ [ERROR] Incremental refresh script failed"; exit 1; }
                
                # Clean up
                echo "⚙️ [PROCESS] Cleaning up temporary files..."
                rm model_incremental_refresh.sh || true
                echo "✅ [SUCCESS] Cleanup completed"
                
                cd .. || { echo "❌ [ERROR] Failed to return to parent directory"; exit 1; }
              else
                echo "📋 [CONFIG] Working on root - skipping $folder"
              fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "🔍 [CHECK] Analyzing refresh results"
            echo "==========================================="

            error_found=0

            if [ ! -f "/workdir/refresh_dbt_incremental_modules.log" ]; then
              echo "📋 [CONFIG] No changes detected - refresh incremental modules log file does not exist"
              echo "✅ [SUCCESS] Proceeding successfully"
            else
              echo "⚙️ [PROCESS] Checking log file for errors..."
              while read -r line; do
                # Skip empty lines
                [ -z "$line" ] && continue
                # Check for actual error patterns
                if echo "$line" | grep -Ei "error|fail|forbidden|denied" | grep -v "ERROR=0" | grep -v "errors.*\[\]" > /dev/null; then
                  echo "❌ [ERROR] Error found in dbt refresh incremental modules procedure"
                  echo "📋 [CONFIG] Log file line: $line"
                  error_found=1
                fi
              done < /workdir/refresh_dbt_incremental_modules.log

              if [ $error_found -eq 1 ]; then
                echo "❌ [ERROR] The dbt refresh incremental modules procedure failed"
                echo "📋 [CONFIG] Please check the logs for details"
                exit 1
              else
                echo "✅ [SUCCESS] No errors found in refresh procedure"
              fi
            fi

            echo "==========================================="
            echo "✅ [SUCCESS] DBT Project incremental modules refresh procedure completed"
            echo "📋 [CONFIG] Environment: Main Branch"
            echo "🔗 [CONFIG] Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="
        env:
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: PRIVATE_ACCESS_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_NAME
          - name: PRIVATE_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_TOKEN
          - name: SECRET_PACKAGE_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_PACKAGE_REPO_TOKEN_NAME
          - name: SECRET_DBT_PACKAGE_REPO_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_DBT_PACKAGE_REPO_TOKEN
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_LOCATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_LOCATION
                optional: true
          - name: COMPOSER_ENVIRONMENT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: COMPOSER_ENVIRONMENT_NAME
                optional: true
          - name: GCP_SA_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_EMAIL
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir-temp
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-model-da-dependencies-import
      inputs:
        parameters:
          - name: revision
          - name: repo_url
          - name: commit_short_sha
        artifacts:
          - name: repo-artifact
            path: /workdir/src
          - name: changed-files-artifact
            path: /workdir/changed_folders.txt
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_analysis_import_core_image_name}}:{{workflow.parameters.data_analysis_import_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "🚀 [INFO] DBT Project Manifest Compilation"
            echo "📋 [CONFIG] Processing Main Branch with Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="

            cd /workdir/src

            # Scan changed folders for DBT compilation
            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "⚙️ [PROCESS] Processing folder: $folder"
                echo "==========================================="
                
                cd "$folder"
                
                # ===========================================
                # 🔐 WAREHOUSE CREDENTIALS SETUP
                # ===========================================
                echo "📋 [CONFIG] Configuring warehouse credentials..."
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  echo "🔍 [CHECK] Validating BigQuery credentials..."
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "❌ [ERROR] Google BigQuery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "⚙️ [PROCESS] Setting up BigQuery authentication..."
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json || { echo "❌ [ERROR] Failed to decode service account"; exit 1; }
                      gcloud auth activate-service-account --key-file /secrets/sa.json || { echo "❌ [ERROR] Failed to activate service account"; exit 1; }
                      gcloud config set project $GCP_PROJECT_NAME || { echo "❌ [ERROR] Failed to set project"; exit 1; }
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "✅ [SUCCESS] BigQuery authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  echo "🔍 [CHECK] Validating Snowflake credentials..."
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "❌ [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "⚙️ [PROCESS] Setting up Snowflake authentication..."
                      mkdir -p /snowsql/secrets/ || { echo "❌ [ERROR] Failed to create secrets directory"; exit 1; }
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8 || { echo "❌ [ERROR] Failed to write private key"; exit 1; }
                      chmod 600 /snowsql/secrets/rsa_key.p8 || { echo "❌ [ERROR] Failed to set key permissions"; exit 1; }
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "✅ [SUCCESS] Snowflake authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  echo "🔍 [CHECK] Validating Redshift credentials..."
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "❌ [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  fi
                  echo "✅ [SUCCESS] Redshift credentials validated"
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  echo "🔍 [CHECK] Validating Fabric credentials..."
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "❌ [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  fi
                  echo "✅ [SUCCESS] Fabric credentials validated"
                fi
                
                # ===========================================
                # 📊 DATA ANALYSIS PLATFORM PROCESSING
                # ===========================================
                echo "🚀 [INFO] Starting BI Model dbt metadata check"
                echo "🔍 [CHECK] Validating Data Analysis platform configuration..."
                
                if [[ -z $DATA_ANALYSIS_PLATFORM ]]; then
                  echo "⚠️ [WARNING] DATA_ANALYSIS_PLATFORM is not defined, skipping data analysis processing"
                else
                  echo "📋 [CONFIG] Data Analysis platform: ${DATA_ANALYSIS_PLATFORM}"
                  
                  if [[ "${DATA_ANALYSIS_PLATFORM,,}" == "lightdash" ]]; then
                    echo "==========================================="
                    echo "⚙️ [PROCESS] Processing Lightdash Integration"
                    echo "==========================================="
                    
                    echo "🔍 [CHECK] Validating Lightdash configuration..."
                    if [[ -z ${LIGHTDASH_API_KEY} || -z ${LIGHTDASH_URL} ]]; then
                      echo "❌ [ERROR] Lightdash configuration incomplete: API key or URL missing"
                      exit 1
                    else
                      echo "✅ [SUCCESS] Lightdash configuration validated"
                      echo "⚙️ [PROCESS] Cleaning Jinja code patterns from profiles.yml..."
                      
                      profile_path='profiles.yml'
                      pattern="{{[^}]*}}"
                      if grep -E "$pattern" "$profile_path"; then
                        echo "🔍 [CHECK] Jinja code found in $profile_path, removing..."
                        sed -i.bak "s/$pattern//g" "$profile_path" && rm "$profile_path.bak" || { echo "❌ [ERROR] Failed to clean profiles.yml"; exit 1; }
                        echo "✅ [SUCCESS] Jinja code removed from $profile_path"
                      else
                        echo "📋 [CONFIG] No Jinja code found in $profile_path"
                      fi
                      
                      echo "⚙️ [PROCESS] Importing variables from Airflow Secret yml file..."
                      export DATA_ANALYSIS_PROJECT=$(grep 'DATA_ANALYSIS_PROJECT:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "❌ [ERROR] Failed to extract DATA_ANALYSIS_PROJECT"; exit 1; }
                      export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "❌ [ERROR] Failed to extract DBT_PROJECT_NAME"; exit 1; }
                      export TARGET=$(grep 'TARGET:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "❌ [ERROR] Failed to extract TARGET"; exit 1; }
                      
                      echo "⚙️ [PROCESS] Extracting project configuration..."
                      DEFAULT_PROJECT_NAME=$(yq -r 'keys | .[0]' profiles.yml) || { echo "❌ [ERROR] Failed to extract project name"; exit 1; }
                      DEFAULT_TARGET=$(yq -r ".${DEFAULT_PROJECT_NAME}.target" profiles.yml) || { echo "❌ [ERROR] Failed to extract default target"; exit 1; }
                      
                      echo "⚙️ [PROCESS] Authenticating to Lightdash..."
                      echo -ne '\n' | lightdash login "${LIGHTDASH_URL}" --token "${LIGHTDASH_API_KEY}" || { echo "❌ [ERROR] Failed to authenticate with Lightdash"; exit 1; }
                      echo "✅ [SUCCESS] Lightdash authentication completed"
                      
                      if [[ -z "${DATA_ANALYSIS_PROJECT}" || "${DATA_ANALYSIS_PROJECT}" == "false" || "${DATA_ANALYSIS_PROJECT}" == "False" ]]; then
                        echo "⚠️ [WARNING] Lightdash project refresh/create is disabled, skipping..."
                      else
                        if [[ "${DATA_ANALYSIS_PROJECT}" == "true" || "${DATA_ANALYSIS_PROJECT}" == "True" ]]; then
                          echo "⚙️ [PROCESS] Creating new Lightdash project..."
                          unset DATA_ANALYSIS_PROJECT
                          unset LIGHTDASH_PROJECT
                          
                          config_file=~/.config/lightdash/config.yaml
                          touch "$config_file" || { echo "❌ [ERROR] Failed to create config file"; exit 1; }
                          if [ -f "$config_file" ]; then
                            echo "⚙️ [PROCESS] Configuring Lightdash answers section..."
                            yq eval -i '.answers = {"permissionToStoreWarehouseCredentials": true}' "$config_file" || { echo "❌ [ERROR] Failed to configure answers section"; exit 1; }
                            echo "✅ [SUCCESS] Answers section added to $config_file"
                          else
                            echo "❌ [ERROR] Configuration file $config_file does not exist"
                            exit 1
                          fi
                          
                          echo "⚙️ [PROCESS] Installing dbt dependencies..."
                          dbt deps --quiet 2>&1 || { echo "❌ [ERROR] Failed to install dbt dependencies"; exit 1; }
                          
                          echo "⚙️ [PROCESS] Running dbt models..."
                          dbt run --profiles-dir . --exclude test_type:generic test_type:singular package:re_data --target ${DEFAULT_TARGET} --empty --quiet 2>&1 || { echo "❌ [ERROR] Failed to run dbt models"; exit 1; }
                          
                          echo "⚙️ [PROCESS] Creating Lightdash project..."
                          (echo -ne '\n'; echo -ne '\n') | lightdash deploy --create "${DBT_PROJECT_NAME}" --profiles-dir . --exclude package:re_data --verbose || { echo "❌ [ERROR] Failed to create Lightdash project"; exit 1; }
                          echo "✅ [SUCCESS] Lightdash project created successfully"
                        else
                          echo "⚙️ [PROCESS] Refreshing existing Lightdash project..."
                          dbt deps --quiet 2>&1 || { echo "❌ [ERROR] Failed to install dbt dependencies"; exit 1; }
                          export LIGHTDASH_PROJECT=${DATA_ANALYSIS_PROJECT}
                          lightdash deploy --profiles-dir . --exclude package:re_data --verbose || { echo "❌ [ERROR] Failed to refresh Lightdash project"; exit 1; }
                          echo "✅ [SUCCESS] Lightdash project refreshed successfully"
                        fi
                      fi
                    fi
                    
                  elif [[ "${DATA_ANALYSIS_PLATFORM,,}" == "superset" ]]; then
                    echo "==========================================="
                    echo "⚙️ [PROCESS] Processing Superset Integration"
                    echo "==========================================="
                    
                    echo "🔍 [CHECK] Validating Superset configuration..."
                    if [[ -z $SUPERSET_BASE_URL || -z $SUPERSET_USER || -z $SUPERSET_USER_PASSWORD ]]; then
                      echo "❌ [ERROR] Superset configuration incomplete: one or more required parameters missing"
                      exit 1
                    else
                      echo "✅ [SUCCESS] Superset configuration validated"
                      export DATA_ANALYSIS_PROJECT=$(grep 'DATA_ANALYSIS_PROJECT:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "❌ [ERROR] Failed to extract DATA_ANALYSIS_PROJECT"; exit 1; }
                      
                      echo "⚙️ [PROCESS] Extracting project configuration..."
                      SNOWFLAKE_PROJECT_NAME=$(yq -r 'keys | .[0]' profiles.yml) || { echo "❌ [ERROR] Failed to extract project name"; exit 1; }
                      DEFAULT_TARGET=$(yq -r ".${SNOWFLAKE_PROJECT_NAME}.target" profiles.yml) || { echo "❌ [ERROR] Failed to extract default target"; exit 1; }
                      export SNOWFLAKE_WAREHOUSE=$(yq -r ".${SNOWFLAKE_PROJECT_NAME}.outputs.${DEFAULT_TARGET}.warehouse" profiles.yml) || { echo "❌ [ERROR] Failed to extract warehouse"; exit 1; }
                      export SNOWFLAKE_ROLE=$(yq -r ".${SNOWFLAKE_PROJECT_NAME}.outputs.${DEFAULT_TARGET}.role" profiles.yml) || { echo "❌ [ERROR] Failed to extract role"; exit 1; }
                      echo "✅ [SUCCESS] Project configuration extracted"
                      
                      if [[ -z "${DATA_ANALYSIS_PROJECT}" || "${DATA_ANALYSIS_PROJECT}" == "false" || "${DATA_ANALYSIS_PROJECT}" == "False" ]]; then
                        echo "⚠️ [WARNING] Superset Metadata Validation is disabled, skipping..."
                      else
                        echo "⚙️ [PROCESS] Copying Superset integration files..."
                        cp -r /usr/app/data_analysis_import/superset/dbt2superset/* ./ || { echo "❌ [ERROR] Failed to copy Superset files"; exit 1; }
                        echo "⚙️ [PROCESS] Running dbt2superset integration..."
                        /bin/bash dbt2superset.sh 2>&1 | tee -a /workdir/data_analysis_validating_procedure.log || { echo "❌ [ERROR] Failed to run dbt2superset integration"; exit 1; }
                        echo "✅ [SUCCESS] Superset integration completed"
                      fi
                    fi
                    
                  elif [[ "${DATA_ANALYSIS_PLATFORM,,}" == "metabase" ]]; then
                    echo "==========================================="
                    echo "⚙️ [PROCESS] Processing Metabase Integration"
                    echo "==========================================="
                    
                    echo "🔍 [CHECK] Validating Metabase configuration..."
                    if [[ -z $METABASE_URL || -z $METABASE_API_KEY ]]; then
                      echo "❌ [ERROR] Metabase configuration incomplete: URL or API key missing"
                      exit 1
                    else
                      echo "✅ [SUCCESS] Metabase configuration validated"
                      export DATA_ANALYSIS_PROJECT=$(grep 'DATA_ANALYSIS_PROJECT:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "❌ [ERROR] Failed to extract DATA_ANALYSIS_PROJECT"; exit 1; }
                      
                      if [[ -z "${DATA_ANALYSIS_PROJECT}" || "${DATA_ANALYSIS_PROJECT}" == "false" || "${DATA_ANALYSIS_PROJECT}" == "False" ]]; then
                        echo "⚠️ [WARNING] Metabase Metadata refresh/create is disabled, skipping..."
                      else
                        echo "⚙️ [PROCESS] Copying Metabase integration files..."
                        cp -r /usr/app/data_analysis_import/metabase/dbt2metabase/* ./ || { echo "❌ [ERROR] Failed to copy Metabase files"; exit 1; }
                        echo "⚙️ [PROCESS] Running dbt2metabase integration..."
                        /bin/bash dbt2metabase.sh 2>&1 || { echo "❌ [ERROR] Failed to run dbt2metabase integration"; exit 1; }
                        echo "✅ [SUCCESS] Metabase integration completed"
                      fi
                    fi
                    
                  elif [[ "${DATA_ANALYSIS_PLATFORM,,}" == "looker" ]]; then
                    echo "==========================================="
                    echo "⚙️ [PROCESS] Processing Looker Integration"
                    echo "==========================================="
                    
                    echo "🔍 [CHECK] Validating Looker configuration..."
                    if [[ -z $LOOKERSDK_BASE_URL || -z $LOOKERSDK_CLIENT_ID || -z $LOOKERSDK_CLIENT_SECRET ]]; then
                      echo "❌ [ERROR] Looker configuration incomplete: one or more required parameters missing"
                      exit 1
                    else
                      echo "✅ [SUCCESS] Looker configuration validated"
                      echo "📋 [CONFIG] Processing Google Looker Enterprise integration"
                      export DATA_ANALYSIS_PROJECT=$(grep 'DATA_ANALYSIS_PROJECT:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "❌ [ERROR] Failed to extract DATA_ANALYSIS_PROJECT"; exit 1; }
                      
                      if [[ -z "${DATA_ANALYSIS_PROJECT}" || "${DATA_ANALYSIS_PROJECT}" == "false" || "${DATA_ANALYSIS_PROJECT}" == "False" ]]; then
                        echo "⚠️ [WARNING] Looker Metadata refresh/create is disabled, skipping..."
                      else
                        echo "⚙️ [PROCESS] Copying Looker integration files..."
                        cp -r /usr/app/data_analysis_import/looker_enterprise/dbt2looker-bigquery/* ./ || { echo "❌ [ERROR] Failed to copy Looker files"; exit 1; }
                        echo "⚙️ [PROCESS] Running dbt2looker integration..."
                        /bin/bash dbt2looker_bigquery.sh 2>&1 || { echo "❌ [ERROR] Failed to run dbt2looker integration"; exit 1; }
                        echo "✅ [SUCCESS] Looker integration completed"
                      fi
                    fi
                  fi
                fi
                
                cd ..
                echo "✅ [SUCCESS] Completed processing folder: $folder"
              else
                echo "⚠️ [WARNING] Skipping non-directory item: $folder"
              fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "✅ [SUCCESS] DBT Project Manifest Compilation completed successfully"
            echo "📋 [CONFIG] All folders processed with commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="
        env:        
          - name: DATA_ANALYSIS_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_ANALYSIS_PLATFORM
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: PRIVATE_ACCESS_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_NAME
          - name: PRIVATE_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_TOKEN
          - name: SECRET_PACKAGE_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_PACKAGE_REPO_TOKEN_NAME
          - name: SECRET_DBT_PACKAGE_REPO_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_DBT_PACKAGE_REPO_TOKEN
          - name: LIGHTDASH_API_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LIGHTDASH_API_KEY
                optional: true
          - name: LIGHTDASH_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LIGHTDASH_URL
                optional: true
          - name: SUPERSET_BASE_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SUPERSET_BASE_URL
                optional: true
          - name: SUPERSET_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SUPERSET_USER
                optional: true
          - name: SUPERSET_USER_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SUPERSET_USER_PASSWORD
                optional: true
          - name: METABASE_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: METABASE_URL
                optional: true
          - name: METABASE_API_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: METABASE_API_KEY
                optional: true
          - name: LOOKERSDK_BASE_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKERSDK_BASE_URL
                optional: true
          - name: LOOKERSDK_CLIENT_ID
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKERSDK_CLIENT_ID
                optional: true
          - name: LOOKERSDK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKERSDK_CLIENT_SECRET
                optional: true
          - name: LOOKER_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKER_REPO_TOKEN_NAME
                optional: true
          - name: LOOKER_REPO_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKER_REPO_ACCESS_TOKEN
                optional: true
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_DATA_ANALYSIS_SA_SECRET
                optional: true
          - name: GCP_DATA_ANALYSIS_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_DATA_ANALYSIS_SA_SECRET
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          - name: SNOWFLAKE_ACCOUNT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_ACCOUNT
                optional: true
          - name: SNOWFLAKE_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_USER
                optional: true
          - name: SNOWFLAKE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSWORD
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir-temp
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-model-data-governance-metadata-import
      inputs:
        parameters:
          - name: revision
          - name: commit_short_sha
        artifacts:
          - name: repo-artifact
            path: /workdir/src
          - name: changed-files-artifact
            path: /workdir/changed_folders.txt
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_governance_collector_core_image_name}}:{{workflow.parameters.data_governance_collector_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "🚀 [INFO] DBT Project Metadata Import for Data-Governance"
            echo "📋 [CONFIG] Target: Main Branch | Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="

            cd /workdir/src

            # Scan changed folders for DBT compilation
            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "⚙️ [PROCESS] Processing folder: $folder"
                echo "==========================================="
                
                cd "$folder"
                
                # ===========================================
                # 🔐 [CONFIG] Warehouse Credentials Setup
                # ===========================================
                echo "🔍 [CHECK] Configuring warehouse credentials for platform: ${DATA_WAREHOUSE_PLATFORM,,}"
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "❌ [ERROR] Google Bigquery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "⚙️ [PROCESS] Setting up BigQuery authentication"
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                      gcloud auth activate-service-account --key-file /secrets/sa.json
                      gcloud config set project $GCP_PROJECT_NAME
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "✅ [SUCCESS] BigQuery authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "❌ [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "⚙️ [PROCESS] Setting up Snowflake authentication"
                      mkdir -p /snowsql/secrets/
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8
                      chmod 600 /snowsql/secrets/rsa_key.p8
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "✅ [SUCCESS] Snowflake authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "❌ [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  else
                    echo "✅ [SUCCESS] Redshift credentials validated"
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "❌ [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  else
                    echo "✅ [SUCCESS] Fabric credentials validated"
                  fi
                fi

                touch /workdir/import_data_governance_metadata_procedure.log
                
                echo "==========================================="
                echo "📋 [CONFIG] Data-Governance Metadata Import Setup"
                echo "==========================================="
                echo "⚙️ [PROCESS] Importing Data-Governance metadata from ${folder}"
                
                # ===========================================
                # 🔍 [CHECK] DataHub Configuration Validation
                # ===========================================
                echo "🔍 [CHECK] Validating DataHub configuration variables"
                if [[ -z ${DATAHUB_GMS_TOKEN} || -z ${DATAHUB_GMS_URL} ]]; then
                  echo "❌ [ERROR] DataHub configuration incomplete - missing required variables"
                  exit 1
                else
                  echo "✅ [SUCCESS] DataHub configuration validated"
                  
                  # ===========================================
                  # 📋 [CONFIG] Airflow Secret Variables Import
                  # ===========================================
                  echo "⚙️ [PROCESS] Importing variables from Airflow Secret configuration"
                  
                  export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export DATAHUB_ENABLED=$(grep 'DATAHUB_ENABLED:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export DBT_PROJECT_ROOT=$(pwd)
                  
                  echo "📋 [CONFIG] DBT Project: $DBT_PROJECT_NAME"
                  echo "📋 [CONFIG] DataHub Enabled: $DATAHUB_ENABLED"
                  
                  # Check and set PROJECT_LEVEL
                  echo "🔍 [CHECK] Validating PROJECT_LEVEL configuration"
                  project_level=$(grep 'PROJECT_LEVEL:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [ -z "$project_level" ]; then
                    echo "⚠️ [WARNING] PROJECT_LEVEL not specified - using default: PROD"
                    export PROJECT_LEVEL='PROD'
                  else
                    export PROJECT_LEVEL="${project_level}"
                    echo "📋 [CONFIG] PROJECT_LEVEL set to: $PROJECT_LEVEL"
                  fi

                  if [[ "$DATAHUB_ENABLED" == "true" || "$DATAHUB_ENABLED" == "True" ]]; then
                    echo "==========================================="
                    echo "🚀 [INFO] DataHub Metadata Generation & Upload Process"
                    echo "==========================================="
                    echo "✅ [SUCCESS] DataHub is enabled - proceeding with metadata processing"
                    
                    # ===========================================
                    # 🔍 [CHECK] Required Environment Variables
                    # ===========================================
                    echo "🔍 [CHECK] Validating required environment variables"
                    if [[ -z "$GIT_PROVIDER" || -z "$DATA_WAREHOUSE_PLATFORM" ]]; then
                      echo "❌ [ERROR] Missing required variables: GIT_PROVIDER and DATA_WAREHOUSE_PLATFORM must be set"
                      exit 1
                    fi
                    echo "✅ [SUCCESS] Required environment variables validated"
                    
                    # ===========================================
                    # 📋 [CONFIG] Metadata Collector Setup
                    # ===========================================
                    echo "⚙️ [PROCESS] Setting up metadata collector configuration"
                    SOURCE_PATH="/usr/app/dbt/${DATA_WAREHOUSE_PLATFORM}/${GIT_PROVIDER}/datahub-metadata-collector.dhub.${GIT_PROVIDER}.yml"
                    
                    if [[ ! -f "$SOURCE_PATH" ]]; then
                      echo "❌ [ERROR] Metadata collector file not found at expected location"
                      exit 1
                    fi
                    
                    cp "$SOURCE_PATH" ./datahub-metadata-collector.dhub.yml
                    echo "✅ [SUCCESS] Metadata collector configuration copied"

                    # ===========================================
                    # 🧹 [PROCESS] Package Cleanup
                    # ===========================================
                    echo "⚙️ [PROCESS] Cleaning up Data Quality packages from Data-Governance flow"
                    
                    # Function to check and modify packages.yml if needed
                    check_and_modify_packages() {
                      local packages_file="packages.yml"
                      
                      echo "🔍 [CHECK] Analyzing packages configuration"
                      
                      # Check if re-data package exists in packages.yml
                      if grep -q "re-data/re_data" "$packages_file"; then
                        echo "⚠️ [WARNING] Found re-data package - removing for Data-Governance flow"
                        
                        # Create a temporary file without the re-data package
                        yq 'del(.packages[] | select(.package == "re-data/re_data"))' "$packages_file" > "${packages_file}.tmp"
                        
                        # Verify the temporary file was created and has content
                        if [ -s "${packages_file}.tmp" ]; then
                          mv "${packages_file}.tmp" "$packages_file"
                          echo "✅ [SUCCESS] Re-data package removed from configuration"
                        else
                          echo "❌ [ERROR] Failed to create temporary file without re-data package"
                          exit 1
                        fi
                        
                        # Force recompilation by removing manifest and packages
                        echo "🧹 [PROCESS] Cleaning dbt project artifacts"
                        dbt clean --quiet 2>/dev/null || true
                        rm -rf ./dbt_packages/ 2>/dev/null || true
                        echo "✅ [SUCCESS] DBT project cleanup completed"
                      else
                        echo "📋 [CONFIG] No re-data package found - no cleanup needed"
                      fi
                    }
                    
                    if [ -f "packages.yml" ]; then
                      check_and_modify_packages
                    fi

                    # ===========================================
                    # 🔧 [PROCESS] DBT Dependencies & Documentation
                    # ===========================================
                    echo "==========================================="
                    echo "⚙️ [PROCESS] DBT Project Processing"
                    echo "==========================================="
                    
                    echo "⚙️ [PROCESS] Installing DBT dependencies"
                    if ! dbt deps --quiet 2>&1 | tee -a /workdir/import_data_governance_metadata_procedure.log; then
                      echo "❌ [ERROR] DBT dependencies installation failed"
                      exit 1
                    fi
                    echo "✅ [SUCCESS] DBT dependencies installed"

                    echo "⚙️ [PROCESS] Generating DBT documentation"
                    if ! dbt docs generate --exclude package:re_data --profiles-dir . --quiet 2>&1 | \
                      tee -a /workdir/import_data_governance_metadata_procedure.log; then
                      echo "❌ [ERROR] DBT documentation generation failed"
                      exit 1
                    fi
                    echo "✅ [SUCCESS] DBT documentation generated"

                    # ===========================================
                    # 📤 [PROCESS] DataHub Metadata Upload
                    # ===========================================
                    echo "==========================================="
                    echo "📤 [PROCESS] DataHub Metadata Upload"
                    echo "==========================================="
                    
                    echo "⚙️ [PROCESS] Uploading metadata to DataHub"
                    if ! datahub ingest -c datahub-metadata-collector.dhub.yml 2>&1 | tee -a /workdir/import_data_governance_metadata_procedure.log; then
                      echo "❌ [ERROR] DataHub metadata upload failed"
                      exit 1
                    fi
                    echo "✅ [SUCCESS] Metadata upload completed for Data-Governance service"
                    
                  else
                    echo "⚠️ [WARNING] DataHub is disabled - skipping metadata generation and upload process"
                  fi
                fi
                cd ..
              else
                echo "📋 [CONFIG] Skipping root directory processing: $folder"
              fi
            done < /workdir/changed_folders.txt

            # ===========================================
            # 🔍 [CHECK] Final Status Validation
            # ===========================================
            echo "==========================================="
            echo "🔍 [CHECK] Final Status Validation"
            echo "==========================================="

            # Check if log file exists (indicates processing occurred)
            if [ ! -f "/workdir/import_data_governance_metadata_procedure.log" ]; then
              echo "📋 [CONFIG] No changes detected - data governance metadata log file does not exist"
              echo "✅ [SUCCESS] Proceeding successfully without processing"
              exit 0
            fi

            # Look for critical failures in the log
            if grep -q "^FAILED" /workdir/import_data_governance_metadata_procedure.log; then
              echo "❌ [ERROR] Critical failure detected in the metadata import procedure"
              exit 1
            fi

            echo "==========================================="
            echo "✅ [SUCCESS] Data-Governance Metadata Import Completed Successfully"
            echo "==========================================="
        env:
          - name: CUSTOMER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: DATAHUB_GMS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATAHUB_GMS_TOKEN
          - name: DATAHUB_GMS_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATAHUB_GMS_URL
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: PRIVATE_ACCESS_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_NAME
          - name: PRIVATE_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_TOKEN
          - name: SECRET_PACKAGE_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_PACKAGE_REPO_TOKEN_NAME
          - name: SECRET_DBT_PACKAGE_REPO_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_DBT_PACKAGE_REPO_TOKEN
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GIT_REPO_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_REPO_URL
                optional: true
          - name: GIT_BRANCH_MAIN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_BRANCH_MAIN
                optional: true
          - name: GIT_PROVIDER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_PROVIDER
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir-temp
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-model-data-catalog-deployment
      inputs:
        parameters:
          - name: revision
          - name: repo_url
          - name: commit_short_sha
        artifacts:
          - name: repo-artifact
            path: /workdir/src
          - name: changed-files-artifact
            path: /workdir/changed_folders.txt
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_catalog_template_core_image_name}}:{{workflow.parameters.data_catalog_template_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "🚀 [INFO] DBT Project Data-Catalog Deployment"
            echo "📋 [CONFIG] Target: Main Branch"
            echo "🔗 [CONFIG] Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="

            cd /workdir/src

            # Initialize deployment log
            touch /workdir/data-catalog-deploy.log
            echo "📝 [CONFIG] Deployment log initialized: /workdir/data-catalog-deploy.log"

            echo "==========================================="
            echo "🔍 [CHECK] Processing Changed Folders"
            echo "==========================================="

            while read folder; do
              if [ -d "$folder" ]; then
                echo "⚙️ [PROCESS] Processing folder: $folder"
                cd "$folder"
                
                echo "==========================================="
                echo "🔍 [CHECK] Validating Environment Variables"
                echo "==========================================="
                
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "❌ [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                else
                  echo "✅ [SUCCESS] All required variables are defined"
                fi
                
                echo "==========================================="
                echo "⚙️ [PROCESS] Updating Configuration Variables"
                echo "==========================================="
                
                # Update TARGET variable
                echo "📋 [CONFIG] Checking TARGET configuration"
                target=$(grep 'TARGET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                if [ -z "$target" ]; then
                  echo "⚙️ [PROCESS] Adding TARGET variable"
                  DBT_PROJECT_TARGET=$(yq eval '.profile' dbt_project.yml) || { echo "❌ [ERROR] Failed to extract DBT project target"; exit 1; }
                  DBT_TARGET=$(yq eval '.'''${DBT_PROJECT_TARGET}'''.target' profiles.yml) || { echo "❌ [ERROR] Failed to extract DBT target"; exit 1; }
                  sed -i '$ a\  TARGET: '''${DBT_TARGET}'''' $AIRFLOW_SECRET_FILE_NAME || { echo "❌ [ERROR] Failed to add TARGET to secret file"; exit 1; }
                  echo "✅ [SUCCESS] TARGET variable added"
                else
                  echo "✅ [SUCCESS] TARGET variable already configured"
                fi
                
                # Update GITLINK_SECRET variable
                echo "📋 [CONFIG] Checking GITLINK_SECRET configuration"
                gitlink_secret=$(grep 'GITLINK_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                if [ -z "$gitlink_secret" ]; then
                  echo "⚙️ [PROCESS] Adding GITLINK_SECRET variable"
                  domain_and_path=$(echo {{ inputs.parameters.repo_url }} | sed -e 's/https:\/\///')
                  domain=$(echo ${domain_and_path} | cut -d "/" -f 1)
                  path=$(echo ${domain_and_path} | cut -d "/" -f 2-)
                  git_url="https://${PRIVATE_ACCESS_NAME}:${PRIVATE_ACCESS_TOKEN}@${domain}/${path}.git"
                  sed -i '$ a\  GITLINK_SECRET: '''${git_url}'''' $AIRFLOW_SECRET_FILE_NAME || { echo "❌ [ERROR] Failed to add GITLINK_SECRET to secret file"; exit 1; }
                  echo "✅ [SUCCESS] GITLINK_SECRET variable added"
                else
                  echo "✅ [SUCCESS] GITLINK_SECRET variable already configured"
                fi
                
                # Update DBT_PROJECT_OWNER variable
                echo "📋 [CONFIG] Checking DBT_PROJECT_OWNER configuration"
                dbt_project_owner=$(grep 'DBT_PROJECT_OWNER:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                if [ -z "$dbt_project_owner" ]; then
                  echo "⚙️ [PROCESS] Adding DBT_PROJECT_OWNER variable"
                  sed -i '$ a\  DBT_PROJECT_OWNER: '''${CUSTOMER}'''' $AIRFLOW_SECRET_FILE_NAME || { echo "❌ [ERROR] Failed to add DBT_PROJECT_OWNER to secret file"; exit 1; }
                  echo "✅ [SUCCESS] DBT_PROJECT_OWNER variable added"
                else
                  echo "✅ [SUCCESS] DBT_PROJECT_OWNER variable already configured"
                fi
                
                echo "==========================================="
                echo "⚙️ [PROCESS] Preparing Variables for Compilation"
                echo "==========================================="
                
                # Export required variables
                export DBT_REPO_NAME=$(grep 'DBT_PROJECT_DIRECTORY:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export DBT_PROJECT_OWNER=$(grep 'DBT_PROJECT_OWNER:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export TARGET=$(grep 'TARGET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export GITLINK_SECRET=$(grep 'GITLINK_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export PROJECT_LEVEL=$(grep 'PROJECT_LEVEL:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export DBT_PROJECT_VERSION=$(grep 'version:' dbt_project.yml | tail -n1 | awk '{print $2}' | tr -d "'" )
                
                # Set DATA_CATALOG to True for Fast.BI Deployment
                DATA_CATALOG=True
                echo "📋 [CONFIG] DATA_CATALOG set to True for Fast.BI deployment"
                
                echo "==========================================="
                echo "🚀 [INFO] Starting Compilation & Deployment"
                echo "==========================================="
                
                # Copy required template files
                echo "⚙️ [PROCESS] Copying template files"
                cp /usr/app/dbt/dbt-docs-template.yaml ./ || { echo "❌ [ERROR] Failed to copy dbt-docs-template.yaml"; exit 1; }
                cp /usr/app/dbt/template.dbt-nginx-cm.yaml ./ || { echo "❌ [ERROR] Failed to copy template.dbt-nginx-cm.yaml"; exit 1; }
                cp /usr/app/dbt/compile_deployment.sh ./ || { echo "❌ [ERROR] Failed to copy compile_deployment.sh"; exit 1; }
                echo "✅ [SUCCESS] Template files copied"
                
                if [[ "$DATA_CATALOG" == "true" || "$DATA_CATALOG" == "True" ]]; then
                  echo "🚀 [INFO] Data Catalog is enabled. Starting deployment process"
                  bash compile_deployment.sh 2>&1 | tee -a /workdir/data-catalog-deploy.log || true
                  echo "✅ [SUCCESS] Deployment process completed for ${DBT_PROJECT_NAME}"
                else
                  echo "⚠️ [WARNING] Data Catalog is disabled. Skipping deployment process"
                fi
                
                echo "==========================================="
                echo "✅ [SUCCESS] DBT Docs - Data-Catalog Deployment completed for ${DBT_PROJECT_NAME}"
                echo "==========================================="
                
                cd ..
              else
                echo "⚠️ [WARNING] Skipping non-directory item: $folder"
              fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "🔍 [CHECK] Validating Deployment Results"
            echo "==========================================="

            error_found=0

            if [ ! -f "/workdir/data-catalog-deploy.log" ]; then
              echo "✅ [SUCCESS] No changes detected - data catalog deployment log file does not exist. Proceeding successfully."
            else
              echo "⚙️ [PROCESS] Analyzing deployment log for errors"
              while read -r line; do
                # Check for errors excluding the "NotFound" scenario
                if echo "$line" | grep -Ei "fail|error|forbidden|denied" | grep -vi "NotFound" > /dev/null; then
                  echo "❌ [ERROR] Error found in Data-Catalog deployment log: $line"
                  error_found=1
                fi
              done < /workdir/data-catalog-deploy.log

              if [ $error_found -eq 1 ]; then
                echo "❌ [ERROR] Data-Catalog deployments failed. Please check the logs for details."
                exit 1
              else
                echo "✅ [SUCCESS] Data-Catalog deployment completed successfully."
              fi
            fi

            echo "==========================================="
            echo "🎉 [SUCCESS] DBT Project Data-Catalog Deployment Complete"
            echo "📋 [CONFIG] All folders processed successfully"
            echo "==========================================="
        env:
          - name: NAMESPACE
            value: "data-catalog"
          - name: DBT_DOCS_CORE_IMAGE
            value: "4fastbi/data-catalog-core:v0.1.0"
          - name: CUSTOMER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: DOMAIN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DOMAIN
          - name: DCDQ_METADATACOLLECTOR_API_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DCDQ_METADATACOLLECTOR_API_URL
          - name: DCDQ_METADATACOLLECTOR_API_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DCDQ_METADATACOLLECTOR_API_TOKEN
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: PRIVATE_ACCESS_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_NAME
          - name: PRIVATE_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_TOKEN
          - name: SECRET_PACKAGE_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_PACKAGE_REPO_TOKEN_NAME
          - name: SECRET_DBT_PACKAGE_REPO_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_DBT_PACKAGE_REPO_TOKEN
        resources: {}
        volumeMounts:
          - name: workdir-temp
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-model-data-quality-deployment
      inputs:
        parameters:
          - name: revision
          - name: repo_url
          - name: commit_short_sha
        artifacts:
          - name: repo-artifact
            path: /workdir/src
          - name: changed-files-artifact
            path: /workdir/changed_folders.txt
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_quality_template_core_image_name}}:{{workflow.parameters.data_quality_template_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "🚀 [INFO] DBT Project Data-Quality Deployment"
            echo "📋 [CONFIG] Target: Main Branch"
            echo "🔗 [CONFIG] Commit: {{inputs.parameters.commit_short_sha}}"
            echo "==========================================="

            cd /workdir/src

            echo "⚙️ [PROCESS] Starting DBT Project Data-Quality deploy procedure on Main with Commit: {{inputs.parameters.commit_short_sha}}"

            # Initialize deployment log
            touch /workdir/data-quality-deploy.log

            echo "🔍 [CHECK] Scanning changed folders for DBT compilation..."

            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "📁 [PROCESS] Processing folder: $folder"
                echo "==========================================="
                
                cd "$folder"
                
                echo "⚙️ [PROCESS] Updating configuration variables..."
                
                # Update TARGET variable
                echo "📋 [CONFIG] Checking TARGET configuration..."
                target=$(grep 'TARGET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                if [ -z "$target" ]; then
                  echo "⚙️ [PROCESS] TARGET is empty - adding variable..."
                  DBT_PROJECT_TARGET=$(yq eval '.profile' dbt_project.yml) || { echo "❌ [ERROR] Failed to extract DBT project target"; exit 1; }
                  DBT_TARGET=$(yq eval '.'''${DBT_PROJECT_TARGET}'''.target' profiles.yml) || { echo "❌ [ERROR] Failed to extract DBT target"; exit 1; }
                  sed -i '$ a\  TARGET: '''${DBT_TARGET}'''' $AIRFLOW_SECRET_FILE_NAME || { echo "❌ [ERROR] Failed to add TARGET to secret file"; exit 1; }
                  echo "✅ [SUCCESS] TARGET variable added"
                else
                  echo "✅ [SUCCESS] TARGET is already configured"
                fi
                
                # Update GITLINK_SECRET variable
                echo "📋 [CONFIG] Checking GITLINK_SECRET configuration..."
                gitlink_secret=$(grep 'GITLINK_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                if [ -z "$gitlink_secret" ]; then
                  echo "⚙️ [PROCESS] GITLINK_SECRET is empty - adding variable..."
                  domain_and_path=$(echo {{ inputs.parameters.repo_url }}| sed -e 's/https:\/\///')
                  domain=$(echo ${domain_and_path} | cut -d "/" -f 1)
                  path=$(echo ${domain_and_path} | cut -d "/" -f 2-)
                  git_url="https://${PRIVATE_ACCESS_NAME}:${PRIVATE_ACCESS_TOKEN}@${domain}/${path}.git"
                  sed -i '$ a\  GITLINK_SECRET: '''${git_url}'''' $AIRFLOW_SECRET_FILE_NAME || { echo "❌ [ERROR] Failed to add GITLINK_SECRET to secret file"; exit 1; }
                  echo "✅ [SUCCESS] GITLINK_SECRET variable added"
                else
                  echo "✅ [SUCCESS] GITLINK_SECRET already exists"
                fi
                
                # Update DBT_PROJECT_OWNER variable
                echo "📋 [CONFIG] Checking DBT_PROJECT_OWNER configuration..."
                dbt_project_owner=$(grep 'DBT_PROJECT_OWNER:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                if [ -z "$dbt_project_owner" ]; then
                  echo "⚙️ [PROCESS] DBT_PROJECT_OWNER is empty - adding variable..."
                  sed -i '$ a\  DBT_PROJECT_OWNER: '''${CUSTOMER}'''' $AIRFLOW_SECRET_FILE_NAME || { echo "❌ [ERROR] Failed to add DBT_PROJECT_OWNER to secret file"; exit 1; }
                  echo "✅ [SUCCESS] DBT_PROJECT_OWNER variable added"
                else
                  echo "✅ [SUCCESS] DBT_PROJECT_OWNER already exists"
                fi
                
                echo "⚙️ [PROCESS] Preparing variables for compilation..."
                
                # Export required variables
                export DBT_REPO_NAME=$(grep 'DBT_PROJECT_DIRECTORY:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export DBT_PROJECT_OWNER=$(grep 'DBT_PROJECT_OWNER:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export TARGET=$(grep 'TARGET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export GITLINK_SECRET=$(grep 'GITLINK_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export PROJECT_LEVEL=$(grep 'PROJECT_LEVEL:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export DBT_PROJECT_VERSION=$(grep 'version:' dbt_project.yml | tail -n1 | awk '{print $2}' | tr -d "'" )
                export DATA_QUALITY=$(grep 'DATA_QUALITY:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{ print $2 }' | tr -d "'\"")
                
                echo "✅ [SUCCESS] Variables prepared for compilation"
                
                echo "🚀 [INFO] Starting compilation and deployment..."
                
                # Copy required template files
                cp /usr/app/dbt/re-data-template.yaml ./ || { echo "❌ [ERROR] Failed to copy re-data template"; exit 1; }
                cp /usr/app/dbt/template.dbt-nginx-cm.yaml ./ || { echo "❌ [ERROR] Failed to copy nginx template"; exit 1; }
                cp /usr/app/dbt/compile_deployment.sh ./ || { echo "❌ [ERROR] Failed to copy deployment script"; exit 1; }
                
                if [[ -z "$DATA_QUALITY" || "$DATA_QUALITY" == "false" || "$DATA_QUALITY" == "False" ]]; then
                  echo "⚠️ [WARNING] Data Quality Service is disabled in DBT Project"
                else
                  echo "⚙️ [PROCESS] Executing data quality deployment..."
                  bash compile_deployment.sh 2>&1 | tee -a /workdir/data-quality-deploy.log || true
                  echo "✅ [SUCCESS] Data quality deployment script completed"
                fi
                
                echo "✅ [SUCCESS] Completed re_data - Data-Quality Deployment procedure for ${DBT_PROJECT_NAME}"
                cd ..
              else
                echo "⚠️ [WARNING] Skipping non-directory item: $folder"
              fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "🔍 [CHECK] Analyzing deployment results..."
            echo "==========================================="

            error_found=0

            if [ ! -f "/workdir/data-quality-deploy.log" ]; then
              echo "✅ [SUCCESS] No changes detected - data quality deployment log file does not exist. Proceeding successfully."
            else
              echo "📋 [CONFIG] Checking deployment log for errors..."
              
              while read -r line; do
                # Check for errors excluding the "NotFound" scenario
                if echo "$line" | grep -Ei "fail|error|forbidden|denied" | grep -vi "NotFound" > /dev/null; then
                  echo "❌ [ERROR] Error found in Data-Quality deployment log: $line"
                  error_found=1
                fi
              done < /workdir/data-quality-deploy.log

              if [ $error_found -eq 1 ]; then
                echo "❌ [ERROR] Data-Quality deployments failed. Please check the logs for details."
                exit 1
              else
                echo "✅ [SUCCESS] Data-Quality deployment completed successfully."
              fi
            fi

            echo "==========================================="
            echo "🎉 [SUCCESS] DBT Project Data-Quality Deployment Complete"
            echo "📋 [CONFIG] All folders processed successfully"
            echo "==========================================="
        env:
          - name: NAMESPACE
            value: "data-quality"
          - name: DBT_REDATA_CORE_IMAGE
            value: "4fastbi/data-quality-core:v0.1.0"
          - name: CUSTOMER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: DOMAIN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DOMAIN
          - name: DCDQ_METADATACOLLECTOR_API_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DCDQ_METADATACOLLECTOR_API_URL
          - name: DCDQ_METADATACOLLECTOR_API_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DCDQ_METADATACOLLECTOR_API_TOKEN
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: PRIVATE_ACCESS_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_NAME
          - name: PRIVATE_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_TOKEN
          - name: SECRET_PACKAGE_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_PACKAGE_REPO_TOKEN_NAME
          - name: SECRET_DBT_PACKAGE_REPO_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_DBT_PACKAGE_REPO_TOKEN
        resources: {}
        volumeMounts:
          - name: workdir-temp
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: k8s-pvc-cleanup
      inputs: {}
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: bitnami/kubectl
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash

            # ===========================================
            # Kubernetes PVC Cleanup Script
            # ===========================================
            # Purpose: Clean up Persistent Volume Claims
            #          for Argo Workflows in cicd-workflows namespace
            # ===========================================



            echo "🚀 [INFO] Starting Kubernetes PVC cleanup process"
            echo "=========================================="

            # Configuration
            PVC_NAME="{{workflow.name}}-workdir"
            NAMESPACE="cicd-workflows"

            echo "📋 [CONFIG] PVC Name: $PVC_NAME"
            echo "📋 [CONFIG] Target Namespace: $NAMESPACE"
            echo ""

            echo "⚙️ [PROCESS] Attempting to delete PVC $PVC_NAME in namespace $NAMESPACE..."

            # Execute the PVC deletion command (keeping exact same logic)
            kubectl delete pvc $PVC_NAME --namespace $NAMESPACE --ignore-not-found 2>/dev/null

            # Check the result and provide appropriate feedback
            if [ $? -eq 0 ]; then
              echo "✅ [SUCCESS] Successfully deleted PVC $PVC_NAME or it did not exist"
            else
              echo "❌ [ERROR] Failed to delete PVC $PVC_NAME. Check logs for details"
              exit 1
            fi

            echo ""
            echo "✅ [SUCCESS] Kubernetes PVC cleanup process completed"
            echo "=========================================="
        resources: {}
    - name: check-task-status
      inputs:
        parameters:
          - name: workflow_status
          - name: argo_main_address
      script:
        image: alpine
        command:
          - sh
          - '-c'
        args:
          - |
            #!/bin/sh

            # ===========================================
            # Argo Workflow Task Status Checker
            # ===========================================



            echo "🚀 [INFO] Starting Argo Workflow Task Status Check"
            echo "=========================================="

            # ===========================================
            # Dependencies Installation
            # ===========================================

            echo "⚙️ [PROCESS] Installing required dependencies..."
            apk add --no-cache jq >/dev/null 2>&1 || { 
              echo "❌ [ERROR] Failed to install jq dependency"
              exit 1
            }
            echo "✅ [SUCCESS] Dependencies installed successfully"

            # ===========================================
            # Workflow Status Processing
            # ===========================================

            echo "🔍 [CHECK] Parsing workflow failure information..."
            workflow_status_output='{{inputs.parameters.workflow_status}}'

            # Validate workflow status input
            if [ -z "$workflow_status_output" ] || [ "$workflow_status_output" = "null" ]; then
              echo "✅ [SUCCESS] No failure information available - All tasks completed successfully"
              echo "=========================================="
              echo "🎉 [COMPLETE] Task status check completed successfully"
              exit 0
            fi

            # ===========================================
            # Argo Workflow URL Generation
            # ===========================================

            echo "📋 [CONFIG] Generating Argo Workflow URL..."
            ARGO_URL=$(echo "{{inputs.parameters.argo_main_address}}" | sed 's/:443//')
            echo "🔗 [INFO] Argo Workflow URL:"
            echo "https://${ARGO_URL}/workflows/cicd-workflows/{{workflow.name}}?tab=workflow&uid={{workflow.uid}}"

            # ===========================================
            # Failure Analysis
            # ===========================================

            echo "🔍 [CHECK] Analyzing task execution results..."
            failed_tasks=$(echo "$workflow_status_output" | jq -r 'fromjson | .[] | select(.phase == "Failed") | "Task: \(.displayName)\nStatus: \(.phase)\nMessage: \(.message)\nFinished at: \(.finishedAt)\n"')

            if [ -n "$failed_tasks" ]; then
              echo "❌ [ERROR] Failed tasks detected:"
              echo "------------------------------------------"
              echo "$failed_tasks"
              echo "------------------------------------------"
              
              failure_count=$(echo "$workflow_status_output" | jq 'fromjson | [ .[] | select(.phase == "Failed") ] | length')
              echo "❌ [ERROR] Pipeline failed due to $failure_count failure(s)"
              echo "=========================================="
              echo "💥 [FAILED] Task status check completed with failures"
              exit 1
            else
              echo "✅ [SUCCESS] No failures detected - All tasks completed successfully"
              echo "=========================================="
              echo "🎉 [COMPLETE] Task status check completed successfully"
              exit 0
            fi
        resources: {}
  entrypoint: main
  arguments:
    parameters:
      - name: storage_class
        value: "standard"
      - name: docker_image_repository_hub
        value: docker.io
      - name: docker_image_repository
        value: 4fastbi
#fast.bi CICD Pipeline Images
      - name: data_analysis_import_core_image_name
        value: data-analysis-import-core
      - name: data_analysis_import_core_image_tag
        value: v0.1.0
      - name: data_orchestrator_core_image_name
        value: data-orchestrator-core
      - name: data_orchestrator_core_image_tag
        value: v0.1.0
      - name: dbt_workflow_core_image_name
        value: dbt-workflow-core
      - name: dbt_workflow_core_image_tag
        value: v0.1.0
      - name: data_governance_collector_core_image_name
        value: data-governance-collector-core
      - name: data_governance_collector_core_image_tag
        value: v0.1.0
      - name: data_catalog_template_core_image_name
        value: data-catalog-template-core
      - name: data_catalog_template_core_image_tag
        value: v0.1.0
      - name: data_quality_template_core_image_name
        value: data-quality-template-core
      - name: data_quality_template_core_image_tag
        value: v0.1.0
      - name: repo_url
      - name: revision
      - name: commit_short_sha
      - name: commit_before_sha
      - name: git_username
      - name: git_token
      - name: argo_main_address
  volumes:
    - name: fastbi-argo-workflows-secrets
      secret:
        secretName: fastbi-argo-workflows-secrets
    - name: gcp-sa-secret-file
      emptyDir: {}
    - name: workdir-temp
      emptyDir: {}
    - name: ssh-keys
      secret:
        secretName: argo-workflows-git-secret
        defaultMode: 384
        items:
          - key: private
            path: id_rsa
            mode: 384
          - key: public
            path: id_rsa.pub
            mode: 420
  volumeClaimTemplates:
    - metadata:
        name: workdir
        creationTimestamp: null
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
        storageClassName: "{{workflow.parameters.storage_class}}"
      status: {}
  onExit: exit-handler
  ttlStrategy:
    secondsAfterCompletion: 86400
  activeDeadlineSeconds: 7200
  podGC:
    strategy: OnPodCompletion
  podSpecPatch: |
    containers:
      - name: main
        imagePullPolicy: Always
        env:
          - name: PYTHONWARNINGS
            value: "ignore"
