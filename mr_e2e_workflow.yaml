apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: fastbi-mr-e2e-workflow-
spec:
  templates:
    - name: exit-handler
      inputs: {}
      outputs: {}
      metadata: {}
      steps:
        - - name: check-all-task-status
            template: check-task-status
            arguments:
              parameters:
                - name: workflow_status
                  value: '{{workflow.failures}}'
                - name: argo_main_address
                  value: '{{workflow.parameters.argo_main_address}}'
          - name: k8s-pvc-cleanup
            template: k8s-pvc-cleanup
            arguments: {}
            when: >-
              {{workflow.status}} != Succeeded
    - name: main
      inputs:
        parameters:
          - name: repo_url
          - name: revision
          - name: branch
          - name: commit_short_sha
          - name: git_username
          - name: git_token
          - name: target_branch
          - name: ci_job_id
      outputs: {}
      metadata: {}
      steps:
        - - name: checkout
            template: checkout
            arguments:
              parameters:
                - name: repo_url
                  value: '{{inputs.parameters.repo_url}}'
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: git_username
                  value: '{{inputs.parameters.git_username}}'
                - name: git_token
                  value: '{{inputs.parameters.git_token}}'
        - - name: detect-changes
            template: detect-changes
            arguments:
              parameters:
                - name: revision
                  value: '{{inputs.parameters.revision}}'
                - name: target_branch
                  value: '{{inputs.parameters.target_branch}}'
        - - name: lint-yaml-e2e
            template: lint-yaml-e2e
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
        - - name: data-orchestration-variables-import-e2e
            template: data-orchestration-variables-import-e2e
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
                - name: target_branch
                  value: '{{inputs.parameters.target_branch}}'
                - name: repo_url
                  value: '{{inputs.parameters.repo_url}}'
        - - name: compile-dbt-manifest-e2e
            template: compile-dbt-manifest-e2e
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
        - - name: compile-dag-e2e
            template: compile-dag-e2e
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
                - name: ci_job_id
                  value: '{{inputs.parameters.ci_job_id}}'
                - name: target_branch
                  value: '{{inputs.parameters.target_branch}}'
        - - name: data-model-e2e-dag-run-trigger
            template: data-model-e2e-dag-run-trigger
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
        - - name: data-model-e2e-dag-status-check
            template: data-model-e2e-dag-status-check
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
                - name: dag_run_id
                  value: '{{steps.data-model-e2e-dag-run-trigger.outputs.parameters.dag_run_id}}'
            continueOn:
              failed: true
        - - name: validating-da-dependencies-e2e
            template: validating-da-dependencies-e2e
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
        - - name: data-model-e2e-data-tagging
            template: data-model-e2e-data-tagging
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
        - - name: data-model-e2e-data-cleanning
            template: data-model-e2e-data-cleanning
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
            when: "{{steps.data-model-e2e-dag-status-check.status}} == Succeeded"
          - name: data-model-e2e-data-orchestration-cleanup
            template: data-model-e2e-data-orchestration-cleanup
            arguments:
              parameters:
                - name: branch
                  value: '{{inputs.parameters.branch}}'
                - name: commit_short_sha
                  value: '{{inputs.parameters.commit_short_sha}}'
            when: "{{steps.data-model-e2e-dag-status-check.status}} == Succeeded"
        - - name: k8s-pvc-cleanup
            template: k8s-pvc-cleanup
            arguments: {}
            when: >-
              {{workflow.status}} != Succeeded || {{workflow.status}} ==
              Succeeded
    - name: checkout
      inputs:
        parameters:
          - name: repo_url
          - name: revision
          - name: git_username
          - name: git_token
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: alpine/git
        command:
          - sh
          - '-c'
        args:
          - >
            #!/bin/sh


            # ===========================================
            # üöÄ Repository Checkout Script
            # ===========================================


            echo "==========================================="
            echo "üöÄ [INFO] Starting repository checkout process"
            echo "==========================================="

            # ===========================================
            # üìã [CONFIG] System Setup
            # ===========================================

            echo "‚öôÔ∏è [PROCESS] Installing required packages..."

            apk add --no-cache jq || { echo "‚ùå [ERROR] Failed to install jq package"; exit 1; }

            # ===========================================
            # üîç [CHECK] Environment Validation
            # ===========================================

            echo "üîç [CHECK] Validating repository configuration..."

            REPO_URL="{{inputs.parameters.repo_url}}"
            REPO_URL_NO_PROTOCOL="${REPO_URL#https://}"

            echo "üìã [CONFIG] Repository URL configured: ${REPO_URL%/*}/[REPO_NAME]"

            # ===========================================
            # ‚öôÔ∏è [PROCESS] Git Configuration
            # ===========================================

            echo "‚öôÔ∏è [PROCESS] Configuring git settings..."

            git config --global advice.detachedHead false || { echo "‚ùå [ERROR] Failed to configure global git settings"; exit 1; }

            # ===========================================
            # üöÄ [INFO] Repository Operations
            # ===========================================

            echo "üöÄ [INFO] Cloning repository..."

            git clone https://{{inputs.parameters.git_username}}:{{inputs.parameters.git_token}}@${REPO_URL_NO_PROTOCOL} /workdir/src || { echo "‚ùå [ERROR] Failed to clone repository"; exit 1; }

            echo "‚öôÔ∏è [PROCESS] Navigating to repository directory..."

            cd /workdir/src || { echo "‚ùå [ERROR] Failed to navigate to repository directory"; exit 1; }

            echo "‚öôÔ∏è [PROCESS] Configuring local git settings..."

            git config advice.detachedHead false || { echo "‚ùå [ERROR] Failed to configure local git settings"; exit 1; }

            echo "‚öôÔ∏è [PROCESS] Checking out specified revision..."

            git checkout {{inputs.parameters.revision}} || { echo "‚ùå [ERROR] Failed to checkout revision {{inputs.parameters.revision}}"; exit 1; }

            # ===========================================
            # ‚úÖ [SUCCESS] Completion
            # ===========================================

            echo "==========================================="

            echo "‚úÖ [SUCCESS] Repository cloned and checked out successfully"
            
            echo "üìã [INFO] Working directory: /workdir/src"
            
            echo "üìã [INFO] Revision: {{inputs.parameters.revision}}"
            
            echo "==========================================="
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
    - name: detect-changes
      inputs:
        parameters:
          - name: revision
          - name: target_branch
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: alpine/git
        command:
          - sh
          - '-c'
        args:
          - |
            #!/bin/sh

            # ===========================================
            # Git Change Detection Script
            # Detects changed folders between Git branches
            # ===========================================


            echo "üöÄ [INFO] Starting Git change detection process"
            echo "=========================================="

            # ===========================================
            # Environment Setup
            # ===========================================

            echo "üìã [CONFIG] Setting up working directory"
            cd /workdir/src || { echo "‚ùå [ERROR] Failed to change to /workdir/src directory"; exit 1; }

            # ===========================================
            # Git Operations
            # ===========================================

            echo "‚öôÔ∏è [PROCESS] Fetching latest changes from remote repository"
            git fetch --all || { echo "‚ùå [ERROR] Failed to fetch from remote repository"; exit 1; }

            echo "‚öôÔ∏è [PROCESS] Analyzing changes between branches"
            echo "üîç [CHECK] Comparing {{inputs.parameters.target_branch}} with {{inputs.parameters.revision}}"

            git diff --name-only remotes/origin/{{inputs.parameters.target_branch}}...{{inputs.parameters.revision}} \
              | cut -d'/' -f1 \
              | sort \
              | uniq > /workdir/changed_folders.txt || { echo "‚ùå [ERROR] Failed to identify changed folders"; exit 1; }

            # ===========================================
            # Filter Hidden Folders
            # ===========================================

            echo "‚öôÔ∏è [PROCESS] Filtering out hidden folders and files"

            > /workdir/temp.txt
            while read -r line; do
              # Remove leading whitespace (if any)
              line=$(echo "$line" | sed 's/^[ \t]*//')
              # Get the first character
              first_char=$(echo "$line" | cut -c1)
              if [ "$first_char" != "." ]; then
                echo "$line" >> /workdir/temp.txt
              fi
            done < /workdir/changed_folders.txt

            mv /workdir/temp.txt /workdir/changed_folders.txt

            # ===========================================
            # Fallback Configuration
            # ===========================================

            if [ ! -s /workdir/changed_folders.txt ]; then
              echo "‚ö†Ô∏è [WARNING] No changed folders detected, using default fallback"
              echo "README.md" > /workdir/changed_folders.txt
            fi

            # ===========================================
            # Results Summary
            # ===========================================

            echo "‚úÖ [SUCCESS] Change detection completed successfully"
            echo "üìã Folders changed: $(cat /workdir/changed_folders.txt)"
            echo "üìã [CONFIG] Changed folders identified:"
            echo "=========================================="

            # Display count of changed folders instead of full content for security
            folder_count=$(wc -l < /workdir/changed_folders.txt)
            echo "üîç [CHECK] Total changed folders detected: $folder_count"

            echo "‚úÖ [SUCCESS] Git change detection process completed"
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
    - name: lint-yaml-e2e
      inputs:
        parameters:
          - name: branch
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: cytopia/yamllint
        command:
          - sh
          - '-c'
        args:
          - |
            #!/bin/bash

            # =============================================================================
            # DBT YAML File Linting Script for E2E Environment
            # =============================================================================



            echo "üöÄ [INFO] ==========================================="
            echo "üöÄ [INFO] Starting DBT YML File Linting Procedure"
            echo "üöÄ [INFO] Branch: {{inputs.parameters.branch}}"
            echo "üöÄ [INFO] Environment: E2E"
            echo "üöÄ [INFO] ==========================================="

            # =============================================================================
            # INITIALIZATION & VALIDATION
            # =============================================================================

            echo "üìã [CONFIG] Changing to source directory"
            cd /workdir/src || { echo "‚ùå [ERROR] Failed to change to source directory"; exit 1; }

            # =============================================================================
            # CHANGED FOLDERS PROCESSING
            # =============================================================================

            echo "üîç [CHECK] Validating changed folders file"
            if [ -f /workdir/changed_folders.txt ]; then
              echo "‚úÖ [SUCCESS] Changed folders file found - proceeding with YAML linting"
              
              echo "‚öôÔ∏è [PROCESS] Scanning YAML files in changed folders"
              while read -r folder; do
                if [ -d "$folder" ]; then
                  echo "‚öôÔ∏è [PROCESS] Linting YAML in folder: $folder"
                  cd "$folder" || { echo "‚ùå [ERROR] Failed to change to folder: $folder"; exit 1; }
                  yamllint -c yamllint-config.yaml -f colored --no-warnings . 2>&1 | tee -a ../yamllint_lint.log || true
                  cd .. || { echo "‚ùå [ERROR] Failed to return to parent directory"; exit 1; }
                else
                  echo "‚ö†Ô∏è [WARNING] Skipping root directory processing"
                fi
              done < /workdir/changed_folders.txt
            else
              echo "‚ö†Ô∏è [WARNING] No changed folders file found - skipping linting step"
            fi

            # =============================================================================
            # YAML LINT LOG ANALYSIS
            # =============================================================================

            echo "üîç [CHECK] Analyzing YAML lint results"
            if [ -f ./yamllint_lint.log ]; then
              echo "üìã [CONFIG] YAML lint log file found - checking for errors"
              
              echo "‚öôÔ∏è [PROCESS] Scanning log file for error patterns"
              while read -r line; do
                echo "${line}" | grep -i "error" > /dev/null
                if [ $? -eq 0 ]; then
                  echo "‚ùå [ERROR] YAML linting error detected in log file"
                  echo "‚ùå [ERROR] Error line: ${line}"
                  echo "‚ùå [ERROR] YAML linting failed - exiting with error code 1"
                  sleep 5
                  exit 1
                fi
              done < ./yamllint_lint.log
              
              echo "‚úÖ [SUCCESS] No errors found in YAML lint log"
            else
              echo "üìã [CONFIG] No YAML lint log file found - no errors to report"
            fi

            # =============================================================================
            # COMPLETION
            # =============================================================================

            echo "‚úÖ [SUCCESS] ==========================================="
            echo "‚úÖ [SUCCESS] DBT YML File Linting Procedure Completed"
            echo "‚úÖ [SUCCESS] All YAML files validated successfully"
            echo "‚úÖ [SUCCESS] ==========================================="
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
    - name: validating-da-dependencies-e2e
      inputs:
        parameters:
          - name: branch
          - name: commit_short_sha
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_analysis_import_core_image_name}}:{{workflow.parameters.data_analysis_import_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "üöÄ [INFO] Data Analysis Dependencies Validation"
            echo "üìã [CONFIG] Branch: {{inputs.parameters.branch}}"
            echo "==========================================="

            # Default parameter
            cd /workdir/src || { echo "‚ùå [ERROR] Failed to change to /workdir/src"; exit 1; }

            echo "üîç [CHECK] Scanning Data Analysis dependencies in changed folders"

            # Scan Data Analysis dependencies in changed folders
            while read folder; do
                if [ -d "$folder" ]; then
                    echo "==========================================="
                    echo "‚öôÔ∏è [PROCESS] Validating Data Analysis dependencies in $folder"
                    echo "==========================================="
                    
                    cd "$folder" || { echo "‚ùå [ERROR] Failed to change to directory $folder"; exit 1; }
                    
                    # Set warehouse credentials
                    echo "üìã [CONFIG] Configuring data warehouse credentials"
                    
                    if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                        if [[ -z "$GCP_SA_SECRET" ]]; then
                            echo "‚ùå [ERROR] Google Bigquery credentials are missing: Service Account JSON not provided"
                            exit 1
                        else
                            if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                                echo "üîê [CONFIG] Setting up Google Cloud authentication"
                                echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json || { echo "‚ùå [ERROR] Failed to decode service account secret"; exit 1; }
                                gcloud auth activate-service-account --key-file /secrets/sa.json || { echo "‚ùå [ERROR] Failed to activate service account"; exit 1; }
                                gcloud config set project $GCP_PROJECT_NAME || { echo "‚ùå [ERROR] Failed to set GCP project"; exit 1; }
                                export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                                echo "‚úÖ [SUCCESS] Google Cloud authentication configured"
                            fi
                        fi
                    fi
                    
                    if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                        if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" || -z "$SNOWFLAKE_ACCOUNT" || -z "$SNOWFLAKE_USER" || -z "$SNOWFLAKE_PASSWORD" ]]; then
                            echo "‚ùå [ERROR] Snowflake credentials are missing: Private Key and Passphrase Or Username and Password not provided"
                            exit 1
                        else
                            if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                                echo "üîê [CONFIG] Setting up Snowflake authentication"
                                mkdir -p /snowsql/secrets/ || { echo "‚ùå [ERROR] Failed to create Snowflake secrets directory"; exit 1; }
                                echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8 || { echo "‚ùå [ERROR] Failed to write Snowflake private key"; exit 1; }
                                chmod 600 /snowsql/secrets/rsa_key.p8 || { echo "‚ùå [ERROR] Failed to set private key permissions"; exit 1; }
                                export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                                echo "‚úÖ [SUCCESS] Snowflake authentication configured"
                            fi
                        fi
                    fi
                    
                    if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                        if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                            echo "‚ùå [ERROR] Redshift credentials are missing: username and/or password not provided"
                            exit 1
                        fi
                        echo "‚úÖ [SUCCESS] Redshift credentials validated"
                    fi
                    
                    if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                        if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                            echo "‚ùå [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                            exit 1
                        fi
                        echo "‚úÖ [SUCCESS] Fabric credentials validated"
                    fi
                    
                    echo "üîç [CHECK] Starting BI Model dbt metadata validation"
                    echo "üîç [CHECK] Validating Data Analysis platform configuration"
                    
                    if [[ -z $DATA_ANALYSIS_PLATFORM ]]; then
                        echo "‚ö†Ô∏è [WARNING] DATA_ANALYSIS_PLATFORM is not defined, skipping validation"
                    else
                        echo "‚úÖ [SUCCESS] DATA_ANALYSIS_PLATFORM is defined: ${DATA_ANALYSIS_PLATFORM}"
                        
                        if [[ "${DATA_ANALYSIS_PLATFORM,,}" == "lightdash" ]]; then
                            echo "‚öôÔ∏è [PROCESS] Processing Lightdash platform configuration"
                            if [[ -z $LIGHTDASH_URL || -z $LIGHTDASH_API_KEY ]]; then
                                echo "‚ùå [ERROR] Lightdash configuration incomplete: URL or API key missing"
                                exit 1
                            else
                                echo "‚úÖ [SUCCESS] Lightdash configuration validated"
                                echo "üìã [CONFIG] Extracting project configuration from secrets"
                                
                                export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract DBT_PROJECT_NAME"; exit 1; }
                                export DATA_ANALYSIS_PROJECT=$(grep 'DATA_ANALYSIS_PROJECT:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract DATA_ANALYSIS_PROJECT"; exit 1; }
                                export TARGET=$(grep 'TARGET:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract TARGET"; exit 1; }
                                export GIT_BRANCH=$(grep 'GIT_BRANCH:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract GIT_BRANCH"; exit 1; }
                                
                                if [[ -z "${DATA_ANALYSIS_PROJECT}" || "${DATA_ANALYSIS_PROJECT}" == "false" || "${DATA_ANALYSIS_PROJECT}" == "False" ]]; then
                                    echo "‚ö†Ô∏è [WARNING] Lightdash Metadata Validation is disabled, skipping"
                                else
                                    echo "‚öôÔ∏è [PROCESS] Running Lightdash metadata validation"
                                    cp -r /usr/app/data_analysis_import/lightdash/dbt2lightdash/* ./ || { echo "‚ùå [ERROR] Failed to copy Lightdash validation files"; exit 1; }
                                    /bin/bash dbt2lightdash.sh 2>&1 | tee -a /workdir/data_analysis_validating_procedure.log || { echo "‚ùå [ERROR] Lightdash validation failed"; exit 1; }
                                    echo "‚úÖ [SUCCESS] Lightdash metadata validation completed"
                                fi
                            fi
                        elif [[ "${DATA_ANALYSIS_PLATFORM,,}" == "superset" ]]; then
                            echo "‚öôÔ∏è [PROCESS] Processing Superset platform configuration"
                            if [[ -z $SUPERSET_BASE_URL || -z $SUPERSET_USER || -z $SUPERSET_USER_PASSWORD ]]; then
                                echo "‚ùå [ERROR] Superset configuration incomplete: Base URL, user, or password missing"
                                exit 1
                            else
                                echo "‚úÖ [SUCCESS] Superset configuration validated"
                                echo "üìã [CONFIG] Extracting project configuration from secrets"
                                
                                export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract DBT_PROJECT_NAME"; exit 1; }
                                export DATA_ANALYSIS_PROJECT=$(grep 'DATA_ANALYSIS_PROJECT:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract DATA_ANALYSIS_PROJECT"; exit 1; }
                                export TARGET=$(grep 'TARGET:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract TARGET"; exit 1; }
                                export GIT_BRANCH=$(grep 'GIT_BRANCH:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract GIT_BRANCH"; exit 1; }
                                
                                # Get the first top-level key (project name) dynamically
                                SNOWFLAKE_PROJECT_NAME=$(yq -r 'keys | .[0]' profiles.yml) || { echo "‚ùå [ERROR] Failed to extract Snowflake project name"; exit 1; }
                                
                                # Get the default target for that project
                                DEFAULT_TARGET=$(yq -r ".${SNOWFLAKE_PROJECT_NAME}.target" profiles.yml) || { echo "‚ùå [ERROR] Failed to extract default target"; exit 1; }
                                export SNOWFLAKE_WAREHOUSE=$(yq -r ".${SNOWFLAKE_PROJECT_NAME}.outputs.${DEFAULT_TARGET}.warehouse" profiles.yml) || { echo "‚ùå [ERROR] Failed to extract Snowflake warehouse"; exit 1; }
                                export SNOWFLAKE_ROLE=$(yq -r ".${SNOWFLAKE_PROJECT_NAME}.outputs.${DEFAULT_TARGET}.role" profiles.yml) || { echo "‚ùå [ERROR] Failed to extract Snowflake role"; exit 1; }
                                echo "‚úÖ [SUCCESS] Snowflake configuration exported"
                                
                                if [[ -z "${DATA_ANALYSIS_PROJECT}" || "${DATA_ANALYSIS_PROJECT}" == "false" || "${DATA_ANALYSIS_PROJECT}" == "False" ]]; then
                                    echo "‚ö†Ô∏è [WARNING] Superset Metadata Validation is disabled, skipping"
                                else
                                    echo "‚öôÔ∏è [PROCESS] Running Superset metadata validation"
                                    cp -r /usr/app/data_analysis_import/superset/dbt2superset/* ./ || { echo "‚ùå [ERROR] Failed to copy Superset validation files"; exit 1; }
                                    /bin/bash dbt2superset.sh 2>&1 | tee -a /workdir/data_analysis_validating_procedure.log || { echo "‚ùå [ERROR] Superset validation failed"; exit 1; }
                                    echo "‚úÖ [SUCCESS] Superset metadata validation completed"
                                fi
                            fi
                        elif [[ "${DATA_ANALYSIS_PLATFORM,,}" == "metabase" ]]; then
                            echo "‚öôÔ∏è [PROCESS] Processing Metabase platform configuration"
                            if [[ -z $METABASE_URL || -z $METABASE_API_KEY ]]; then
                                echo "‚ùå [ERROR] Metabase configuration incomplete: URL or API key missing"
                                exit 1
                            else
                                echo "‚úÖ [SUCCESS] Metabase configuration validated"
                                echo "üìã [CONFIG] Extracting project configuration from secrets"
                                
                                export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract DBT_PROJECT_NAME"; exit 1; }
                                export DATA_ANALYSIS_PROJECT=$(grep 'DATA_ANALYSIS_PROJECT:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract DATA_ANALYSIS_PROJECT"; exit 1; }
                                export TARGET=$(grep 'TARGET:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract TARGET"; exit 1; }
                                export GIT_BRANCH=$(grep 'GIT_BRANCH:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract GIT_BRANCH"; exit 1; }
                                
                                if [[ -z "${DATA_ANALYSIS_PROJECT}" || "${DATA_ANALYSIS_PROJECT}" == "false" || "${DATA_ANALYSIS_PROJECT}" == "False" ]]; then
                                    echo "‚ö†Ô∏è [WARNING] Metabase Metadata Validation is disabled, skipping"
                                else
                                    echo "‚öôÔ∏è [PROCESS] Running Metabase metadata validation"
                                    cp -r /usr/app/data_analysis_import/metabase/dbt2metabase/* ./ || { echo "‚ùå [ERROR] Failed to copy Metabase validation files"; exit 1; }
                                    /bin/bash dbt2metabase.sh 2>&1 | tee -a /workdir/data_analysis_validating_procedure.log || { echo "‚ùå [ERROR] Metabase validation failed"; exit 1; }
                                    echo "‚úÖ [SUCCESS] Metabase metadata validation completed"
                                fi
                            fi
                        elif [[ "${DATA_ANALYSIS_PLATFORM,,}" == "looker" ]]; then
                            echo "‚öôÔ∏è [PROCESS] Processing Google Looker Enterprise configuration"
                            if [[ -z $LOOKERSDK_BASE_URL || -z $LOOKERSDK_CLIENT_ID || -z $LOOKERSDK_CLIENT_SECRET ]]; then
                                echo "‚ùå [ERROR] Looker configuration incomplete: Base URL, client ID, or client secret missing"
                                exit 1
                            else
                                echo "‚úÖ [SUCCESS] Looker configuration validated"
                                echo "üìã [CONFIG] Extracting project configuration from secrets"
                                
                                export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract DBT_PROJECT_NAME"; exit 1; }
                                export DATA_ANALYSIS_PROJECT=$(grep 'DATA_ANALYSIS_PROJECT:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract DATA_ANALYSIS_PROJECT"; exit 1; }
                                export TARGET=$(grep 'TARGET:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract TARGET"; exit 1; }
                                export GIT_BRANCH=$(grep 'GIT_BRANCH:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract GIT_BRANCH"; exit 1; }
                                
                                if [[ -z "${DATA_ANALYSIS_PROJECT}" || "${DATA_ANALYSIS_PROJECT}" == "false" || "${DATA_ANALYSIS_PROJECT}" == "False" ]]; then
                                    echo "‚ö†Ô∏è [WARNING] Looker Metadata Validation is disabled, skipping"
                                else
                                    echo "‚öôÔ∏è [PROCESS] Running Looker metadata validation"
                                    cp -r /usr/app/data_analysis_import/looker_enterprise/dbt2looker-bigquery/* ./ || { echo "‚ùå [ERROR] Failed to copy Looker validation files"; exit 1; }
                                    /bin/bash dbt2looker_bigquery.sh 2>&1 | tee -a /workdir/data_analysis_validating_procedure.log || { echo "‚ùå [ERROR] Looker validation failed"; exit 1; }
                                    echo "‚úÖ [SUCCESS] Looker metadata validation completed"
                                fi
                            fi
                        fi
                    fi
                    
                    cd .. || { echo "‚ùå [ERROR] Failed to return to parent directory"; exit 1; }
                else
                    echo "‚ö†Ô∏è [WARNING] Working on root - skipping validation"
                fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "üîç [CHECK] Analyzing validation results"
            echo "==========================================="

            error_found=0
            warning_count=0

            if [ ! -f "/workdir/data_analysis_validating_procedure.log" ]; then
                echo "‚úÖ [SUCCESS] No Data Analysis metadata validations were executed - log file not found. Skipping analysis."
            else
                while read -r line; do
                    [[ -z "$line" ]] && continue

                    # First check if this is a known good pattern (whitelist)
                    if echo "$line" | grep -E "is up to date$|Field '.*' not in|Done\. PASS=|commands may fail, consider upgrading your CLI" > /dev/null; then
                        continue
                    fi

                    if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                        ((warning_count++))
                        continue
                    fi

                    # Then check for actual error patterns
                    if echo "$line" | grep -Ei '\b(fail|error|forbidden|denied)\b' > /dev/null \
                        && ! echo "$line" | grep -E "'.*ERROR.*'" > /dev/null; then
                        echo "‚ùå [ERROR] Error found in E2E Test DA validating procedure, log file line: $line"
                        error_found=1
                    fi
                done < /workdir/data_analysis_validating_procedure.log
            fi

            echo "==========================================="
            if [ $error_found -eq 1 ]; then
                echo "‚ùå [ERROR] E2E Test Data Analysis dependencies validation procedure has failed"
                echo "üìã [INFO] Please check the logs for details"
                exit 1
            else
                echo "‚úÖ [SUCCESS] Data Analysis dependencies validation completed successfully"
                if [ $warning_count -gt 0 ]; then
                    echo "‚ö†Ô∏è [WARNING] Found $warning_count warnings during validation"
                fi
            fi
            echo "==========================================="
        env:
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_DATA_ANALYSIS_SA_SECRET
                optional: true
          - name: GCP_DATA_ANALYSIS_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_DATA_ANALYSIS_SA_SECRET
                optional: true
          - name: DATA_ANALYSIS_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_ANALYSIS_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: LIGHTDASH_API_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LIGHTDASH_API_KEY
                optional: true
          - name: LIGHTDASH_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LIGHTDASH_URL
                optional: true
          - name: SUPERSET_BASE_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SUPERSET_BASE_URL
                optional: true
          - name: SUPERSET_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SUPERSET_USER
                optional: true
          - name: SUPERSET_USER_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SUPERSET_USER_PASSWORD
                optional: true
          - name: METABASE_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: METABASE_URL
                optional: true
          - name: METABASE_API_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: METABASE_API_KEY
                optional: true
          - name: LOOKERSDK_BASE_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKERSDK_BASE_URL
                optional: true
          - name: LOOKERSDK_CLIENT_ID
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKERSDK_CLIENT_ID
                optional: true
          - name: LOOKERSDK_CLIENT_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKERSDK_CLIENT_SECRET
                optional: true
          - name: LOOKER_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKER_REPO_TOKEN_NAME
                optional: true
          - name: LOOKER_REPO_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: LOOKER_REPO_ACCESS_TOKEN
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          - name: SNOWFLAKE_ACCOUNT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_ACCOUNT
                optional: true
          - name: SNOWFLAKE_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_USER
                optional: true
          - name: SNOWFLAKE_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSWORD
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-orchestration-variables-import-e2e
      inputs:
        parameters:
          - name: branch
          - name: commit_short_sha
          - name: target_branch
          - name: repo_url
      outputs:
        parameters:
          - name: airflow_secret_key
            valueFrom:
              path: /workdir/airflow_secrets.yaml
          - name: airflow_dag_id
            valueFrom:
              path: /workdir/airflow_dags.yaml
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_orchestrator_core_image_name}}:{{workflow.parameters.data_orchestrator_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash
            # Exit on error, treat unset variables as errors, and return value of pipeline is the status of the last command


            echo "==========================================="
            echo "üöÄ [INFO] DBT Project Data Orchestration Variables Import"
            echo "üìã [CONFIG] Branch: {{inputs.parameters.branch}}"
            echo "==========================================="

            cd /workdir/src

            # Create YAML files for the next steps
            echo "‚öôÔ∏è [PROCESS] Creating configuration files..."
            cat > /workdir/airflow_secrets.yaml << EOF
            secrets:
              platform: {}
            EOF

            cat > /workdir/airflow_dags.yaml << EOF
            dags: []
            EOF

            echo "‚úÖ [SUCCESS] Configuration files created"

            # Scan changed folders for DBT compilation
            echo "==========================================="
            echo "üîç [CHECK] Processing changed folders for DBT compilation"
            echo "==========================================="

            while read folder; do
              if [ -d "$folder" ]; then
                echo "‚öôÔ∏è [PROCESS] Collecting Variables from $folder"
                cd "$folder"
                
                # Set warehouse credentials
                echo "üîç [CHECK] Validating data warehouse credentials..."
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "‚ùå [ERROR] Google Bigquery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "‚öôÔ∏è [PROCESS] Configuring Google Cloud authentication..."
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                      gcloud auth activate-service-account --key-file /secrets/sa.json
                      gcloud config set project $GCP_PROJECT_NAME
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "‚úÖ [SUCCESS] Google Cloud authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "‚ùå [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "‚öôÔ∏è [PROCESS] Configuring Snowflake authentication..."
                      mkdir -p /snowsql/secrets/
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8
                      chmod 600 /snowsql/secrets/rsa_key.p8
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "‚úÖ [SUCCESS] Snowflake authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "‚ùå [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "‚ùå [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  fi
                fi
                
                echo "‚úÖ [SUCCESS] Data warehouse credentials validated"
                
                echo "üîç [CHECK] Validating required environment variables..."
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "‚ùå [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                else  
                  echo "‚úÖ [SUCCESS] All required variables are defined"
                  echo "‚öôÔ∏è [PROCESS] Updating variables for Pull/Merge Request E2E testing"

                  branch_name="{{inputs.parameters.branch}}"
                  if [[ "$branch_name" =~ [^a-zA-Z0-9_] ]]; then
                    new_branch_name=$(echo "$branch_name" | sed 's/[^a-zA-Z0-9_]/_/g')
                    echo "‚ö†Ô∏è [WARNING] Branch name contains symbols. Changing branch name to: ${new_branch_name}"
                    branch_name="${new_branch_name}"
                  fi

                  echo "==========================================="
                  echo "‚öôÔ∏è [PROCESS] Updating Airflow Secret Configuration"
                  echo "==========================================="

                  # Update Airflow Secret file
                  echo "‚öôÔ∏è [PROCESS] Updating Airflow_SECRET_NAME..."
                  secret_key=$(grep -Eo '^[a-zA-Z0-9_]+:' $AIRFLOW_SECRET_FILE_NAME | head -n 1 | tr -d ':')
                  sed -i "s/^${secret_key}:/${secret_key}_${branch_name}:/" $AIRFLOW_SECRET_FILE_NAME

                  echo "‚öôÔ∏è [PROCESS] Updating DAG_ID..."
                  dag_id=$(grep 'DAG_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  sed -i "s/DAG_ID.*/DAG_ID: '${dag_id}_${branch_name}'/" $AIRFLOW_SECRET_FILE_NAME

                  echo "‚öôÔ∏è [PROCESS] Updating MANIFEST_NAME..."
                  manifest_name=$(grep 'MANIFEST_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  sed -i "s/MANIFEST_NAME.*/MANIFEST_NAME: '${manifest_name}_${branch_name}'/" $AIRFLOW_SECRET_FILE_NAME

                  echo "‚öôÔ∏è [PROCESS] Updating TARGET..."
                  target=$(grep 'TARGET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [ -z "$target" ]; then
                    echo "üìã [CONFIG] TARGET: is empty - adding variable"
                    sed -i "$ a\  TARGET: '${DBT_TARGET}'" $AIRFLOW_SECRET_FILE_NAME
                  else
                    sed -i "s/TARGET.*/TARGET: '${DBT_TARGET}'/" $AIRFLOW_SECRET_FILE_NAME
                  fi

                  echo "‚öôÔ∏è [PROCESS] Updating DATA_QUALITY..."
                  data_quality=$(grep 'DATA_QUALITY:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [ -z "$data_quality" ]; then
                    echo "üìã [CONFIG] DATA_QUALITY: is empty - adding variable"
                    sed -i "$ a\  DATA_QUALITY: 'False'" $AIRFLOW_SECRET_FILE_NAME
                  else
                    sed -i "s/^  DATA_QUALITY:.*/  DATA_QUALITY: 'False'/" $AIRFLOW_SECRET_FILE_NAME
                  fi

                  echo "‚öôÔ∏è [PROCESS] Updating AIRBYTE_REPLICATION_FLAG..."
                  airbyte_replication=$(grep 'AIRBYTE_REPLICATION_FLAG:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [ -z "$airbyte_replication" ]; then
                    echo "üìã [CONFIG] AIRBYTE_REPLICATION_FLAG: is empty - not need to add/update variable"
                  else
                    sed -i "s/AIRBYTE_REPLICATION_FLAG:.*/AIRBYTE_REPLICATION_FLAG: 'False'/" "$AIRFLOW_SECRET_FILE_NAME"
                  fi

                  echo "‚öôÔ∏è [PROCESS] Updating DBT_SOURCE..."
                  dbt_source=$(grep 'DBT_SOURCE:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [ -z "$dbt_source" ]; then
                    echo "üìã [CONFIG] DBT_SOURCE: is empty - not need to add/update variable"
                  else
                    sed -i "s/DBT_SOURCE:.*/DBT_SOURCE: 'False'/" "$AIRFLOW_SECRET_FILE_NAME"
                  fi

                  operator=$(grep 'OPERATOR:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [[ "$operator" == "api" ]]; then
                    echo "‚öôÔ∏è [PROCESS] Updating DBT_WORKLOAD_API_CONNECTION_ID..."
                    dbt_workload_connection_id=$(grep 'DBT_WORKLOAD_API_CONNECTION_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                    if [ -z "$dbt_workload_connection_id" ]; then
                      echo "üìã [CONFIG] DBT_WORKLOAD_API_CONNECTION_ID: is empty - adding variable"
                      sed -i "$ a\  DBT_WORKLOAD_API_CONNECTION_ID: '${dag_id,,}_${branch_name,,}'" $AIRFLOW_SECRET_FILE_NAME
                    else
                      sed -i "s/DBT_WORKLOAD_API_CONNECTION_ID.*/DBT_WORKLOAD_API_CONNECTION_ID: '${dag_id,,}_${branch_name,,}'/" $AIRFLOW_SECRET_FILE_NAME
                    fi
                    echo "‚öôÔ∏è [PROCESS] Updating NAMESPACE..."
                    namespace_name=$(grep 'NAMESPACE:' "$AIRFLOW_SECRET_FILE_NAME" | tail -n1 | awk '{print $2}' | tr -d "'" )
                    lowercase_branch_name=${branch_name,,}
                    sed -i 's/NAMESPACE.*/NAMESPACE: '''"${namespace_name}_${lowercase_branch_name}"'''/' "$AIRFLOW_SECRET_FILE_NAME"
                  fi

                  echo "‚öôÔ∏è [PROCESS] Updating GITLINK_SECRET..."
                  gitlink_secret=$(grep 'GITLINK_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [ -z "$gitlink_secret" ]; then
                    echo "üìã [CONFIG] GITLINK_SECRET: is empty - adding variable"
                    domain_and_path=$(echo {{inputs.parameters.repo_url}} | sed -e 's/https:\/\///')
                    domain=$(echo ${domain_and_path} | cut -d "/" -f 1)
                    path=$(echo ${domain_and_path} | cut -d "/" -f 2-)
                    if [[ -z $PRIVATE_ACCESS_NAME || -z $PRIVATE_ACCESS_TOKEN ]]; then
                      echo "‚ùå [ERROR] One or more Git variables are undefined"
                      exit 1
                    fi
                    git_url="https://${PRIVATE_ACCESS_NAME}:${PRIVATE_ACCESS_TOKEN}@${domain}/${path}.git"
                    sed -i "$ a\  GITLINK_SECRET: '${git_url}'" $AIRFLOW_SECRET_FILE_NAME
                  else
                    echo "‚úÖ [SUCCESS] GITLINK_SECRET: exists - no update needed"
                  fi

                  echo "‚öôÔ∏è [PROCESS] Updating GIT_BRANCH..."
                  git_branch=$(grep 'GIT_BRANCH:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [ -z "$git_branch" ]; then
                    echo "üìã [CONFIG] GIT_BRANCH: is empty - adding variable"
                    sed -i "$ a\  GIT_BRANCH: '${branch_name}'" $AIRFLOW_SECRET_FILE_NAME
                  else
                    sed -i "s/GIT_BRANCH.*/GIT_BRANCH: '${branch_name}'/" $AIRFLOW_SECRET_FILE_NAME
                  fi

                  echo "‚öôÔ∏è [PROCESS] Updating SECRET_PACKAGE_REPO_TOKEN_NAME..."
                  package_token_name=$(grep 'SECRET_PACKAGE_REPO_TOKEN_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [ -z "$package_token_name" ]; then
                    echo "üìã [CONFIG] SECRET_PACKAGE_REPO_TOKEN_NAME: is empty - adding variable"
                    sed -i "$ a\  SECRET_PACKAGE_REPO_TOKEN_NAME: '${SECRET_PACKAGE_REPO_TOKEN_NAME}'" $AIRFLOW_SECRET_FILE_NAME
                  else
                    sed -i "s/SECRET_PACKAGE_REPO_TOKEN_NAME.*/SECRET_PACKAGE_REPO_TOKEN_NAME: '${SECRET_PACKAGE_REPO_TOKEN_NAME}'/" $AIRFLOW_SECRET_FILE_NAME
                  fi

                  echo "‚öôÔ∏è [PROCESS] Updating SECRET_DBT_PACKAGE_REPO_TOKEN..."
                  package_token=$(grep 'SECRET_DBT_PACKAGE_REPO_TOKEN:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [ -z "$package_token" ]; then
                    echo "üìã [CONFIG] SECRET_DBT_PACKAGE_REPO_TOKEN: is empty - adding variable"
                    sed -i "$ a\  SECRET_DBT_PACKAGE_REPO_TOKEN: '${SECRET_DBT_PACKAGE_REPO_TOKEN}'" $AIRFLOW_SECRET_FILE_NAME
                  else
                    sed -i "s/SECRET_DBT_PACKAGE_REPO_TOKEN.*/SECRET_DBT_PACKAGE_REPO_TOKEN: '${SECRET_DBT_PACKAGE_REPO_TOKEN}'/" $AIRFLOW_SECRET_FILE_NAME
                  fi

                  echo "‚öôÔ∏è [PROCESS] Updating DAG_SCHEDULE_INTERVAL..."
                  last_day=$(date -d "$(date +'%Y')-12-31" +%F)
                  tomorrow=$(date -d "+1 day" +%F)
                  cron_date=$(date -d "$tomorrow" '+%M %H %d %m *')
                  # Calculate time +2 hours from now
                  schedule_time=$(date -d "+2 hours" '+%M %H')
                  # Extract minute and hour
                  minute=$(echo "$schedule_time" | awk '{print $1}')
                  hour=$(echo "$schedule_time" | awk '{print $2}')
                  # Build the cron expression to run at the specified time every day
                  cron_date="$minute $hour * * *"
                  sed -i "s/DAG_SCHEDULE_INTERVAL.*/DAG_SCHEDULE_INTERVAL: '$cron_date'/" "$AIRFLOW_SECRET_FILE_NAME"

                  echo "‚öôÔ∏è [PROCESS] Updating DATAHUB_ENABLED..."
                  sed -i "s/DATAHUB_ENABLED.*/DATAHUB_ENABLED: 'False'/" "$AIRFLOW_SECRET_FILE_NAME"

                  echo "‚öôÔ∏è [PROCESS] Updating GKE Operator dependencies: SA_SECRET..."
                  operator=$(grep 'OPERATOR:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                  if [[ "$operator" == "gke" ]]; then
                    sa_secret=$(grep 'SA_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                    if [ -z "$sa_secret" ]; then
                      echo "üìã [CONFIG] SA_SECRET: is empty - adding variable"
                      sed -i "$ a\  SA_SECRET: '${GCP_SA_SECRET}'" $AIRFLOW_SECRET_FILE_NAME
                    else
                      sed -i "s/SA_SECRET.*/SA_SECRET: '${GCP_SA_SECRET}'/" $AIRFLOW_SECRET_FILE_NAME
                    fi
                  fi

                  echo "‚úÖ [SUCCESS] Airflow Secret configuration updated"

                  echo "==========================================="
                  echo "‚öôÔ∏è [PROCESS] Importing Variables from Airflow Secret yml file"
                  echo "==========================================="

                  export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" )
                  export DAG_OPERATOR=$(grep 'OPERATOR:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export DBT_REPO_NAME=$(grep 'DBT_PROJECT_DIRECTORY:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export GIT_BRANCH=$(grep 'GIT_BRANCH:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export GITLINK_SECRET=$(grep 'GITLINK_SECRET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export SECRET_DBT_PACKAGE_REPO_TOKEN=$(grep 'SECRET_DBT_PACKAGE_REPO_TOKEN:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export SECRET_PACKAGE_REPO_TOKEN_NAME=$(grep 'SECRET_PACKAGE_REPO_TOKEN_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export PROJECT_ID=$GCP_PROJECT_NAME
                  export LOCATION=$GCP_LOCATION
                  export SERVICE_ACCOUNT_EMAIL=$GCP_SA_EMAIL
                  export DAG_ID=$(grep 'DAG_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export DYNAMIC_DAG=$(grep 'DYNAMIC_DAG:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )

                  if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                    echo "‚öôÔ∏è [PROCESS] Dynamic DAG is enabled. Updating multiple DAG configurations for E2E testing..."
                    DYNAMIC_AIRFLOW_SECRET_FILE_NAME="dynamic_dag_config.yml"
                    
                    # Get all DAG configurations
                    mapfile -t dag_name_list < <(yq -r '(.dags | keys)[]' "$DYNAMIC_AIRFLOW_SECRET_FILE_NAME")
                    
                    # Update each DAG's configuration
                    for dag_name in "${dag_name_list[@]}"; do
                      if [ -z "$dag_name" ]; then
                        continue
                      fi
                      
                      echo "‚öôÔ∏è [PROCESS] Updating E2E test parameters for DAG: $dag_name"
                      
                      # Create a temporary file with the updates
                      temp_file=$(mktemp)
                      # Update all values using standard yq syntax
                      yq -y ".dags.\"${dag_name}\".DATA_QUALITY = \"False\" | \
                            .dags.\"${dag_name}\".AIRBYTE_REPLICATION_FLAG = \"False\" | \
                            .dags.\"${dag_name}\".DBT_SOURCE = \"False\" | \
                            .dags.\"${dag_name}\".DAG_SCHEDULE_INTERVAL = \"$cron_date\" | \
                            .dags.\"${dag_name}\".DATAHUB_ENABLED = \"False\"" "$DYNAMIC_AIRFLOW_SECRET_FILE_NAME" > "$temp_file"
                      mv "$temp_file" "$DYNAMIC_AIRFLOW_SECRET_FILE_NAME"
                    done
                    
                    echo "‚úÖ [SUCCESS] Updated E2E test parameters in $DYNAMIC_AIRFLOW_SECRET_FILE_NAME"
                  fi
                  
                  # Function to create and upload dynamic DAG variables
                  create_dynamic_dag_variables() {
                    local base_secret_file="$1"
                    local dynamic_config_file="dynamic_dag_config.yml"
                    local branch_name="{{inputs.parameters.branch}}"
                    
                    if [ ! -f "$dynamic_config_file" ]; then
                      echo "‚ùå [ERROR] $dynamic_config_file not found"
                      return 1
                    fi

                    # Get all DAG configurations
                    mapfile -t dag_name_list < <(yq -r '(.dags | keys)[]' "$dynamic_config_file")

                    if [ ${#dag_name_list[@]} -eq 0 ]; then
                      echo "‚ùå [ERROR] No DAG configurations found or extracted from '$dynamic_config_file' under .dags"
                      return 1
                    fi

                    # Read the base variables file content
                    local base_vars_content=$(cat "$base_secret_file")
                    echo "üîç [CHECK] Base variables file content validated"

                    # Initialize airflow_secrets.yaml if it doesn't exist
                    if [ ! -f "/workdir/airflow_secrets.yaml" ]; then
                      echo "üìã [CONFIG] Initializing airflow_secrets.yaml"
                      echo "secrets:" > /workdir/airflow_secrets.yaml
                      echo "  platform:" >> /workdir/airflow_secrets.yaml
                      echo "    airflow: []" >> /workdir/airflow_secrets.yaml
                    fi

                    # Process each DAG
                    for dag_name_from_list in "${dag_name_list[@]}"; do
                      if [ -z "$dag_name_from_list" ]; then
                        echo "‚ö†Ô∏è [WARNING] Skipping empty DAG name found in the list from config"
                        continue
                      fi

                      echo "‚öôÔ∏è [PROCESS] Processing DAG configuration for: $dag_name_from_list"
                      
                      # Create individual variable file for this DAG
                      local individual_vars_file="dbt_airflow_variables_${dag_name_from_list}.yml"
                      echo "üìã [CONFIG] Creating variable file: $individual_vars_file"
                      
                      # Get the original base key from base_file_path
                      local original_base_key=$(yq -r 'keys | .[0]' "$base_secret_file")
                      if [ -z "$original_base_key" ] || [ "$original_base_key" == "null" ]; then
                        echo "‚ùå [ERROR] Could not determine original base key from $base_secret_file"
                        continue
                      fi

                      # Construct the new root key - Remove the branch name from original key and append DAG name and branch
                      local base_key_without_branch=$(echo "$original_base_key" | sed -E "s/_${branch_name}$//")
                      local new_root_key="${base_key_without_branch}_${dag_name_from_list^^}_${branch_name}"
                      
                      # Create the initial structure in final_output_for_dag by assigning new key and deleting old
                      yq ".\"${new_root_key}\" = .\"${original_base_key}\" | del(.\"${original_base_key}\")" "$base_secret_file" > "$individual_vars_file"
                      
                      # Get the specific override configurations for the current DAG from dynamic_config_file_path
                      local overrides_for_dag_path=".dags.${dag_name_from_list}"
                      local temp_dag_overrides_file="temp_overrides_${dag_name_from_list}.yml"
                      
                      # Check if overrides exist for this DAG
                      if ! yq -e "${overrides_for_dag_path}" "$dynamic_config_file" > "$temp_dag_overrides_file" 2>/dev/null || [ ! -s "$temp_dag_overrides_file" ]; then
                        echo "‚ö†Ô∏è [WARNING] No overrides found or failed to extract for '${dag_name_from_list}' from '${dynamic_config_file}'. Using base values"
                        rm -f "$temp_dag_overrides_file"
                      else
                        # Merge these overrides into the new_root_key structure
                        yq -y ".\"${new_root_key}\" = (.\"${new_root_key}\" * $(cat "$temp_dag_overrides_file"))" "$individual_vars_file" > "${individual_vars_file}.tmp"
                        if [ $? -ne 0 ]; then
                          echo "‚ùå [ERROR] Failed to merge overrides into '$individual_vars_file' for key '${new_root_key}'"
                          rm -f "$temp_dag_overrides_file"
                          continue
                        fi
                        mv "${individual_vars_file}.tmp" "$individual_vars_file"
                        rm -f "$temp_dag_overrides_file"
                      fi

                      # Overwrite DAG_ID and MANIFEST_NAME
                      yq -y ".\"${new_root_key}\".DAG_ID = \"${DAG_OPERATOR}_operator_${DBT_PROJECT_NAME}_${dag_name_from_list}_${branch_name}\" | \
                        .\"${new_root_key}\".MANIFEST_NAME = \"${DBT_PROJECT_NAME}_manifest_${branch_name}\"" \
                        "$individual_vars_file" > "${individual_vars_file}.tmp" && mv "${individual_vars_file}.tmp" "$individual_vars_file"
                      
                      echo "‚úÖ [SUCCESS] Generated variables file for $individual_vars_file"
                      
                      # Create DAG configuration
                      local dag_config="{\"project\": \"$DBT_PROJECT_NAME\", \"dag_id\": \"${DAG_OPERATOR}_operator_${DBT_PROJECT_NAME}_${dag_name_from_list}_${branch_name}\", \"variables_file\": \"$individual_vars_file\"}"
                      
                      # Add to YAML file using temporary file
                      local temp_file=$(mktemp)
                      yq -y ".dags += [$dag_config]" /workdir/airflow_dags.yaml > "$temp_file"
                      mv "$temp_file" /workdir/airflow_dags.yaml
                      
                      # Add secret key to YAML file using temporary file
                      temp_file=$(mktemp)
                      yq -y ".secrets.platform.\"${PLATFORM,,}\" = (.secrets.platform.\"${PLATFORM,,}\" // []) + [\"${new_root_key}\"]" /workdir/airflow_secrets.yaml > "$temp_file"
                      mv "$temp_file" /workdir/airflow_secrets.yaml
                      
                      # Import variables for this DAG
                      echo "‚öôÔ∏è [PROCESS] Importing variables for DAG: $dag_name_from_list"
                      export AIRFLOW_SECRET_FILE_NAME="$individual_vars_file"
                      python /usr/app/tsb-data-orchestrator-core/create_variables_airflow_bauth.op.py 2>&1 | tee -a /workdir/create_variables_airflow_bauth.log || true
                      
                      # Save the secret key and DAG ID
                      echo "${PLATFORM,,}=${new_root_key}" >> /workdir/airflow_secret_key.log
                      echo "${DAG_OPERATOR}_operator_${DBT_PROJECT_NAME}_${dag_name_from_list}_${branch_name}" >> /workdir/airflow_dag_id.log
                    done
                    
                    # Restore original AIRFLOW_SECRET_FILE_NAME
                    export AIRFLOW_SECRET_FILE_NAME="$base_secret_file"
                  }

                  touch /workdir/create_variables_airflow_bauth.log

                  echo "==========================================="
                  echo "üîç [CHECK] Platform Configuration"
                  echo "==========================================="
                  
                  echo "üìã [CONFIG] Platform: ${PLATFORM}"
                  if [ "${PLATFORM,,}" == "composer" ]; then
                    echo "‚öôÔ∏è [PROCESS] Platform Composer detected"
                    echo "üîç [CHECK] Validating Required Composer env Variables..."
                    if [[ -z $AIRFLOW_URL || -z $AIRFLOW_SECRET_FILE_NAME || -z $PLATFORM || -z $project_id || -z $location || -z $composer_environment_name || -z $service_account_email ]]; then
                      echo "‚ùå [ERROR] One or more variables are undefined"
                      exit 1
                    else  
                      echo "‚úÖ [SUCCESS] All variables are defined"
                      # Deploy DBT API Service if Operator is API.
                      if [[ "$DAG_OPERATOR" == "api" ]]; then
                        echo "==========================================="
                        echo "‚öôÔ∏è [PROCESS] Deploying DBT API Service"
                        echo "==========================================="
                        export HTTPS_ENABLED="true"
                        export PHASE_ENVIRONMENT="e2e"
                        export WORKER_NUM="1"
                        export MAX_REQUESTS="5"
                        export CPU_REQUEST="1500m"
                        export MEMORY_REQUEST="2Gi"
                        export APP_VERSION="{{inputs.parameters.commit_short_sha}}"
                        
                        /bin/bash -c "/usr/app/tsb-data-orchestrator-core/deploy_dbt_api_service.sh" 2>&1 | tee -a /workdir/deploy_dbt_api_service.log || true
                        # Source the environment file to get the variables
                        source "${DBT_PROJECT_NAME}_${APP_VERSION:-deploy}_env.sh"
                        echo "‚öôÔ∏è [PROCESS] Updating DBT_WORKLOAD_API_CONNECTION_ID..."
                        dbt_workload_connection_id=$(grep 'DBT_WORKLOAD_API_CONNECTION_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                        if [ -z "$dbt_workload_connection_id" ]; then
                          echo "üìã [CONFIG] DBT_WORKLOAD_API_CONNECTION_ID: is empty - adding variable"
                          sed -i "$ a\  DBT_WORKLOAD_API_CONNECTION_ID: '$DBT_WORKLOAD_API_CONNECTION_ID'" $AIRFLOW_SECRET_FILE_NAME
                        else
                          sed -i "s/DBT_WORKLOAD_API_CONNECTION_ID.*/DBT_WORKLOAD_API_CONNECTION_ID: '$DBT_WORKLOAD_API_CONNECTION_ID'/" $AIRFLOW_SECRET_FILE_NAME
                        fi
                        echo "‚öôÔ∏è [PROCESS] Updating DBT_WORKLOAD_API_RELEASE..."
                        dbt_workload_release=$(grep 'DBT_WORKLOAD_API_RELEASE:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                        if [ -z "$dbt_workload_release" ]; then
                          echo "üìã [CONFIG] DBT_WORKLOAD_API_RELEASE: is empty - adding variable"
                          sed -i "$ a\  DBT_WORKLOAD_API_RELEASE: '$DBT_WORKLOAD_API_RELEASE'" $AIRFLOW_SECRET_FILE_NAME
                        fi
                        echo "DAG_OPERATOR=${DAG_OPERATOR}" >> /workdir/deploy_dbt_api_parameters.log
                        echo "DBT_WORKLOAD_API_RELEASE=${DBT_WORKLOAD_API_RELEASE}" >> /workdir/deploy_dbt_api_parameters.log
                        echo "DBT_WORKLOAD_API_CONNECTION_ID=${DBT_WORKLOAD_API_CONNECTION_ID}" >> /workdir/deploy_dbt_api_parameters.log
                      fi

                      echo "==========================================="
                      echo "‚öôÔ∏è [PROCESS] Importing Secrets to Airflow vault"
                      echo "==========================================="
                      if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                        echo "‚öôÔ∏è [PROCESS] Dynamic DAG is enabled. Creating multiple DAG configurations..."
                        create_dynamic_dag_variables "$AIRFLOW_SECRET_FILE_NAME"
                      else
                        echo "‚öôÔ∏è [PROCESS] Using standard single DAG configuration..."
                        python /usr/app/tsb-data-orchestrator-core/create_variables_airflow_bauth.op.py 2>&1 | tee -a /workdir/create_variables_airflow_bauth.log || true
                        
                        # Add to YAML files using temporary files
                        dag_config="{
                          \"project\": \"$DBT_PROJECT_NAME\",
                          \"dag_id\": \"${dag_id}_${branch_name}\",
                          \"variables_file\": \"$AIRFLOW_SECRET_FILE_NAME\"
                        }"
                        temp_file=$(mktemp)
                        yq -y ".dags += [$dag_config]" /workdir/airflow_dags.yaml > "$temp_file"
                        mv "$temp_file" /workdir/airflow_dags.yaml
                        
                        # Add secret key to YAML file using temporary file
                        temp_file=$(mktemp)
                        yq -y ".secrets.platform.\"${PLATFORM,,}\" = (.secrets.platform.\"${PLATFORM,,}\" // []) + [\"${secret_key}_${branch_name}\"]" /workdir/airflow_secrets.yaml > "$temp_file"
                        mv "$temp_file" /workdir/airflow_secrets.yaml
                      fi
                    fi 
                  elif [ "${PLATFORM,,}" == "airflow" ]; then
                    echo "‚öôÔ∏è [PROCESS] Platform Airflow detected"
                    echo "üîç [CHECK] Validating Airflow Required env Variables..."
                    if [[ -z $AIRFLOW_URL || -z $AIRFLOW_SECRET_FILE_NAME || -z $PLATFORM || -z $AIRFLOW_USER || -z $AIRFLOW_PASSWORD ]]; then
                      echo "‚ùå [ERROR] One or more variables are undefined"
                      exit 1
                    else  
                      echo "‚úÖ [SUCCESS] All variables are defined"
                      # Deploy DBT API Service if Operator is API.
                      if [[ "$DAG_OPERATOR" == "api" ]]; then
                        echo "==========================================="
                        echo "‚öôÔ∏è [PROCESS] Deploying DBT API Service"
                        echo "==========================================="
                        export HTTPS_ENABLED="false"
                        export PHASE_ENVIRONMENT="e2e"
                        export WORKER_NUM="1"
                        export MAX_REQUESTS="5"
                        export CPU_REQUEST="1500m"
                        export MEMORY_REQUEST="2Gi"
                        export APP_VERSION="{{inputs.parameters.commit_short_sha}}"
                        
                        /bin/bash -c "/usr/app/tsb-data-orchestrator-core/deploy_dbt_api_service.sh" 2>&1 | tee -a /workdir/deploy_dbt_api_service.log || true
                        # Source the environment file to get the variables
                        source "${DBT_PROJECT_NAME}_${APP_VERSION:-deploy}_env.sh"
                        echo "‚öôÔ∏è [PROCESS] Updating DBT_WORKLOAD_API_CONNECTION_ID..."
                        dbt_workload_connection_id=$(grep 'DBT_WORKLOAD_API_CONNECTION_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                        if [ -z "$dbt_workload_connection_id" ]; then
                          echo "üìã [CONFIG] DBT_WORKLOAD_API_CONNECTION_ID: is empty - adding variable"
                          sed -i "$ a\  DBT_WORKLOAD_API_CONNECTION_ID: '$DBT_WORKLOAD_API_CONNECTION_ID'" $AIRFLOW_SECRET_FILE_NAME
                        else
                          sed -i "s/DBT_WORKLOAD_API_CONNECTION_ID.*/DBT_WORKLOAD_API_CONNECTION_ID: '$DBT_WORKLOAD_API_CONNECTION_ID'/" $AIRFLOW_SECRET_FILE_NAME
                        fi
                        echo "‚öôÔ∏è [PROCESS] Updating DBT_WORKLOAD_API_RELEASE..."
                        dbt_workload_release=$(grep 'DBT_WORKLOAD_API_RELEASE:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || true
                        if [ -z "$dbt_workload_release" ]; then
                          echo "üìã [CONFIG] DBT_WORKLOAD_API_RELEASE: is empty - adding variable"
                          sed -i "$ a\  DBT_WORKLOAD_API_RELEASE: '$DBT_WORKLOAD_API_RELEASE'" $AIRFLOW_SECRET_FILE_NAME
                        fi
                        echo "DAG_OPERATOR=${DAG_OPERATOR}" >> /workdir/deploy_dbt_api_parameters.log
                        echo "DBT_WORKLOAD_API_RELEASE=${DBT_WORKLOAD_API_RELEASE}" >> /workdir/deploy_dbt_api_parameters.log
                        echo "DBT_WORKLOAD_API_CONNECTION_ID=${DBT_WORKLOAD_API_CONNECTION_ID}" >> /workdir/deploy_dbt_api_parameters.log
                      fi

                      echo "==========================================="
                      echo "‚öôÔ∏è [PROCESS] Importing Secrets to Airflow vault"
                      echo "==========================================="
                      if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                        echo "‚öôÔ∏è [PROCESS] Dynamic DAG is enabled. Creating multiple DAG configurations..."
                        create_dynamic_dag_variables "$AIRFLOW_SECRET_FILE_NAME"
                      else
                        echo "‚öôÔ∏è [PROCESS] Using standard single DAG configuration..."
                        python /usr/app/tsb-data-orchestrator-core/create_variables_airflow_bauth.op.py 2>&1 | tee -a /workdir/create_variables_airflow_bauth.log || true
                        
                        # Add to YAML files using temporary files
                        dag_config="{
                          \"project\": \"$DBT_PROJECT_NAME\",
                          \"dag_id\": \"${dag_id}_${branch_name}\",
                          \"variables_file\": \"$AIRFLOW_SECRET_FILE_NAME\"
                        }"
                        temp_file=$(mktemp)
                        yq -y ".dags += [$dag_config]" /workdir/airflow_dags.yaml > "$temp_file"
                        mv "$temp_file" /workdir/airflow_dags.yaml
                        
                        # Add secret key to YAML file using temporary file
                        temp_file=$(mktemp)
                        yq -y ".secrets.platform.\"${PLATFORM,,}\" = (.secrets.platform.\"${PLATFORM,,}\" // []) + [\"${secret_key}_${branch_name}\"]" /workdir/airflow_secrets.yaml > "$temp_file"
                        mv "$temp_file" /workdir/airflow_secrets.yaml
                      fi
                    fi
                  else
                    echo "‚ùå [ERROR] Unknown Platform - error"
                    exit 1
                  fi
                fi
                echo "‚úÖ [SUCCESS] DBT Project Variable import procedure completed for $folder"
                cd ..
              else
                echo "‚ö†Ô∏è [WARNING] Working on root - skip $folder"
              fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "üîç [CHECK] Error Analysis and Validation"
            echo "==========================================="

            error_found=0
            warning_count=0

            if [ ! -f "/workdir/deploy_dbt_api_service.log" ]; then
              echo "‚úÖ [SUCCESS] No changes detected - DBT API Service deployment log file does not exist. Proceeding successfully"
            else
              while read -r line; do
                [[ -z "$line" ]] && continue

                if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                  ((warning_count++))
                  continue
                fi

                if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                  echo "‚ùå [ERROR] Error found in E2E Test DBT Project DAG Variable import procedure, log file line: $line"
                  error_found=1
                fi
              done < /workdir/deploy_dbt_api_service.log
            fi

            if [ ! -f "/workdir/create_variables_airflow_bauth.log" ]; then
              echo "‚úÖ [SUCCESS] No changes detected - log file does not exist. Proceeding successfully"
            else
              while read -r line; do
                [[ -z "$line" ]] && continue

                if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                  ((warning_count++))
                  continue
                fi

                if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                  echo "‚ùå [ERROR] Error found in E2E Test DBT Project DAG Variable import procedure, log file line: $line"
                  error_found=1
                fi
              done < /workdir/create_variables_airflow_bauth.log

              if [ $error_found -eq 1 ]; then
                echo "‚ùå [ERROR] E2E Test DBT Project DAG Variable import procedure failed. Please check the logs for details"
                exit 1
              else
                echo "‚úÖ [SUCCESS] DBT Project DAG Variable import procedure completed successfully"
              fi
            fi

            echo "==========================================="
            echo "üéâ [SUCCESS] DBT Project Data Orchestration Variables Import Complete"
            echo "==========================================="
        env:
          - name: CUSTOMER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: DOMAIN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DOMAIN
          - name: DBT_API_SERVER_CONTROLLER_IMAGE_TAG
            value: "v0.0.7.1"
          - name: DBT_API_SERVER_CONTROLLER_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DBT_API_SERVER_CONTROLLER_SECRET
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: DBT_TARGET
            value: "test"
          - name: PRIVATE_ACCESS_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_NAME
          - name: PRIVATE_ACCESS_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PRIVATE_ACCESS_TOKEN
          - name: SECRET_PACKAGE_REPO_TOKEN_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_PACKAGE_REPO_TOKEN_NAME
          - name: SECRET_DBT_PACKAGE_REPO_TOKEN
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SECRET_DBT_PACKAGE_REPO_TOKEN
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_LOCATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_LOCATION
                optional: true
          - name: COMPOSER_ENVIRONMENT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: COMPOSER_ENVIRONMENT_NAME
                optional: true
          - name: GCP_SA_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_EMAIL
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: compile-dbt-manifest-e2e
      inputs:
        parameters:
          - name: branch
          - name: commit_short_sha
      outputs:
        parameters:
          - name: compile_manifest_path
            valueFrom:
              path: /workdir/compile_manifest_path.log
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.dbt_workflow_core_image_name}}:{{workflow.parameters.dbt_workflow_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "üöÄ [INFO] DBT Manifest E2E Compilation"
            echo "üìã [CONFIG] Branch: {{inputs.parameters.branch}}"
            echo "==========================================="

            # Leaving empty files for the next steps
            touch /workdir/compile_manifest_path.log
            cd /workdir/src

            echo "üîç [CHECK] Scanning changed folders for DBT compilation..."

            # Scan changed folders for DBT compilation
            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "‚öôÔ∏è [PROCESS] Compiling DBT Manifest in $folder"
                echo "==========================================="
                
                cd "$folder"
                
                # Set warehouse credentials
                echo "üîê [CONFIG] Configuring data warehouse credentials..."
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "‚ùå [ERROR] Google Bigquery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "üîê [CONFIG] Setting up BigQuery service account authentication"
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                      gcloud auth activate-service-account --key-file /secrets/sa.json
                      gcloud config set project $GCP_PROJECT_NAME
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "‚úÖ [SUCCESS] BigQuery authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "‚ùå [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "üîê [CONFIG] Setting up Snowflake private key authentication"
                      mkdir -p /snowsql/secrets/
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8
                      chmod 600 /snowsql/secrets/rsa_key.p8
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "‚úÖ [SUCCESS] Snowflake authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "‚ùå [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "‚ùå [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  fi
                fi
                
                echo "üîç [CHECK] Validating required Data-Orchestrator environment variables..."
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "‚ùå [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                fi
                
                # Set environment variables
                echo "üìã [CONFIG] Loading configuration from secrets file..."
                export MANIFEST_NAME=$(grep 'MANIFEST_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'")
                export TARGET=$(grep 'TARGET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'")
                export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" )
                export GIT_BRANCH=$(grep 'GIT_BRANCH:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'")
                export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'")
                export operator=$(grep 'OPERATOR:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export DATA_QUALITY=$(grep 'DATA_QUALITY:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export DATA_QUALITY_FORCE_E2E=$(grep 'DATA_QUALITY_FORCE_E2E:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                export SOURCE_E2E_TABLESAMPLE=$(grep 'SOURCE_E2E_TABLESAMPLE:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                
                echo "‚úÖ [SUCCESS] Configuration loaded successfully"

                # Create log file
                touch /workdir/compile_dbt_project_manifest.log

                # Prepare the files for bash operator
                if [[ "$operator" == "bash" ]]; then
                  echo "‚öôÔ∏è [PROCESS] Preparing files for bash operator..."
                  # Create temporary directory
                  mkdir -p /tmp/dbt_project_bash_operator
                  # Copy files to temporary directory
                  cp -r "../$folder" "/tmp/dbt_project_bash_operator/$folder"
                  echo "‚úÖ [SUCCESS] Files prepared for bash operator"
                fi

                # Function to check and modify packages.yml if needed
                check_and_modify_packages() {
                  local packages_file="packages.yml"
                  
                  # Check if re-data package exists in packages.yml
                  if yq -r '.packages[] | select(.package == "re-data/re_data")' "$packages_file" > /dev/null 2>&1; then
                    if [ "$DATA_QUALITY" = "false" ] || [ "$DATA_QUALITY" = "False" ]; then
                      echo "‚ö†Ô∏è [WARNING] DATA_QUALITY is false. Removing re-data package from packages.yml"
                      # Create a temporary file without the re-data package
                      yq 'del(.packages[] | select(.package == "re-data/re_data"))' "$packages_file" > "${packages_file}.tmp"
                      mv "${packages_file}.tmp" "$packages_file"
                      
                      # Force recompilation by removing manifest
                      rm -f "./target/manifest.json"
                      echo "‚úÖ [SUCCESS] re-data package removed and manifest cleared"
                    fi
                  fi
                }

                # Check and modify packages.yml first
                echo "üîç [CHECK] DATA_QUALITY_FORCE_E2E value: ${DATA_QUALITY_FORCE_E2E}"

                # Check and modify packages if not forced
                if [ "${DATA_QUALITY_FORCE_E2E}" != "True" ] && [ "${DATA_QUALITY_FORCE_E2E}" != "true" ] && [ "${DATA_QUALITY_FORCE_E2E}" != "TRUE" ]; then
                  echo "‚öôÔ∏è [PROCESS] Processing packages.yml modifications..."
                  if [ -f "packages.yml" ]; then
                    check_and_modify_packages
                  fi
                fi

                # Handle data quality setup if forced
                if [ "${DATA_QUALITY_FORCE_E2E}" == "True" ] || [ "${DATA_QUALITY_FORCE_E2E}" == "true" ] || [ "${DATA_QUALITY_FORCE_E2E}" == "TRUE" ]; then
                  echo "‚öôÔ∏è [PROCESS] E2E Data Quality force enabled, creating empty data quality schema..."
                  dbt deps --quiet > /dev/null 2>&1
                  dbt run --profiles-dir . --select package:re_data --target ${TARGET} --empty --quiet > /dev/null 2>&1 || true
                  dbt clean --quiet > /dev/null 2>&1
                  echo "‚úÖ [SUCCESS] Empty data quality schema created"
                fi

                echo "==========================================="
                echo "‚öôÔ∏è [PROCESS] Compiling DBT Project Manifest"
                echo "==========================================="
                
                dbt clean --quiet 2>&1
                dbt deps --quiet 2>&1 | tee -a /workdir/compile_dbt_project_manifest.log || true
                dbt seed --quiet --target $TARGET 2>&1 | tee -a /workdir/compile_dbt_project_manifest.log || true
                dbt compile --exclude test_type:generic test_type:singular package:re_data --target $TARGET 2>&1 | tee -a /workdir/compile_dbt_project_manifest.log || true
                
                echo "‚úÖ [SUCCESS] DBT Compilation completed"
                
                echo "==========================================="
                echo "‚öôÔ∏è [PROCESS] Setting up E2E test sample table source data"
                echo "==========================================="
                
                echo "üìã [CONFIG] Adding macro file to the dbt project catalog..."
                macros_path=$(yq '.["macro-paths"][] // "macros"' dbt_project.yml)
                # Trim any extra whitespace
                macros_path=$(echo "$macros_path" | xargs)
                
                if [ -d "$macros_path" ]; then
                  echo "‚úÖ [SUCCESS] DBT macros directory exists: $macros_path"
                else
                  echo "‚öôÔ∏è [PROCESS] Creating DBT macros directory: $macros_path"
                  mkdir -p "$macros_path"  # Use -p to ensure intermediate directories are created if needed.
                  # Check again to confirm creation
                  if [ -d "$macros_path" ]; then
                    echo "‚úÖ [SUCCESS] DBT macros directory created successfully"
                  else
                    echo "‚ùå [ERROR] Failed to create directory $macros_path. Please check permissions."
                    exit 1  # Exit with an error code if directory creation fails
                  fi
                fi
                
                if [ "${SOURCE_E2E_TABLESAMPLE}" != "false" ] && [ "${SOURCE_E2E_TABLESAMPLE}" != "False" ] && [ "${SOURCE_E2E_TABLESAMPLE}" != "FALSE" ]; then
                  if [ -f "/usr/app/dbt/macros/create_tablesample_source.sql" ]; then
                    cp /usr/app/dbt/macros/create_tablesample_source.sql ./"$macros_path"
                    echo "‚úÖ [SUCCESS] Tablesample source macro copied"
                  else
                    echo "‚ùå [ERROR] Source file does not exist: /usr/app/dbt/macros/create_tablesample_source.sql"
                    exit 1
                  fi
                  
                  if [[ ! -z "${TARGET}" ]]; then
                    echo "‚öôÔ∏è [PROCESS] Running warehouse create dataset sample command..."
                    dbt run-operation create_samples_for_dataset --args '{"percent_sample": 1}' --target="${TARGET}" 2>&1 | tee -a /workdir/compile_dbt_project_manifest.log || true
                    echo "‚úÖ [SUCCESS] Dataset sample creation completed"
                  fi
                fi
                
                echo "‚úÖ [SUCCESS] E2E test sample table setup completed"
                
                echo "==========================================="
                echo "‚öôÔ∏è [PROCESS] Handling platform-specific manifest deployment"
                echo "==========================================="
                
                # Handle different platforms
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "üìã [CONFIG] Platform: Google Cloud Composer"
                  
                  if [[ -z $GCP_BUCKET_NAME ]]; then
                    echo "‚ùå [ERROR] GCP_BUCKET_NAME is undefined"
                    exit 1
                  fi
                  
                  echo "‚öôÔ∏è [PROCESS] Uploading Manifest file to Composer GCS Bucket main catalog..."
                  gsutil cp ./target/manifest.json gs://$GCP_BUCKET_NAME/dags/${DBT_PROJECT_NAME}/${MANIFEST_NAME}.json
                  echo "‚úÖ [SUCCESS] Manifest file uploaded to Composer GCS Bucket"
                  
                  echo "üìã [CONFIG] Saving Manifest file path to Artifacts..."
                  echo "${PLATFORM,,}=gs://$GCP_BUCKET_NAME/dags/${DBT_PROJECT_NAME}/${MANIFEST_NAME}.json" >> /workdir/compile_manifest_path.log
                  
                  # Copy files to final destination for bash operator
                  if [[ "$operator" == "bash" ]]; then
                    echo "‚öôÔ∏è [PROCESS] Copying files to final destination for bash operator..."
                    gsutil cp "/tmp/dbt_project_bash_operator/$folder/"* "gs://$GCP_BUCKET_NAME/dags/dbt/${DBT_PROJECT_NAME}_${GIT_BRANCH}"
                    echo "‚úÖ [SUCCESS] Files copied to bash operator destination"
                  fi
                  
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "üìã [CONFIG] Platform: Apache Airflow"
                  
                  if [[ -z ${GIT_USER_EMAIL} || -z ${GIT_USER_NAME} || -z ${DAG_REPO_PUSH} ]]; then
                    echo "‚ùå [ERROR] One or more Git variables are undefined"
                    exit 1
                  fi
                  
                  echo "‚öôÔ∏è [PROCESS] Cloning Airflow dags repository for upload procedure..."
                  git config --global user.email "${GIT_USER_EMAIL}"
                  git config --global user.name "${GIT_USER_NAME}"
                  REPO_DIR="/workdir/airflow-dags"
                  rm -rf $REPO_DIR
                  git clone $DAG_REPO_PUSH $REPO_DIR
                  
                  echo "‚öôÔ∏è [PROCESS] Uploading Manifest file to Airflow DAG repository..."
                  mkdir -p "$REPO_DIR/dags/${DBT_PROJECT_NAME}/"
                  cp ./target/manifest.json $REPO_DIR/dags/${DBT_PROJECT_NAME}/${MANIFEST_NAME}.json
                  
                  echo "üìã [CONFIG] Saving Manifest file path to Artifacts..."
                  echo "${PLATFORM,,}=$REPO_DIR/dags/${DBT_PROJECT_NAME}/${MANIFEST_NAME}.json" >> /workdir/compile_manifest_path.log
                  
                  # Copy files to final destination for bash operator 
                  if [[ "$operator" == "bash" ]]; then
                    echo "‚öôÔ∏è [PROCESS] Copying files to final destination for bash operator..."
                    # Create destination directory if it doesn't exist
                    mkdir -p "$REPO_DIR/dags/dbt/${DBT_PROJECT_NAME}_${GIT_BRANCH}"
                    # Copy files to final destination
                    cp -r "/tmp/dbt_project_bash_operator/$folder/"* "$REPO_DIR/dags/dbt/${DBT_PROJECT_NAME}_${GIT_BRANCH}"
                    echo "${operator,,}=$REPO_DIR/dags/dbt/${DBT_PROJECT_NAME}_${GIT_BRANCH}" >> /workdir/bash_operator_files_path.log
                    echo "‚úÖ [SUCCESS] Files copied to bash operator destination"
                  fi
                  
                  echo "‚öôÔ∏è [PROCESS] Committing and pushing changes to repository..."
                  git -C "$REPO_DIR" add -A
                  git -C "$REPO_DIR" commit -am "Update DBT Project Manifest files"
                  git -C "$REPO_DIR" remote set-url --push origin "${DAG_REPO_PUSH}" 
                  git -C "$REPO_DIR" push || true
                  echo "‚úÖ [SUCCESS] Changes committed and pushed to repository"
                  
                else
                  echo "‚ùå [ERROR] Unknown Platform - error"
                  exit 1
                fi
                
                cd ..
              else
                echo "‚ö†Ô∏è [WARNING] Working on root - skipping folder processing"
              fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "üîç [CHECK] Analyzing compilation results"
            echo "==========================================="

            error_found=0
            warning_count=0

            if [ ! -f "/workdir/compile_dbt_project_manifest.log" ]; then
              echo "‚úÖ [SUCCESS] No changes detected - manifest log file does not exist. Proceeding successfully."
            else
              error_found=0
              
              while read -r line; do
                [[ -z "$line" ]] && continue

                if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                  ((warning_count++))
                  continue
                fi

                # Check if line contains error-like words but in a normal operation context
                if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                  # Skip if it appears to be part of a table/object name
                  if echo "$line" | grep -Ei "creating|sampling|building|generating|_raw_" > /dev/null; then
                    # Log for debugging but don't count as error
                    continue
                  fi
                  
                  # More reliable error indicators - actual errors typically have these patterns
                  if echo "$line" | grep -Ei "(\[ERROR\]|exception |failed:|process.*failed|terminated with|returned code [1-9]|unexpected error)" > /dev/null; then
                    echo "‚ùå [ERROR] Error found in E2E Test DBT Manifest compilation procedure, log file line: $line"
                    error_found=1
                  fi
                fi
              done < /workdir/compile_dbt_project_manifest.log

              if [ $error_found -eq 1 ]; then
                echo "‚ùå [ERROR] E2E Test DBT Manifest compilation procedure failed. Please check the logs for details."
                exit 1
              else
                echo "‚úÖ [SUCCESS] DBT Manifest compilation procedure completed successfully"
              fi
            fi

            echo "==========================================="
            echo "üéâ [SUCCESS] DBT Manifest E2E Compilation Complete"
            echo "üìã [INFO] Branch: {{inputs.parameters.branch}}"
            echo "==========================================="
        env:
          - name: DBT_TARGET
            value: "test"
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PLATFORM
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_BUCKET_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_BUCKET_NAME
                optional: true
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: compile-dag-e2e
      inputs:
        parameters:
          - name: branch
          - name: commit_short_sha
          - name: target_branch
          - name: ci_job_id
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_orchestrator_core_image_name}}:{{workflow.parameters.data_orchestrator_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash
            # Exit on error, treat unset variables as errors, and return value of pipeline is the status of the last command


            echo "==========================================="
            echo "üöÄ [INFO] DBT Project Data-Orchestrator DAG Compilation"
            echo "==========================================="

            cd /workdir/src

            # Set global branch name and transform it if needed
            branch_name="{{inputs.parameters.branch}}"
            if [[ "$branch_name" =~ [^a-zA-Z0-9_] ]]; then
              new_branch_name=$(echo "$branch_name" | sed 's/[^a-zA-Z0-9_]/_/g')
              echo "‚ö†Ô∏è [WARNING] Branch name contains symbols. Changing to: ${new_branch_name}"
              branch_name="${new_branch_name}"
            fi

            echo "üìã [CONFIG] Branch name configured: $branch_name"

            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "‚öôÔ∏è [PROCESS] Compiling Data-Orchestrator DAG in directory: $folder"
                echo "==========================================="
                
                cd "$folder"
                
                # Initialize an array to track created DAG files for this folder
                created_dag_files=()
                
                echo "üìã [CONFIG] Setting up warehouse credentials"
                
                # Set warehouse credentials
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "‚ùå [ERROR] Google Bigquery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                      gcloud auth activate-service-account --key-file /secrets/sa.json
                      gcloud config set project $GCP_PROJECT_NAME
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "‚úÖ [SUCCESS] BigQuery credentials configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "‚ùå [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      mkdir -p /snowsql/secrets/
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8
                      chmod 600 /snowsql/secrets/rsa_key.p8
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "‚úÖ [SUCCESS] Snowflake credentials configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "‚ùå [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  fi
                  echo "‚úÖ [SUCCESS] Redshift credentials validated"
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "‚ùå [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  fi
                  echo "‚úÖ [SUCCESS] Fabric credentials validated"
                fi
                
                echo "üîç [CHECK] Validating required Data-Orchestrator environment variables"
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "‚ùå [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                else
                  echo "‚úÖ [SUCCESS] All required variables are defined"
                  echo "üìã [CONFIG] Airflow secret file: $AIRFLOW_SECRET_FILE_NAME"

                  echo "‚öôÔ∏è [PROCESS] Importing Variables from Airflow Secret yml file"
                  # First check if the variables file exists
                  if [ ! -f "$AIRFLOW_SECRET_FILE_NAME" ]; then
                    echo "‚ùå [ERROR] Variables file $AIRFLOW_SECRET_FILE_NAME not found"
                    exit 1
                  fi

                  echo "‚úÖ [SUCCESS] Variables file found and validated"

                  export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" )
                  export dbt_project_name=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export source_commit_id={{inputs.parameters.commit_short_sha}}
                  export cd_cd_job_id={{inputs.parameters.ci_job_id}}
                  export GIT_BRANCH=$(grep 'GIT_BRANCH:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export DYNAMIC_DAG=$(grep 'DYNAMIC_DAG:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                fi
                
                echo "üìã [CONFIG] Processing dbt project: $dbt_project_name"

                # Function to create and upload dynamic DAGs
                create_dynamic_dags() {
                  local base_secret_file="$1"
                  local current_project="$dbt_project_name"
                  local dynamic_config_file="dynamic_dag_config.yml"
                  
                  echo "‚öôÔ∏è [PROCESS] Creating dynamic DAGs for project: $current_project"
                  
                  if [ ! -f "$dynamic_config_file" ]; then
                    echo "‚ùå [ERROR] Dynamic config file not found: $dynamic_config_file"
                    return 1
                  fi
                  
                  echo "‚úÖ [SUCCESS] Dynamic config file validated"
                  
                  # Get DAG configurations from airflow_dags.yaml
                  echo "üìã [CONFIG] Reading DAG configurations from airflow_dags.yaml"

                  # Use yq to extract DAG configurations for the current project
                  local dag_configs=$(yq -r '.dags[] | select(.project == "'$current_project'") | [.dag_id, .variables_file] | @tsv' /workdir/airflow_dags.yaml)
                  
                  if [ -z "$dag_configs" ]; then
                    echo "‚ùå [ERROR] No DAG configurations found for project $current_project"
                    return 1
                  fi
                  
                  echo "‚úÖ [SUCCESS] Found DAG configurations for processing"
                    
                  # Process each DAG configuration
                  while IFS=$'\t' read -r dag_id variables_file; do
                    if [ -z "$dag_id" ] || [ -z "$variables_file" ]; then
                      echo "‚ö†Ô∏è [WARNING] Skipping invalid DAG configuration"
                      continue
                    fi
                    
                    echo "‚öôÔ∏è [PROCESS] Processing DAG: $dag_id"
                    
                    # Verify variables file exists
                    if [ ! -f "$variables_file" ]; then
                      echo "‚ùå [ERROR] Variables file not found: $variables_file"
                      continue
                    fi
                    
                    echo "‚úÖ [SUCCESS] Variables file validated: $variables_file"
                    
                    # Create DAG file using parameters instead of environment variables
                    echo "‚öôÔ∏è [PROCESS] Creating DAG file for $dag_id"
                    python /usr/app/tsb-data-orchestrator-core/create_dag_from_template.py --variables-file "$variables_file" --dag-id "$dag_id" 2>&1 | tee -a /workdir/create_dag_from_template.log || true
                    
                    echo "üîç [CHECK] Verifying DAG file creation"

                    # Move DAG file to dags directory with proper name
                    local dag_file="${dag_id}_dag.py"
                    
                    if [ -f "$dag_file" ]; then
                      echo "‚öôÔ∏è [PROCESS] Moving DAG file to dags directory"
                      mv "$dag_file" "/workdir/dags/${dag_id}.py"
                      # Track the created DAG file
                      created_dag_files+=("/workdir/dags/${dag_id}.py")
                      echo "‚úÖ [SUCCESS] DAG file created: ${dag_id}.py"
                    else
                      echo "‚ùå [ERROR] DAG file was not created, skipping..."
                      continue
                    fi
                  done <<< "$dag_configs"
                }

                # Function to create and upload standard DAG
                create_standard_dag() {
                  local base_secret_file="$1"
                  local current_project="$dbt_project_name"
                  
                  echo "‚öôÔ∏è [PROCESS] Using standard single DAG configuration"
                  # Get the DAG configuration from airflow_dags.yaml
                  local dag_config=$(yq -r '.dags[] | select(.project == "'$current_project'") | [.dag_id, .variables_file] | @tsv' /workdir/airflow_dags.yaml)
                  
                  if [ -z "$dag_config" ]; then
                    echo "‚ùå [ERROR] Could not find DAG configuration for project $current_project in airflow_dags.yaml"
                    return 1
                  fi
                  
                  IFS=$'\t' read -r dag_id variables_file <<< "$dag_config"
                  
                  echo "üìã [CONFIG] DAG configuration:"
                  echo "  - DAG ID: $dag_id"
                  echo "  - Variables file: $variables_file"
                  
                  # Verify variables file exists
                  if [ ! -f "$variables_file" ]; then
                    echo "‚ùå [ERROR] Variables file not found: $variables_file"
                    return 1
                  fi
                  
                  # Set environment variables
                  export AIRFLOW_SECRET_FILE_NAME="$variables_file"
                  export DAG_ID="$dag_id"
                  export DBT_PROJECT_DAG_ID="$dag_id"
                  
                  echo "‚öôÔ∏è [PROCESS] Creating standard DAG file"
                  python /usr/app/tsb-data-orchestrator-core/create_dag_from_template.py 2>&1 | tee -a /workdir/create_dag_from_template.log || true
                  echo "‚úÖ [SUCCESS] Standard DAG file created"

                  echo "üîç [CHECK] Verifying DAG file creation"
                  
                  # Move DAG file to dags directory with proper name
                  local dag_file="${dag_id}_dag.py"

                  if [ -f "$dag_file" ]; then
                    echo "‚öôÔ∏è [PROCESS] Moving DAG file to dags directory"
                    mv "$dag_file" "/workdir/dags/${dag_id}.py"
                    # Track the created DAG file
                    created_dag_files+=("/workdir/dags/${dag_id}.py")
                    echo "‚úÖ [SUCCESS] DAG file created: ${dag_id}.py"
                  else
                    echo "‚ùå [ERROR] DAG file was not created, skipping..."
                    return 1
                  fi
                }

                # Create directory for DAG files if it doesn't exist
                mkdir -p /workdir/dags
                
                echo "==========================================="
                echo "‚öôÔ∏è [PROCESS] Compiling DBT Project DAG"
                echo "==========================================="
                
                if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                  echo "üìã [CONFIG] Dynamic DAG is enabled. Creating multiple DAG files..."
                  create_dynamic_dags "$AIRFLOW_SECRET_FILE_NAME"
                else
                  echo "üìã [CONFIG] Using standard single DAG configuration..."
                  create_standard_dag "$AIRFLOW_SECRET_FILE_NAME"
                fi

                echo "‚úÖ [SUCCESS] DAG for dbt project $dbt_project_name was created"
                
                echo "==========================================="
                echo "üîç [CHECK] Platform Configuration"
                echo "==========================================="
                
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "üìã [CONFIG] Platform: Composer"
                  echo "üîç [CHECK] Validating required env variables for Composer service"
                  if [[ -z $GCP_BUCKET_NAME ]]; then
                    echo "‚ùå [ERROR] GCP_BUCKET_NAME is undefined"
                    exit 1
                  else  
                    echo "‚úÖ [SUCCESS] All Composer variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Uploading DAG file to Composer GCS Bucket"
                    if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                      if [ ${#created_dag_files[@]} -gt 0 ]; then
                        echo "‚öôÔ∏è [PROCESS] Uploading dynamic DAGs to GCS bucket"
                        for dag_file in "${created_dag_files[@]}"; do
                          gsutil -m cp "$dag_file" "gs://$GCP_BUCKET_NAME/dags/$folder/"
                          echo "${PLATFORM,,}=gs://$GCP_BUCKET_NAME/dags/$folder/$(basename $dag_file)" >> /workdir/compile_dag_path.log
                          echo "‚úÖ [SUCCESS] Uploaded: $(basename $dag_file)"
                        done
                      else
                        echo "‚ùå [ERROR] No DAG files found for upload"
                        exit 1
                      fi
                    else
                      gsutil -m cp "${created_dag_files[0]}" "gs://$GCP_BUCKET_NAME/dags/$folder/"
                      echo "${PLATFORM,,}=gs://$GCP_BUCKET_NAME/dags/$folder/$(basename ${created_dag_files[0]})" >> /workdir/compile_dag_path.log
                      echo "‚úÖ [SUCCESS] Uploaded: $(basename ${created_dag_files[0]})"
                    fi
                    echo "‚úÖ [SUCCESS] TSB DBT Parser Libraries check completed"
                    echo "‚úÖ [SUCCESS] Upload to Composer completed"
                  fi
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "üìã [CONFIG] Platform: Airflow"
                  echo "üîç [CHECK] Validating required env variables for Data-Orchestrator service"
                  if [[ -z ${GIT_USER_EMAIL} || -z ${GIT_USER_NAME} || -z ${DAG_REPO_PUSH} ]]; then
                    echo "‚ùå [ERROR] One or more Airflow variables are undefined"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] All Airflow variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Cloning Airflow dags repository for upload procedure"
                    git config --global user.email "${GIT_USER_EMAIL}"
                    git config --global user.name "${GIT_USER_NAME}"
                    REPO_DIR=/workdir/airflow-dags
                    if [ ! -d "$REPO_DIR" ]; then
                      echo "üìã [CONFIG] Repository directory does not exist - creating"
                    else
                      echo "üìã [CONFIG] Repository directory exists - cleaning up"
                      rm -fr $REPO_DIR
                    fi
                    git clone $DAG_REPO_PUSH $REPO_DIR
                    if [ ! -d "$REPO_DIR/dags/" ]; then
                      echo "üìã [CONFIG] Dags directory does not exist - creating"
                      mkdir "$REPO_DIR/dags/"
                    else
                      echo "üìã [CONFIG] Dags directory exists - continuing"
                    fi
                    echo "‚öôÔ∏è [PROCESS] Uploading DAG file to Airflow DAG repository"
                    if [ ! -d "$REPO_DIR/dags/$folder/" ]; then
                      echo "üìã [CONFIG] Project directory does not exist - creating"
                      mkdir $REPO_DIR/dags/$folder/
                    else
                      echo "üìã [CONFIG] Project directory exists - continuing"
                    fi
                    if [ "${DYNAMIC_DAG,,}" == "true" ]; then
                      if [ ${#created_dag_files[@]} -gt 0 ]; then
                        echo "‚öôÔ∏è [PROCESS] Copying dynamic DAGs to repository"
                        for dag_file in "${created_dag_files[@]}"; do
                          cp "$dag_file" $REPO_DIR/dags/$folder/
                          echo "${PLATFORM,,}=$REPO_DIR/dags/$folder/$(basename $dag_file)" >> /workdir/compile_dag_path.log
                          echo "‚úÖ [SUCCESS] Copied: $(basename $dag_file)"
                        done
                      else
                        echo "‚ùå [ERROR] No DAG files found for upload"
                        exit 1
                      fi
                    else
                      cp "${created_dag_files[0]}" $REPO_DIR/dags/$folder/
                      echo "${PLATFORM,,}=$REPO_DIR/dags/$folder/$(basename ${created_dag_files[0]})" >> /workdir/compile_dag_path.log
                      echo "‚úÖ [SUCCESS] Copied: $(basename ${created_dag_files[0]})"
                    fi
                    echo "‚úÖ [SUCCESS] TSB DBT Parser Libraries check completed"
                    echo "‚úÖ [SUCCESS] Upload to repository completed"
                    git -C "$REPO_DIR" add -A
                    git -C "$REPO_DIR" commit -am"Update DBT Project DAG files."
                    git -C "$REPO_DIR" remote set-url --push origin "${DAG_REPO_PUSH}" 
                    git -C "$REPO_DIR" push || true
                    echo "‚úÖ [SUCCESS] Git push completed"
                  fi
                else
                  echo "‚ùå [ERROR] Unknown Platform: ${PLATFORM}"
                  exit 1
                fi
                cd ..
              else
                echo "üìã [CONFIG] Working on root - skipping"
              fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "üîç [CHECK] Error Analysis"
            echo "==========================================="

            error_found=0
            warning_count=0

            if [ ! -f "/workdir/create_dag_from_template.log" ]; then
              echo "üìã [CONFIG] No changes detected - DAG template log file does not exist. Proceeding successfully."
            else
              while read -r line; do
                [[ -z "$line" ]] && continue

                if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                  ((warning_count++))
                  continue
                fi

                if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                  echo "‚ùå [ERROR] Error found in E2E Test DAG Compilation procedure, log file line: $line"
                  error_found=1
                fi
              done < /workdir/create_dag_from_template.log

              if [ $error_found -eq 1 ]; then
                echo "‚ùå [ERROR] E2E Test DAG Compilation procedure failed. Please check the logs for details."
                exit 1
              else
                echo "‚úÖ [SUCCESS] DBT Project DAG Compilation procedure completed successfully"
              fi
            fi

            echo "==========================================="
            echo "‚úÖ [SUCCESS] DBT Project Data-Orchestrator DAG Compilation Completed"
            echo "==========================================="
        env:
          - name: DBT_TARGET
            value: "test"
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_BUCKET_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_BUCKET_NAME
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-model-e2e-dag-run-trigger
      inputs:
        parameters:
          - name: branch
          - name: commit_short_sha
      outputs:
        parameters:
          - name: dag_run_id
            valueFrom:
              path: /workdir/dag_run_id.env
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_orchestrator_core_image_name}}:{{workflow.parameters.data_orchestrator_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash
            # Exit on error, treat unset variables as errors, and return value of pipeline is the status of the last command


            # ===========================================
            # E2E Data-Pipeline DAG Trigger Script
            # ===========================================
            # This script is used to trigger the E2E Data-Pipeline DAGs for the DBT Project.
            # It is used in the Argo Workflow to trigger the DAGs for the E2E Test Run.
            # ===========================================

            echo "üöÄ [INFO] Starting DBT Project DAG E2E Data-Pipeline Trigger procedure"
            echo "=========================================="

            # Navigate to source directory
            cd /workdir/src || { echo "‚ùå [ERROR] Failed to navigate to /workdir/src"; exit 1; }

            # ===========================================
            # DAG RUN ID GENERATION
            # ===========================================
            echo "‚öôÔ∏è [PROCESS] Generating unique DAG run ID"
            touch /workdir/dag_run_id.env || { echo "‚ùå [ERROR] Failed to create dag_run_id.env file"; exit 1; }

            # Generate unique dag_run_id
            uuid=$(uuidgen) || { echo "‚ùå [ERROR] Failed to generate UUID"; exit 1; }
            uuid=${uuid/-/_0x}
            uuid=${uuid%_*-*-*-*}
            export "dag_run_id=e2e_test_run_{{ inputs.parameters.commit_short_sha }}_${uuid}" 
            echo "dag_run_id=$dag_run_id" > ../dag_run_id.env || { echo "‚ùå [ERROR] Failed to write dag_run_id to file"; exit 1; }
            echo "‚úÖ [SUCCESS] DAG run ID generated: ${dag_run_id:0:20}..."

            # ===========================================
            # PRE-PROCESSING BRANCH DAGS
            # ===========================================
            echo "‚öôÔ∏è [PROCESS] Pre-processing branch: {{inputs.parameters.branch}} DAGs"
            python /usr/app/tsb-data-orchestrator-core/airflow_reserialize_dag.py 2>&1 || echo "‚ö†Ô∏è [WARNING] DAG reserialization completed with warnings"

            # ===========================================
            # DAG TRIGGER FUNCTIONS
            # ===========================================

            # Function to trigger DAGs
            trigger_dags() {
              local platform="$1"
              local dag_id="$2"
              local branch_name="{{inputs.parameters.branch}}"
              
              echo "‚öôÔ∏è [PROCESS] Triggering DAG: $dag_id"
              
              if [ "${platform,,}" == "composer" ]; then
                echo "üìã [CONFIG] Platform: Composer"
                echo "üîç [CHECK] Validating Composer environment variables"
                
                if [[ -z $AIRFLOW_URL || -z $PLATFORM || -z $project_id || -z $location || -z $composer_environment_name || -z $service_account_email ]]; then
                  echo "‚ùå [ERROR] One or more Composer environment variables are undefined"
                  return 1
                else  
                  echo "‚úÖ [SUCCESS] All Composer variables are defined"
                  echo "‚öôÔ∏è [PROCESS] Triggering Airflow DAG: $dag_id"
                  export DAG_ID="$dag_id"
                  python /usr/app/tsb-data-orchestrator-core/run_dag_airflow_bauth.op.py 2>&1 | tee -a /workdir/triggering_dag_procedure.log || echo "‚ö†Ô∏è [WARNING] DAG trigger completed with warnings"
                fi 
              elif [ "${platform,,}" == "airflow" ]; then
                echo "üìã [CONFIG] Platform: Airflow"
                echo "üîç [CHECK] Validating Airflow environment variables"
                
                if [[ -z $AIRFLOW_URL || -z $PLATFORM || -z $AIRFLOW_USER || -z $AIRFLOW_PASSWORD ]]; then
                  echo "‚ùå [ERROR] One or more Airflow environment variables are undefined"
                  return 1
                else  
                  echo "‚úÖ [SUCCESS] All Airflow variables are defined"
                  echo "‚öôÔ∏è [PROCESS] Pre-processing DAG"
                  export DAG_ID="$dag_id"
                  echo "‚öôÔ∏è [PROCESS] Triggering Airflow DAG: $dag_id"
                  python /usr/app/tsb-data-orchestrator-core/run_dag_airflow_bauth.op.py 2>&1 | tee -a /workdir/triggering_dag_procedure.log || echo "‚ö†Ô∏è [WARNING] DAG trigger completed with warnings"
                fi
              else
                echo "‚ùå [ERROR] Unknown platform: $platform"
                return 1
              fi
            }

            # Function to process DAGs for a project
            process_project_dags() {
              local project_name="$1"
              local platform="$2"
              
              echo "üìã [CONFIG] Reading DAG configurations from airflow_dags.yaml"
              
              # Get all DAG configurations for the current project
              local dag_configs=$(yq -r '.dags[] | select(.project == "'$project_name'") | .dag_id' /workdir/airflow_dags.yaml) || { echo "‚ùå [ERROR] Failed to read DAG configurations"; return 1; }
              
              if [ -z "$dag_configs" ]; then
                echo "‚ùå [ERROR] No DAG configurations found for project $project_name in airflow_dags.yaml"
                return 1
              fi
              
              echo "‚úÖ [SUCCESS] Found DAG configurations for project $project_name"
              
              # Process each DAG configuration
              while IFS= read -r dag_id; do
                if [ -n "$dag_id" ]; then
                  echo "‚öôÔ∏è [PROCESS] Processing DAG: $dag_id"
                  trigger_dags "$platform" "$dag_id"
                fi
              done <<< "$dag_configs"
            }

            # ===========================================
            # MAIN PROCESSING LOOP
            # ===========================================
            echo "üöÄ [INFO] Starting main processing loop for changed folders"

            while read folder; do
              if [ -d "$folder" ]; then
                echo "=========================================="
                echo "‚öôÔ∏è [PROCESS] Processing directory: $folder"
                echo "üìã [CONFIG] Branch: {{inputs.parameters.branch}}"
                cd "$folder" || { echo "‚ùå [ERROR] Failed to navigate to directory: $folder"; exit 1; }
                
                # ===========================================
                # WAREHOUSE CREDENTIALS SETUP
                # ===========================================
                echo "üîç [CHECK] Setting up warehouse credentials for platform: ${DATA_WAREHOUSE_PLATFORM,,}"
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "‚ùå [ERROR] Google BigQuery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "‚öôÔ∏è [PROCESS] Setting up Google Cloud authentication"
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json || { echo "‚ùå [ERROR] Failed to decode service account secret"; exit 1; }
                      gcloud auth activate-service-account --key-file /secrets/sa.json || { echo "‚ùå [ERROR] Failed to activate service account"; exit 1; }
                      gcloud config set project $GCP_PROJECT_NAME || { echo "‚ùå [ERROR] Failed to set GCP project"; exit 1; }
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "‚úÖ [SUCCESS] Google Cloud authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "‚ùå [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "‚öôÔ∏è [PROCESS] Setting up Snowflake authentication"
                      mkdir -p /snowsql/secrets/ || { echo "‚ùå [ERROR] Failed to create Snowflake secrets directory"; exit 1; }
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8 || { echo "‚ùå [ERROR] Failed to write Snowflake private key"; exit 1; }
                      chmod 600 /snowsql/secrets/rsa_key.p8 || { echo "‚ùå [ERROR] Failed to set Snowflake key permissions"; exit 1; }
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "‚úÖ [SUCCESS] Snowflake authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "‚ùå [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] Redshift credentials validated"
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "‚ùå [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] Fabric credentials validated"
                  fi
                fi
                
                # ===========================================
                # AIRFLOW VARIABLES IMPORT
                # ===========================================
                echo "üîç [CHECK] Validating Data-Orchestrator environment variables"
                
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "‚ùå [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                else
                  echo "‚úÖ [SUCCESS] All required variables are defined"
                  echo "‚öôÔ∏è [PROCESS] Importing variables from Airflow secret file"
                  
                  export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" ) || { echo "‚ùå [ERROR] Failed to extract PLATFORM from secret file"; exit 1; }
                  export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || { echo "‚ùå [ERROR] Failed to extract DBT_PROJECT_NAME from secret file"; exit 1; }
                  
                  echo "üìã [CONFIG] Platform: $PLATFORM"
                  echo "üìã [CONFIG] DBT Project: $DBT_PROJECT_NAME"
                fi 

                # Create log file for DAG triggering
                touch /workdir/triggering_dag_procedure.log || { echo "‚ùå [ERROR] Failed to create triggering log file"; exit 1; }

                # Process DAGs for the current project
                echo "‚öôÔ∏è [PROCESS] Processing DAGs for project: $DBT_PROJECT_NAME"
                process_project_dags "$DBT_PROJECT_NAME" "$PLATFORM"
                
                cd .. || { echo "‚ùå [ERROR] Failed to return to parent directory"; exit 1; }
              else
                echo "üìã [CONFIG] Working on root - skipping directory processing"
              fi
            done < /workdir/changed_folders.txt

            # ===========================================
            # ERROR ANALYSIS AND COMPLETION
            # ===========================================
            echo "=========================================="
            echo "üîç [CHECK] Analyzing DAG trigger procedure results"

            error_found=0

            if [ ! -f "/workdir/triggering_dag_procedure.log" ]; then
              echo "‚úÖ [SUCCESS] No changes detected - DAG trigger log file does not exist. Proceeding successfully."
            else
              echo "‚öôÔ∏è [PROCESS] Checking trigger procedure log for errors"
              
              while read -r line; do
                if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                  echo "‚ùå [ERROR] Error found in E2E Test DBT Project DAG trigger procedure log: $line"
                  error_found=1
                fi
              done < /workdir/triggering_dag_procedure.log

              if [ $error_found -eq 1 ]; then
                echo "‚ùå [ERROR] E2E Test DBT Project DAG trigger procedure has failed. Please check the logs for details."
                exit 1
              else
                echo "‚úÖ [SUCCESS] E2E Test DBT Project DAG trigger procedure completed successfully"
              fi
            fi

            echo "=========================================="
            echo "üéâ [SUCCESS] DBT Project DAG E2E Data-Pipeline Trigger procedure completed"
            echo "=========================================="
        env:
          - name: DBT_TARGET
            value: "test"
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_BUCKET_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_BUCKET_NAME
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-model-e2e-dag-status-check
      inputs:
        parameters:
          - name: branch
          - name: dag_run_id
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_orchestrator_core_image_name}}:{{workflow.parameters.data_orchestrator_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash
            # Exit on error, treat unset variables as errors, and return value of pipeline is the status of the last command


            # ===========================================
            # E2E Data-Pipeline DAG Status Check Script
            # ===========================================
            # This script checks the status of E2E Data-Pipeline DAGs for the DBT Project
            # Used in Argo Workflow for E2E Test Run validation
            # ===========================================

            echo "üöÄ [INFO] Starting DBT Project DAG E2E Data-Pipeline Test procedure"
            cd /workdir/src

            # ===========================================
            # DAG Run ID Configuration
            # ===========================================
            echo "üìã [CONFIG] Configuring DAG run ID"

            # Check if dag_run_id environment variable is already set
            if [ -z "$dag_run_id" ]; then
              echo "‚öôÔ∏è [PROCESS] dag_run_id not set, reading from ../dag_run_id.env file"
              
              # Check if the file exists and read the value from the file
              if [ -f "../dag_run_id.env" ]; then
                export dag_run_id=$(grep 'dag_run_id=' ../dag_run_id.env | cut -d'=' -f2)
                echo "‚úÖ [SUCCESS] dag_run_id configured from file"
              else
                echo "‚ùå [ERROR] dag_run_id.env file not found!"
                exit 1
              fi
            else
              echo "‚úÖ [SUCCESS] dag_run_id already configured"
            fi

            echo "üîç [CHECK] DAG run ID validation complete"

            # ===========================================
            # DAG Status Check Functions
            # ===========================================

            # Function to check DAG status
            check_dag_status() {
                local platform="$1"
                local dag_id="$2"
                
                echo "üîç [CHECK] Checking status for DAG: $dag_id"
                
                if [ "${platform,,}" == "composer" ]; then
                    echo "‚öôÔ∏è [PROCESS] Platform: Composer"
                    echo "üîç [CHECK] Validating Composer environment variables"
                    
                    if [[ -z $AIRFLOW_URL || -z $PLATFORM || -z $project_id || -z $location || -z $composer_environment_name || -z $service_account_email ]]; then
                        echo "‚ùå [ERROR] One or more Composer variables are undefined"
                        return 1
                    else  
                        echo "‚úÖ [SUCCESS] All Composer variables are defined"
                        echo "‚öôÔ∏è [PROCESS] Getting status for Airflow DAG: $dag_id"
                        export dag_id="$dag_id"
                        python /usr/app/tsb-data-orchestrator-core/get_dag_status_airflow_bauth.op.py 2>&1 | tee -a /workdir/status_check_dag_procedure.log || true
                    fi 
                elif [ "${platform,,}" == "airflow" ]; then
                    echo "‚öôÔ∏è [PROCESS] Platform: Airflow"
                    echo "üîç [CHECK] Validating Airflow environment variables"
                    
                    if [[ -z $AIRFLOW_URL || -z $PLATFORM || -z $AIRFLOW_USER || -z $AIRFLOW_PASSWORD ]]; then
                        echo "‚ùå [ERROR] One or more Airflow variables are undefined"
                        return 1
                    else  
                        echo "‚úÖ [SUCCESS] All Airflow variables are defined"
                        echo "‚öôÔ∏è [PROCESS] Getting status for Airflow DAG: $dag_id"
                        export dag_id="$dag_id"
                        python /usr/app/tsb-data-orchestrator-core/get_dag_status_airflow_bauth.op.py 2>&1 | tee -a /workdir/status_check_dag_procedure.log || true
                    fi
                else
                    echo "‚ùå [ERROR] Unknown platform: $platform"
                    return 1
                fi
            }

            # Function to process DAGs for a project
            process_project_dags() {
                local project_name="$1"
                local platform="$2"
                
                echo "üìã [CONFIG] Reading DAG configurations from airflow_dags.yaml"
                
                # Get all DAG configurations for the current project
                local dag_configs=$(yq -r '.dags[] | select(.project == "'$project_name'") | .dag_id' /workdir/airflow_dags.yaml)
                
                if [ -z "$dag_configs" ]; then
                    echo "‚ùå [ERROR] No DAG configurations found for project $project_name in airflow_dags.yaml"
                    return 1
                fi
                
                echo "‚úÖ [SUCCESS] Found DAG configurations for project $project_name"
                
                # Process each DAG configuration
                while IFS= read -r dag_id; do
                    if [ -n "$dag_id" ]; then
                        echo "‚öôÔ∏è [PROCESS] Processing DAG: $dag_id"
                        check_dag_status "$platform" "$dag_id"
                    fi
                done <<< "$dag_configs"
            }

            # ===========================================
            # Main Processing Loop
            # ===========================================

            while read folder; do
              if [ -d "$folder" ]; then
                echo "=========================================="
                echo "üîç [CHECK] Processing Data-Orchestrator in directory: $folder"
                echo "üìã [CONFIG] Branch: {{inputs.parameters.branch}}"
                echo "=========================================="
                
                cd "$folder"
                
                # ===========================================
                # Data Warehouse Credentials Configuration
                # ===========================================
                echo "üìã [CONFIG] Configuring data warehouse credentials"
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "‚ùå [ERROR] Google BigQuery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "‚öôÔ∏è [PROCESS] Setting up Google Cloud authentication"
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                      gcloud auth activate-service-account --key-file /secrets/sa.json
                      gcloud config set project $GCP_PROJECT_NAME
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "‚úÖ [SUCCESS] Google Cloud authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "‚ùå [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "‚öôÔ∏è [PROCESS] Setting up Snowflake authentication"
                      mkdir -p /snowsql/secrets/
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8
                      chmod 600 /snowsql/secrets/rsa_key.p8
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "‚úÖ [SUCCESS] Snowflake authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "‚ùå [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] Redshift credentials validated"
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "‚ùå [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] Fabric credentials validated"
                  fi
                fi
                
                # ===========================================
                # Data-Orchestrator Environment Configuration
                # ===========================================
                echo "üîç [CHECK] Validating Data-Orchestrator environment variables"
                
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "‚ùå [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                else
                  echo "‚úÖ [SUCCESS] All Data-Orchestrator variables are defined"
                  echo "‚öôÔ∏è [PROCESS] Importing variables from Airflow Secret yml file"
                  export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" )
                  export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  echo "‚úÖ [SUCCESS] Platform and DBT project name imported"
                fi

                touch /workdir/status_check_dag_procedure.log

                # Process DAGs for the current project
                process_project_dags "$DBT_PROJECT_NAME" "$PLATFORM"
                
                cd ..
              else
                echo "‚ö†Ô∏è [WARNING] Working on root - skipping directory processing"
              fi
            done < /workdir/changed_folders.txt

            # ===========================================
            # Error Analysis and Final Status
            # ===========================================
            echo "=========================================="
            echo "üîç [CHECK] Analyzing E2E test results"
            echo "=========================================="

            error_found=0

            if [ ! -f "/workdir/status_check_dag_procedure.log" ]; then
                echo "‚úÖ [SUCCESS] No changes detected - status check log file does not exist"
                echo "‚úÖ [SUCCESS] E2E test procedure completed successfully"
            else
                echo "‚öôÔ∏è [PROCESS] Analyzing status check log for errors"
                
                while read -r line; do
                  if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                    echo "‚ùå [ERROR] Error found in E2E test procedure: $line"
                    error_found=1
                  fi
                done < /workdir/status_check_dag_procedure.log

                if [ $error_found -eq 1 ]; then
                  echo "‚ùå [ERROR] E2E test procedure failed. Please check the logs for details"
                  exit 1
                else
                  echo "‚úÖ [SUCCESS] E2E test procedure completed successfully"
                fi
            fi

            echo "=========================================="
            echo "üéâ [COMPLETE] DBT Project DAG E2E Data-Pipeline Test procedure finished"
            echo "=========================================="
        env:
          - name: DBT_TARGET
            value: "test"
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_BUCKET_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_BUCKET_NAME
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-model-e2e-data-tagging
      inputs:
        parameters:
          - name: branch
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.dbt_workflow_core_image_name}}:{{workflow.parameters.dbt_workflow_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "üöÄ [INFO] DBT Project DAG E2E BQ Dataset Labeling"
            echo "üìã [CONFIG] Branch: {{inputs.parameters.branch}}"
            echo "==========================================="

            echo "‚öôÔ∏è [PROCESS] Starting DBT Project DAG E2E BQ Dataset labeling procedure"
            cd /workdir/src

            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "üîç [CHECK] Processing directory: $folder"
                echo "üìã [CONFIG] Branch: {{inputs.parameters.branch}}"
                echo "==========================================="
                
                cd "$folder"
                
                # Set warehouse credentials
                echo "üîê [CONFIG] Configuring data warehouse credentials"
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "‚ùå [ERROR] Google BigQuery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "‚öôÔ∏è [PROCESS] Setting up Google Cloud authentication"
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json || { echo "‚ùå [ERROR] Failed to decode service account secret"; exit 1; }
                      gcloud auth activate-service-account --key-file /secrets/sa.json || { echo "‚ùå [ERROR] Failed to activate service account"; exit 1; }
                      gcloud config set project $GCP_PROJECT_NAME || { echo "‚ùå [ERROR] Failed to set GCP project"; exit 1; }
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "‚úÖ [SUCCESS] Google Cloud authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  echo "‚ö†Ô∏è [WARNING] Skipping tagging procedure - not required for Snowflake data warehouse"
                  exit 0
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "‚ùå [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "‚öôÔ∏è [PROCESS] Setting up Snowflake authentication"
                      mkdir -p /snowsql/secrets/ || { echo "‚ùå [ERROR] Failed to create Snowflake secrets directory"; exit 1; }
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8 || { echo "‚ùå [ERROR] Failed to write Snowflake private key"; exit 1; }
                      chmod 600 /snowsql/secrets/rsa_key.p8 || { echo "‚ùå [ERROR] Failed to set Snowflake key permissions"; exit 1; }
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "‚úÖ [SUCCESS] Snowflake authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  echo "‚ö†Ô∏è [WARNING] Skipping tagging procedure - not required for Redshift data warehouse"
                  exit 0
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "‚ùå [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  echo "‚ö†Ô∏è [WARNING] Skipping tagging procedure - not required for Fabric data warehouse"
                  exit 0
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "‚ùå [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  fi
                fi
                
                echo "üîç [CHECK] Validating Data-Orchestrator environment variables"
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "‚ùå [ERROR] Required environment variables are undefined"
                  exit 1
                else
                  echo "‚úÖ [SUCCESS] All required variables are defined"
                  echo "‚öôÔ∏è [PROCESS] Importing variables from Airflow Secret YAML file"
                  
                  export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" ) || { echo "‚ùå [ERROR] Failed to extract PLATFORM variable"; exit 1; }
                  export dag_id=$(grep 'DAG_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || { echo "‚ùå [ERROR] Failed to extract DAG_ID variable"; exit 1; }
                  export TARGET=$(grep 'TARGET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || { echo "‚ùå [ERROR] Failed to extract TARGET variable"; exit 1; }
                  export GIT_BRANCH=$(grep 'GIT_BRANCH:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || { echo "‚ùå [ERROR] Failed to extract GIT_BRANCH variable"; exit 1; }
                  export DBT_PROJECT_NAME=$(grep 'DBT_PROJECT_NAME:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'") || { echo "‚ùå [ERROR] Failed to extract DBT_PROJECT_NAME variable"; exit 1; }
                  export DATA_QUALITY_FORCE_E2E=$(grep 'DATA_QUALITY_FORCE_E2E:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" ) || { echo "‚ùå [ERROR] Failed to extract DATA_QUALITY_FORCE_E2E variable"; exit 1; }
                  
                  echo "‚úÖ [SUCCESS] Variables imported successfully"
                fi
                
                if [[ "${DATA_QUALITY_FORCE_E2E}" == "true" || "${DATA_QUALITY_FORCE_E2E}" == "True" || "${DATA_QUALITY_FORCE_E2E}" == "TRUE" ]]; then
                  echo "‚öôÔ∏è [PROCESS] E2E Data Quality force enabled - creating empty data quality schema"
                  dbt deps --quiet > /dev/null 2>&1 || { echo "‚ö†Ô∏è [WARNING] DBT deps command completed with warnings"; }
                  dbt run --profiles-dir . --select package:re_data --target ${TARGET} --empty --quiet > /dev/null 2>&1 || { echo "‚ö†Ô∏è [WARNING] DBT run command completed with warnings"; }
                  dbt clean --quiet > /dev/null 2>&1 || { echo "‚ö†Ô∏è [WARNING] DBT clean command completed with warnings"; }
                  echo "‚úÖ [SUCCESS] Empty data quality schema created"
                fi
                
                touch /workdir/create_dbt_bq_dataset_labels.log || { echo "‚ùå [ERROR] Failed to create log file"; exit 1; }
                
                echo "üîç [CHECK] Validating platform configuration"
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "üìã [CONFIG] Platform: Composer"
                  echo "üîç [CHECK] Validating Composer environment variables"
                  if [[ -z $AIRFLOW_URL || -z $PLATFORM || -z $project_id || -z $location || -z $composer_environment_name || -z $service_account_email || -z $dag_id ]]; then
                    echo "‚ùå [ERROR] One or more Composer variables are undefined"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] All Composer variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Executing DBT BigQuery dataset labeling for Composer"
                    bash /usr/app/dbt/dbt_bq_dataset_label_add.sh 2>&1 | tee -a /workdir/create_dbt_bq_dataset_labels.log || { echo "‚ö†Ô∏è [WARNING] Dataset labeling completed with warnings"; }
                  fi
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "üìã [CONFIG] Platform: Airflow"
                  echo "üîç [CHECK] Validating Airflow environment variables"
                  if [[ -z $AIRFLOW_URL || -z $PLATFORM || -z $AIRFLOW_USER || -z $AIRFLOW_PASSWORD || -z $dag_id ]]; then
                    echo "‚ùå [ERROR] One or more Airflow variables are undefined"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] All Airflow variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Executing DBT BigQuery dataset labeling for Airflow"
                    bash /usr/app/dbt/dbt_bq_dataset_label_add.sh 2>&1 | tee -a /workdir/create_dbt_bq_dataset_labels.log || { echo "‚ö†Ô∏è [WARNING] Dataset labeling completed with warnings"; }
                  fi
                else
                  echo "‚ùå [ERROR] Unknown platform: ${PLATFORM}"
                  exit 1
                fi
                
                cd ..
              else
                echo "‚ö†Ô∏è [WARNING] Working on root - skipping directory processing"
              fi
            done < /workdir/changed_folders.txt

            echo "==========================================="
            echo "üîç [CHECK] Analyzing dataset labeling results"
            echo "==========================================="

            error_found=0
            warning_count=0

            if [ ! -f "/workdir/create_dbt_bq_dataset_labels.log" ]; then
              echo "‚úÖ [SUCCESS] No changes detected - BQ dataset labeling log file does not exist. Proceeding successfully."
            else
              echo "‚öôÔ∏è [PROCESS] Processing dataset labeling log file"
              while IFS= read -r line || [[ -n "$line" ]]; do
                [[ -z "$line" ]] && continue
                
                if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                  ((warning_count++))
                  continue
                fi
                
                # Check for actual errors (after filtering warnings)
                if echo "$line" | grep -Ei "(fail|error|forbidden|denied|exception|timeout|connection refused|permission denied|authentication failed|quota exceeded)" > /dev/null; then
                  # Determine error type
                  if echo "$line" | grep -Ei "fail" > /dev/null; then
                    error_type="Failure"
                  elif echo "$line" | grep -Ei "error" > /dev/null; then
                    error_type="Error"
                  elif echo "$line" | grep -Ei "forbidden|denied|permission" > /dev/null; then
                    error_type="Permission"
                  elif echo "$line" | grep -Ei "timeout|connection" > /dev/null; then
                    error_type="Connection"
                  else
                    error_type="Error"
                  fi
                  
                  echo "‚ùå [$error_type] During BQ dataset labeling: $line"
                  error_found=1
                fi
              done < /workdir/create_dbt_bq_dataset_labels.log
              
              if [ $warning_count -gt 0 ]; then
                echo "‚ö†Ô∏è [WARNING] Found $warning_count warnings during processing"
              fi
              
              if [ $error_found -eq 1 ]; then
                echo "‚ùå [ERROR] E2E Test DBT Project DAG E2E BQ Dataset labeling procedure has failed. Please check the logs for details."
                exit 1
              else
                echo "‚úÖ [SUCCESS] DBT Project DAG E2E BQ Dataset labeling procedure completed successfully"
              fi
            fi

            echo "==========================================="
            echo "üéâ [SUCCESS] DBT Project DAG E2E BQ Dataset Labeling Complete"
            echo "==========================================="
        env:
          - name: DBT_TARGET
            value: "test"
          - name: DATASET_LABEL_KEY
            value: "fast-bi-e2e-environment"
          - name: DATASET_LABEL_VALUE
            value: "e2e-test"
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_BUCKET_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_BUCKET_NAME
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-model-e2e-data-cleanning
      inputs:
        parameters:
          - name: branch
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.dbt_workflow_core_image_name}}:{{workflow.parameters.dbt_workflow_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash


            echo "==========================================="
            echo "üöÄ [INFO] DBT Project DAG E2E Dataset Cleaning"
            echo "==========================================="
            echo "üìã [CONFIG] Starting cleanup procedure for branch: {{inputs.parameters.branch}}"
            echo ""

            cd /workdir/src

            while read folder; do
              if [ -d "$folder" ]; then
                echo "==========================================="
                echo "‚öôÔ∏è [PROCESS] Processing directory: $folder"
                echo "==========================================="
                
                cd "$folder"
                
                # ===========================================
                # üîê [SECURITY] Warehouse Credentials Setup
                # ===========================================
                echo "üîç [CHECK] Configuring warehouse credentials..."
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "bigquery" ]]; then
                  if [[ -z "$GCP_SA_SECRET" ]]; then
                    echo "‚ùå [ERROR] Google BigQuery credentials are missing: Service Account JSON not provided"
                    exit 1
                  else
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "üîê [CONFIG] Setting up Google Cloud authentication..."
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                      gcloud auth activate-service-account --key-file /secrets/sa.json
                      gcloud config set project $GCP_PROJECT_NAME
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                      echo "‚úÖ [SUCCESS] Google Cloud authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "snowflake" ]]; then
                  if [[ -z "$SNOWFLAKE_PRIVATE_KEY" || -z "$SNOWFLAKE_PASSPHRASE" ]]; then
                    echo "‚ùå [ERROR] Snowflake credentials are missing: Private Key and Passphrase not provided"
                    exit 1
                  else
                    if [[ -z "$SNOWSQL_PRIVATE_KEY_PASSPHRASE" ]]; then
                      echo "üîê [CONFIG] Setting up Snowflake authentication..."
                      mkdir -p /snowsql/secrets/
                      echo "$SNOWFLAKE_PRIVATE_KEY" > /snowsql/secrets/rsa_key.p8
                      chmod 600 /snowsql/secrets/rsa_key.p8
                      export SNOWSQL_PRIVATE_KEY_PASSPHRASE=$SNOWFLAKE_PASSPHRASE
                      echo "‚úÖ [SUCCESS] Snowflake authentication configured"
                    fi
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "redshift" ]]; then
                  if [[ -z "$REDSHIFT_PASSWORD" || -z "$REDSHIFT_USER" || -z "$REDSHIFT_HOST" || -z "$REDSHIFT_DATABASE" || -z "$REDSHIFT_PORT" ]]; then
                    echo "‚ùå [ERROR] Redshift credentials are missing: username and/or password not provided"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] Redshift credentials validated"
                  fi
                fi
                
                if [[ "${DATA_WAREHOUSE_PLATFORM,,}" == "fabric" ]]; then
                  if [[ -z "$FABRIC_SERVER" || -z "$FABRIC_DATABASE" || -z "$FABRIC_PORT" || -z "$FABRIC_USER" || -z "$FABRIC_PASSWORD" || -z "$FABRIC_AUTHENTICATION" ]]; then
                    echo "‚ùå [ERROR] Fabric credentials are missing: one or more required parameters not provided"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] Fabric credentials validated"
                  fi
                fi
                
                # ===========================================
                # üìã [CONFIG] Environment Variables Import
                # ===========================================
                echo "üîç [CHECK] Validating required Data-Orchestrator environment variables..."
                
                if [[ -z $AIRFLOW_SECRET_FILE_NAME ]]; then
                  echo "‚ùå [ERROR] AIRFLOW_SECRET_FILE_NAME is undefined"
                  exit 1
                else
                  echo "‚úÖ [SUCCESS] Environment variables file validated"
                  echo "üìã [CONFIG] Importing variables from Airflow Secret YAML file..."
                  
                  export PLATFORM=$(grep -E '^[[:space:]]*PLATFORM:[[:space:]]*' $AIRFLOW_SECRET_FILE_NAME | awk '{print $2}' | tr -d "'" )
                  export dag_id=$(grep 'DAG_ID:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export TARGET=$(grep 'TARGET:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export GIT_BRANCH=$(grep 'GIT_BRANCH:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  export SOURCE_E2E_TABLESAMPLE=$(grep 'SOURCE_E2E_TABLESAMPLE:' $AIRFLOW_SECRET_FILE_NAME | tail -n1 | awk '{print $2}' | tr -d "'" )
                  
                  echo "‚úÖ [SUCCESS] Environment variables imported successfully"
                fi

                touch /workdir/data_model_e2e_data_cleanning.log

                # ===========================================
                # üèóÔ∏è [PLATFORM] Platform-Specific Processing
                # ===========================================
                echo "üîç [CHECK] Detecting platform type..."
                
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "üìã [CONFIG] Platform: Google Cloud Composer"
                  echo "üîç [CHECK] Validating required Composer environment variables..."
                  
                  if [[ -z $AIRFLOW_URL || -z $PLATFORM || -z $project_id || -z $location || -z $composer_environment_name || -z $service_account_email || -z $dag_id ]]; then
                    echo "‚ùå [ERROR] One or more Composer environment variables are undefined"
                    exit 1
                  else  
                    echo "‚úÖ [SUCCESS] All Composer variables are defined"
                    
                    # ===========================================
                    # üßπ [CLEANUP] DBT Dataset Cleanup Process
                    # ===========================================
                    echo "‚öôÔ∏è [PROCESS] Starting DBT dataset cleanup process..."
                    
                    echo "üìã [CONFIG] Setting up macro directory..."
                    macros_path=$(yq '.["macro-paths"][] // "macros"' dbt_project.yml)
                    macros_path=$(echo "$macros_path" | xargs)
                    
                    if [ -d "$macros_path" ]; then
                      echo "‚úÖ [SUCCESS] Macro directory exists: $macros_path"
                    else
                      echo "üìã [CONFIG] Creating macro directory: $macros_path"
                      mkdir -p "$macros_path" || { echo "‚ùå [ERROR] Failed to create macro directory"; exit 1; }
                      
                      if [ -d "$macros_path" ]; then
                        echo "‚úÖ [SUCCESS] Macro directory created successfully"
                      else
                        echo "‚ùå [ERROR] Failed to create macro directory. Please check permissions"
                        exit 1
                      fi
                    fi
                    
                    echo "üìã [CONFIG] Copying cleanup macro to project..."
                    if [ -f "/usr/app/dbt/macros/cleanup_dbt_dataset.sql" ]; then
                      cp /usr/app/dbt/macros/cleanup_dbt_dataset.sql ./"$macros_path"
                      echo "‚úÖ [SUCCESS] Cleanup macro copied successfully"
                    else
                      echo "‚ùå [ERROR] Source cleanup macro file not found: /usr/app/dbt/macros/cleanup_dbt_dataset.sql"
                      exit 1
                    fi

                    echo "‚öôÔ∏è [PROCESS] Executing DBT cleanup operation..."
                    if [[ ! -z "${TARGET}" ]]; then
                      echo "üìã [CONFIG] Running warehouse cleanup with target: ${TARGET}"
                      dbt run-operation cleanup_dbt_dataset --args '{"dry_run": False}' --target="${TARGET}" 2>&1 | tee -a /workdir/data_model_e2e_data_cleanning.log || true
                    else
                      echo "üìã [CONFIG] Running warehouse cleanup in dry-run mode"
                      dbt run-operation cleanup_dbt_dataset --args '{"dry_run": True}'
                    fi

                    # ===========================================
                    # üß™ [SAMPLE] Sample Data Cleanup
                    # ===========================================
                    if [ -z "${SOURCE_E2E_TABLESAMPLE}" ] || { [ "${SOURCE_E2E_TABLESAMPLE}" != "false" ] && [ "${SOURCE_E2E_TABLESAMPLE}" != "False" ] && [ "${SOURCE_E2E_TABLESAMPLE}" != "FALSE" ]; }; then
                      echo "üìã [CONFIG] Condition evaluated to True value with SOURCE_E2E_TABLESAMPLE"
                      
                      if [ -f "/usr/app/dbt/macros/create_tablesample_source.sql" ]; then
                        cp /usr/app/dbt/macros/create_tablesample_source.sql ./"$macros_path"
                        echo "‚úÖ [SUCCESS] Sample cleanup macro copied successfully"
                        echo "‚öôÔ∏è [PROCESS] Executing sample cleanup operation..."
                        dbt run-operation delete_samples_for_dataset --target="${TARGET}" 2>&1 | tee -a /workdir/data_model_e2e_data_cleanning.log || true
                      else
                        echo "‚ùå [ERROR] Source sample cleanup macro file not found: /usr/app/dbt/macros/create_tablesample_source.sql"
                        exit 1
                      fi
                    else
                      echo "üìã [CONFIG] Sample cleanup skipped (SOURCE_E2E_TABLESAMPLE=false)"
                    fi
                    
                    echo "‚úÖ [SUCCESS] DBT cleanup process completed"
                  fi
                  
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "üìã [CONFIG] Platform: Apache Airflow"
                  echo "üîç [CHECK] Validating required Airflow environment variables..."
                  
                  if [[ -z $AIRFLOW_URL || -z $PLATFORM || -z $AIRFLOW_USER || -z $AIRFLOW_PASSWORD || -z $dag_id ]]; then
                    echo "‚ùå [ERROR] One or more Airflow environment variables are undefined"
                    exit 1
                  else  
                    echo "‚úÖ [SUCCESS] All Airflow variables are defined"
                    
                    # ===========================================
                    # üßπ [CLEANUP] DBT Dataset Cleanup Process
                    # ===========================================
                    echo "‚öôÔ∏è [PROCESS] Starting DBT dataset cleanup process..."
                    
                    echo "üìã [CONFIG] Setting up macro directory..."
                    macros_path=$(yq '.["macro-paths"][] // "macros"' dbt_project.yml)
                    macros_path=$(echo "$macros_path" | xargs)
                    
                    if [ -d "$macros_path" ]; then
                      echo "‚úÖ [SUCCESS] Macro directory exists: $macros_path"
                    else
                      echo "üìã [CONFIG] Creating macro directory: $macros_path"
                      mkdir -p "$macros_path" || { echo "‚ùå [ERROR] Failed to create macro directory"; exit 1; }
                      
                      if [ -d "$macros_path" ]; then
                        echo "‚úÖ [SUCCESS] Macro directory created successfully"
                      else
                        echo "‚ùå [ERROR] Failed to create macro directory. Please check permissions"
                        exit 1
                      fi
                    fi
                    
                    echo "üìã [CONFIG] Copying cleanup macro to project..."
                    if [ -f "/usr/app/dbt/macros/cleanup_dbt_dataset.sql" ]; then
                      cp /usr/app/dbt/macros/cleanup_dbt_dataset.sql ./"$macros_path"
                      echo "‚úÖ [SUCCESS] Cleanup macro copied successfully"
                    else
                      echo "‚ùå [ERROR] Source cleanup macro file not found: /usr/app/dbt/macros/cleanup_dbt_dataset.sql"
                      exit 1
                    fi

                    echo "‚öôÔ∏è [PROCESS] Executing DBT cleanup operation..."
                    if [[ ! -z "${TARGET}" ]]; then
                      echo "üìã [CONFIG] Running warehouse cleanup with target: ${TARGET}"
                      dbt run-operation cleanup_dbt_dataset --args '{"dry_run": False}' --target="${TARGET}" 2>&1 | tee -a /workdir/data_model_e2e_data_cleanning.log || true
                    else
                      echo "üìã [CONFIG] Running warehouse cleanup in dry-run mode"
                      dbt run-operation cleanup_dbt_dataset --args '{"dry_run": True}'
                    fi

                    # ===========================================
                    # üß™ [SAMPLE] Sample Data Cleanup
                    # ===========================================
                    if [ -z "${SOURCE_E2E_TABLESAMPLE}" ] || { [ "${SOURCE_E2E_TABLESAMPLE}" != "false" ] && [ "${SOURCE_E2E_TABLESAMPLE}" != "False" ] && [ "${SOURCE_E2E_TABLESAMPLE}" != "FALSE" ]; }; then
                      echo "üìã [CONFIG] Condition evaluated to True value with SOURCE_E2E_TABLESAMPLE"
                      
                      if [ -f "/usr/app/dbt/macros/create_tablesample_source.sql" ]; then
                        cp /usr/app/dbt/macros/create_tablesample_source.sql ./"$macros_path"
                        echo "‚úÖ [SUCCESS] Sample cleanup macro copied successfully"
                        echo "‚öôÔ∏è [PROCESS] Executing sample cleanup operation..."
                        dbt run-operation delete_samples_for_dataset --target="${TARGET}" 2>&1 | tee -a /workdir/data_model_e2e_data_cleanning.log || true
                      else
                        echo "‚ùå [ERROR] Source sample cleanup macro file not found: /usr/app/dbt/macros/create_tablesample_source.sql"
                        exit 1
                      fi
                    else
                      echo "üìã [CONFIG] Sample cleanup skipped (SOURCE_E2E_TABLESAMPLE=false)"
                    fi
                    
                    echo "‚úÖ [SUCCESS] DBT cleanup process completed"
                  fi
                  
                else
                  echo "‚ùå [ERROR] Unknown platform: ${PLATFORM}"
                  exit 1
                fi
                
                cd ..
              else
                echo "üìã [CONFIG] Working on root directory - skipping"
              fi
            done < /workdir/changed_folders.txt

            # ===========================================
            # üîç [VALIDATION] Log Analysis & Error Check
            # ===========================================
            echo "==========================================="
            echo "üîç [VALIDATION] Analyzing cleanup results..."
            echo "==========================================="

            error_found=0
            warning_count=0
            ignored_error_count=0

            if [ ! -f "/workdir/data_model_e2e_data_cleanning.log" ]; then
              echo "üìã [CONFIG] No changes detected - cleanup log file does not exist"
              echo "‚úÖ [SUCCESS] Proceeding successfully without cleanup operations"
            else
              echo "üìã [CONFIG] Analyzing cleanup log file for errors and warnings..."
              
              # Read all lines into an array for lookahead processing
              mapfile -t log_lines < /workdir/data_model_e2e_data_cleanning.log
              
              for i in "${!log_lines[@]}"; do
                line="${log_lines[$i]}"
                [[ -z "$line" ]] && continue

                if echo "$line" | grep -Ei "(warning|FutureWarning|DeprecationWarning|UserWarning|RuntimeWarning|\[WARNING\]|WARN:|WARNING:|/usr/local/lib/python.*/(site-packages|dist-packages))" > /dev/null; then
                  ((warning_count++))
                  continue
                fi

                # Check for errors, but ignore "Not found" errors that are expected during cleanup
                if echo "$line" | grep -Ei "fail|error|forbidden|denied" > /dev/null; then
                  # Check if this error line contains "Not found" patterns
                  if echo "$line" | grep -Ei "not found|was not found|does not exist" > /dev/null; then
                    ((ignored_error_count++))
                    echo "üìã [CONFIG] Ignoring expected error (resource already deleted/does not exist): $line"
                    continue
                  fi
                  
                  # Check if this is a "Database Error" line and if the next line contains "Not found"
                  if echo "$line" | grep -Ei "encountered an error while running operation: database error" > /dev/null; then
                    next_line_index=$((i + 1))
                    if [ $next_line_index -lt ${#log_lines[@]} ]; then
                      next_line="${log_lines[$next_line_index]}"
                      if echo "$next_line" | grep -Ei "not found|was not found|does not exist" > /dev/null; then
                        ((ignored_error_count++))
                        echo "üìã [CONFIG] Ignoring expected error (resource already deleted/does not exist): $line"
                        echo "üìã [CONFIG] Ignoring expected error (resource already deleted/does not exist): $next_line"
                        continue
                      fi
                    fi
                  fi
                  
                  echo "‚ùå [ERROR] Error detected in cleanup log: $line"
                  error_found=1
                fi
              done

              if [ $warning_count -gt 0 ]; then
                echo "‚ö†Ô∏è [WARNING] Found $warning_count warnings in cleanup log"
              fi

              if [ $error_found -eq 1 ]; then
                echo "‚ùå [ERROR] DBT dataset cleanup procedure failed"
                echo "üìã [CONFIG] Please check the logs for detailed error information"
                exit 1
              else
                echo "‚úÖ [SUCCESS] No errors found in cleanup log"
                if [ $ignored_error_count -gt 0 ]; then
                  echo "üìã [CONFIG] Summary: Processed log file with $warning_count warnings, $ignored_error_count ignored errors, and no actual errors"
                fi
              fi
            fi

            echo "==========================================="
            echo "‚úÖ [SUCCESS] DBT Project DAG E2E Dataset Cleaning Completed"
            echo "==========================================="
        env:
          - name: DBT_TARGET
            value: "test"
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_BUCKET_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_BUCKET_NAME
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: data-model-e2e-data-orchestration-cleanup
      inputs:
        parameters:
          - name: branch
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: '{{workflow.parameters.docker_image_repository_hub}}/{{workflow.parameters.docker_image_repository}}/{{workflow.parameters.data_orchestrator_core_image_name}}:{{workflow.parameters.data_orchestrator_core_image_tag}}'
        command:
          - bash
          - '-c'
        args:
          - |
            #!/bin/bash
            # Exit on error, treat unset variables as errors, and return value of pipeline is the status of the last command


            echo "==========================================="
            echo "üöÄ [INFO] DBT Project DAG E2E Data-Orchestrator Cleanup"
            echo "==========================================="

            cd /workdir/src || { echo "‚ùå [ERROR] Failed to change to /workdir/src"; exit 1; }

            echo "‚öôÔ∏è [PROCESS] Starting cleanup procedures..."

            # ===========================================
            # MANIFEST CLEANUP SECTION
            # ===========================================
            echo "==========================================="
            echo "üìã [CONFIG] Processing Manifest Files"
            echo "==========================================="

            if [[ -f "/workdir/compile_manifest_path.log" && -s "/workdir/compile_manifest_path.log" ]]; then
              echo "‚öôÔ∏è [PROCESS] Processing manifest files..."
              for n in $(cat /workdir/compile_manifest_path.log); do
                IFS="=" read -r platform manifest_path <<< "$n"
                ## TODO: Temporary set - UpperCase ${PLATFORM,,}
                echo "üîç [CHECK] Processing platform: ${PLATFORM,,}"
                
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "üìã [CONFIG] Platform: Composer"
                  if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                    echo "‚öôÔ∏è [PROCESS] Setting up GCP credentials..."
                    echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                    gcloud auth activate-service-account --key-file /secrets/sa.json
                    gcloud config set project $GCP_PROJECT_NAME
                    export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                  fi
                  
                  echo "üîç [CHECK] Validating required environment variables..."
                  if [[ -z $GCP_BUCKET_NAME ]]; then
                    echo "‚ùå [ERROR] Required variables are undefined"
                    exit 1
                  else  
                    echo "‚úÖ [SUCCESS] All variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Deleting manifest file from Composer GCS Bucket"
                    gsutil rm -f ${manifest_path} || { echo "‚ùå [ERROR] Failed to delete manifest file"; exit 1; }
                    echo "‚úÖ [SUCCESS] Manifest file deletion completed"
                  fi
                  
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "üìã [CONFIG] Platform: Airflow"
                  echo "üîç [CHECK] Validating required environment variables..."
                  if [[ -z ${GIT_USER_EMAIL} || -z ${GIT_USER_NAME} || -z ${DAG_REPO_PUSH} ]]; then
                    echo "‚ùå [ERROR] Required variables are undefined"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] All variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Setting up git configuration..."
                    git config --global user.email "${GIT_USER_EMAIL}"
                    git config --global user.name "${GIT_USER_NAME}"
                    
                    REPO_DIR="/workdir/airflow-dags"
                    if [ ! -d "$REPO_DIR" ]; then
                      echo "üìã [CONFIG] Repository directory does not exist - creating"
                    else
                      echo "‚öôÔ∏è [PROCESS] Removing existing repository directory"
                      rm -fr $REPO_DIR
                    fi
                    
                    echo "‚öôÔ∏è [PROCESS] Cloning Airflow dags repository..."
                    git clone $DAG_REPO_PUSH $REPO_DIR || { echo "‚ùå [ERROR] Failed to clone repository"; exit 1; }
                    
                    echo "‚öôÔ∏è [PROCESS] Deleting manifest file from Airflow repo"
                    rm -rf ${manifest_path}
                    echo "‚úÖ [SUCCESS] Manifest file deletion completed"
                    
                    echo "‚öôÔ∏è [PROCESS] Committing and pushing changes..."
                    git -C "$REPO_DIR" add -A
                    git -C "$REPO_DIR" commit -am"Delete DBT Project Manifest file."
                    git -C "$REPO_DIR" remote set-url --push origin "${DAG_REPO_PUSH}" 
                    git -C "$REPO_DIR" push || true
                    sleep 5
                    echo "‚úÖ [SUCCESS] Changes committed and pushed"
                  fi
                else
                  echo "‚ùå [ERROR] Unknown platform: ${PLATFORM,,}"
                  exit 1
                fi
              done
            else
              echo "‚ö†Ô∏è [WARNING] Manifest cleanup log not found or empty - skipping"
            fi

            # ===========================================
            # BASH OPERATOR CLEANUP SECTION
            # ===========================================
            echo "==========================================="
            echo "üìã [CONFIG] Processing Bash Operator Files"
            echo "==========================================="

            if [[ -f "/workdir/bash_operator_files_path.log" ]]; then
              if [[ -s "/workdir/bash_operator_files_path.log" ]]; then
                echo "‚öôÔ∏è [PROCESS] Processing bash operator files..."
                while IFS= read -r n || [[ -n "$n" ]]; do
                  if [[ -z "$n" ]]; then
                    continue
                  fi

                  IFS="=" read -r operator file_path <<< "$n"
                  
                  if [[ -z "$operator" || -z "$file_path" ]]; then
                    echo "‚ö†Ô∏è [WARNING] Invalid line format in bash operator log"
                    continue
                  fi

                  echo "‚öôÔ∏è [PROCESS] Processing deletion for operator: ${operator}"
                  
                  if [[ "${PLATFORM,,}" == "composer" ]]; then
                    echo "üìã [CONFIG] Platform: Composer"
                    if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                      echo "‚öôÔ∏è [PROCESS] Setting up GCP credentials..."
                      echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                      gcloud auth activate-service-account --key-file /secrets/sa.json
                      gcloud config set project $GCP_PROJECT_NAME
                      export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                    fi
                    
                    if [[ -z $GCP_BUCKET_NAME ]]; then
                      echo "‚ùå [ERROR] GCP_BUCKET_NAME is undefined"
                      exit 1
                    else
                      echo "‚úÖ [SUCCESS] All required variables are defined"
                      echo "‚öôÔ∏è [PROCESS] Deleting files from Composer GCS Bucket"
                      if gsutil rm -rf "${file_path}" 2>/dev/null; then
                        echo "‚úÖ [SUCCESS] Deletion completed"
                      else
                        echo "‚ö†Ô∏è [WARNING] Failed to delete or path not found"
                      fi
                    fi
                    
                  elif [[ "${PLATFORM,,}" == "airflow" ]]; then
                    echo "üìã [CONFIG] Platform: Airflow"
                    if [[ -z ${GIT_USER_EMAIL} || -z ${GIT_USER_NAME} || -z ${DAG_REPO_PUSH} ]]; then
                      echo "‚ùå [ERROR] Required variables are undefined"
                      exit 1
                    else
                      echo "‚úÖ [SUCCESS] All required variables are defined"
                      
                      echo "‚öôÔ∏è [PROCESS] Configuring git..."
                      git config --global user.email "${GIT_USER_EMAIL}"
                      git config --global user.name "${GIT_USER_NAME}"
                      
                      REPO_DIR="/workdir/airflow-dags/"
                      if [[ -d "$REPO_DIR" ]]; then
                        echo "‚öôÔ∏è [PROCESS] Removing existing repository directory..."
                        rm -rf "$REPO_DIR"
                      fi
                      
                      echo "‚öôÔ∏è [PROCESS] Cloning Airflow dags repository..."
                      if ! git clone "$DAG_REPO_PUSH" "$REPO_DIR"; then
                        echo "‚ùå [ERROR] Failed to clone repository"
                        exit 1
                      fi
                      
                      echo "‚öôÔ∏è [PROCESS] Deleting files from Airflow repo"
                      if [[ -e "${file_path}" ]]; then
                        rm -rf "${file_path}"
                        echo "‚úÖ [SUCCESS] Deletion completed"
                        
                        echo "‚öôÔ∏è [PROCESS] Committing and pushing changes..."
                        git -C "$REPO_DIR" add -A
                        if git -C "$REPO_DIR" diff --cached --quiet; then
                          echo "üìã [CONFIG] No changes to commit"
                        else
                          git -C "$REPO_DIR" commit -am "Delete DBT Project files for bash operator"
                          git -C "$REPO_DIR" remote set-url --push origin "${DAG_REPO_PUSH}"
                          git -C "$REPO_DIR" push || true
                          sleep 5
                          echo "‚úÖ [SUCCESS] Changes committed and pushed"
                        fi
                      else
                        echo "‚ö†Ô∏è [WARNING] Path not found: ${file_path}"
                      fi
                    fi
                    
                  else
                    echo "‚ùå [ERROR] Unknown platform"
                    exit 1
                  fi
                done < "/workdir/bash_operator_files_path.log"
              else
                echo "‚ö†Ô∏è [WARNING] Bash operator log file is empty - nothing to delete"
              fi
            else
              echo "‚ö†Ô∏è [WARNING] Bash operator log file not found - nothing to delete"
            fi

            # ===========================================
            # DAG CLEANUP SECTION
            # ===========================================
            echo "==========================================="
            echo "üìã [CONFIG] Processing DAG Files"
            echo "==========================================="

            echo "‚öôÔ∏è [PROCESS] Starting DAGs deletion procedure..."

            if [[ -f "/workdir/compile_dag_path.log" && -s "/workdir/compile_dag_path.log" ]]; then
              echo "‚öôÔ∏è [PROCESS] Processing DAG files..."
              for n in $(cat /workdir/compile_dag_path.log); do
                IFS="=" read -r platform dag_path <<< "$n"
                echo "üîç [CHECK] Processing platform: ${PLATFORM,,}"
                
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "üìã [CONFIG] Platform: Composer"
                  if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                    echo "‚öôÔ∏è [PROCESS] Setting up GCP credentials..."
                    echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                    gcloud auth activate-service-account --key-file /secrets/sa.json
                    gcloud config set project $GCP_PROJECT_NAME
                    export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                  fi
                  
                  echo "üîç [CHECK] Validating required environment variables..."
                  if [[ -z $GCP_BUCKET_NAME ]]; then
                    echo "‚ùå [ERROR] Required variables are undefined"
                    exit 1
                  else  
                    echo "‚úÖ [SUCCESS] All variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Deleting DAG file from Composer GCS Bucket"
                    gsutil rm -f ${dag_path} || { echo "‚ùå [ERROR] Failed to delete DAG file"; exit 1; }
                    echo "‚úÖ [SUCCESS] DAG file deletion completed"
                  fi
                  
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "üìã [CONFIG] Platform: Airflow"
                  echo "üîç [CHECK] Validating required environment variables..."
                  if [[ -z ${GIT_USER_EMAIL} || -z ${GIT_USER_NAME} || -z ${DAG_REPO_PUSH} ]]; then
                    echo "‚ùå [ERROR] Required variables are undefined"
                    exit 1
                  else
                    echo "‚úÖ [SUCCESS] All variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Setting up git configuration..."
                    git config --global user.email "${GIT_USER_EMAIL}"
                    git config --global user.name "${GIT_USER_NAME}"
                    
                    REPO_DIR="/workdir/airflow-dags"
                    if [ ! -d "$REPO_DIR" ]; then
                      echo "üìã [CONFIG] Repository directory does not exist - creating"
                    else
                      echo "‚öôÔ∏è [PROCESS] Removing existing repository directory"
                      rm -fr $REPO_DIR
                    fi
                    
                    echo "‚öôÔ∏è [PROCESS] Cloning Airflow dags repository..."
                    git clone $DAG_REPO_PUSH $REPO_DIR || { echo "‚ùå [ERROR] Failed to clone repository"; exit 1; }
                    
                    echo "‚öôÔ∏è [PROCESS] Deleting DAG file from Airflow repo"
                    rm -rf ${dag_path}
                    echo "‚úÖ [SUCCESS] DAG file deletion completed"
                    
                    echo "‚öôÔ∏è [PROCESS] Committing and pushing changes..."
                    git -C "$REPO_DIR" add -A
                    git -C "$REPO_DIR" commit -am"Delete DBT Project DAG file."
                    git -C "$REPO_DIR" remote set-url --push origin "${DAG_REPO_PUSH}" 
                    git -C "$REPO_DIR" push || true
                    sleep 5
                    echo "‚úÖ [SUCCESS] Changes committed and pushed"
                  fi
                else
                  echo "‚ùå [ERROR] Unknown platform: ${PLATFORM,,}"
                  exit 1
                fi
              done
            else
              echo "‚ö†Ô∏è [WARNING] DAG cleanup log not found or empty - skipping"
            fi

            # ===========================================
            # DBT API SERVER CLEANUP SECTION
            # ===========================================
            echo "==========================================="
            echo "üìã [CONFIG] Processing DBT API Server"
            echo "==========================================="

            if [ -f "/workdir/deploy_dbt_api_parameters.log" ]; then
              echo "‚öôÔ∏è [PROCESS] Processing DBT API parameters..."
              while IFS='=' read -r key value; do
                case "$key" in
                "DAG_OPERATOR")
                  export DAG_OPERATOR="$value"
                  echo "üìã [CONFIG] DAG_OPERATOR: $DAG_OPERATOR"
                  if [[ "$DAG_OPERATOR" == "api" ]]; then
                    echo "‚úÖ [SUCCESS] DBT API Server is enabled"
                  else
                    echo "üìã [CONFIG] DBT API Server is disabled"
                  fi
                  ;;
                "DBT_WORKLOAD_API_RELEASE")
                  export DBT_WORKLOAD_API_RELEASE="$value"
                  ;;
                "DBT_WORKLOAD_API_CONNECTION_ID")
                  export DBT_WORKLOAD_API_CONNECTION_ID="$value"
                  ;;
                esac
              done < /workdir/deploy_dbt_api_parameters.log
              
              if [[ "$DAG_OPERATOR" == "api" ]]; then
                echo "‚öôÔ∏è [PROCESS] Deleting DBT API service..."
                /bin/bash -c "/usr/app/tsb-data-orchestrator-core/delete_dbt_api_service.sh" || { echo "‚ùå [ERROR] Failed to delete DBT API service"; exit 1; }
                echo "‚úÖ [SUCCESS] DBT API service deletion completed"
              fi
            else
              echo "‚ö†Ô∏è [WARNING] DBT API parameters log not found - skipping cleanup"
            fi

            # ===========================================
            # AIRFLOW VARIABLES CLEANUP SECTION
            # ===========================================
            echo "==========================================="
            echo "üìã [CONFIG] Processing Airflow Variables"
            echo "==========================================="

            echo "‚öôÔ∏è [PROCESS] Starting Airflow variables deletion procedure..."

            if [ -f "/workdir/airflow_secrets.yaml" ]; then
              echo "‚úÖ [SUCCESS] Airflow secrets configuration found"
              
              echo "‚öôÔ∏è [PROCESS] Extracting secret values from airflow_secrets.yaml"
              mapfile -t secret_values < <(yq -r ".secrets.platform.\"${PLATFORM,,}\"[]" /workdir/airflow_secrets.yaml)
              echo "üìã [CONFIG] Found ${#secret_values[@]} secret values to process for platform: ${PLATFORM,,}"
              
              for secret_value in "${secret_values[@]}"; do
                if [ -z "$secret_value" ]; then
                  echo "‚ö†Ô∏è [WARNING] Skipping empty secret value"
                  continue
                fi
                
                echo "‚öôÔ∏è [PROCESS] Processing secret value"
                export variable_secret_key="$secret_value"
                
                echo "üîç [CHECK] Validating environment variables..."
                
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "üìã [CONFIG] Platform: Composer"
                  if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                    echo "‚öôÔ∏è [PROCESS] Setting up GCP credentials..."
                    echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                    gcloud auth activate-service-account --key-file /secrets/sa.json
                    gcloud config set project $GCP_PROJECT_NAME
                    export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                  fi
                  
                  echo "üîç [CHECK] Validating required Composer environment variables..."
                  if [[ -z $AIRFLOW_URL || -z $AIRFLOW_SECRET_FILE_NAME || -z $PLATFORM || -z $project_id || -z $location || -z $composer_environment_name || -z $service_account_email ]]; then
                    echo "‚ùå [ERROR] Required variables are undefined"
                    exit 1
                  else  
                    echo "‚úÖ [SUCCESS] All variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Deleting secrets from Airflow/Composer vault"
                    python /usr/app/tsb-data-orchestrator-core/delete_variables_airflow_bauth.op.py || { echo "‚ùå [ERROR] Failed to delete variables"; exit 1; }
                    echo "‚úÖ [SUCCESS] Variables deletion completed"
                  fi 
                  
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "üìã [CONFIG] Platform: Airflow"
                  echo "üîç [CHECK] Validating required Airflow environment variables..."
                  if [[ -z $AIRFLOW_URL || -z $AIRFLOW_SECRET_FILE_NAME || -z $PLATFORM || -z $AIRFLOW_USER || -z $AIRFLOW_PASSWORD ]]; then
                    echo "‚ùå [ERROR] Required variables are undefined"
                    exit 1
                  else  
                    echo "‚úÖ [SUCCESS] All variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Deleting secrets from Airflow vault"
                    python /usr/app/tsb-data-orchestrator-core/delete_variables_airflow_bauth.op.py || { echo "‚ùå [ERROR] Failed to delete variables"; exit 1; }
                    echo "‚úÖ [SUCCESS] Variables deletion completed"
                  fi
                else
                  echo "‚ùå [ERROR] Unknown platform: ${PLATFORM,,}"
                  exit 1
                fi
              done
            else
              echo "‚ö†Ô∏è [WARNING] Airflow secrets configuration not found"
            fi

            # ===========================================
            # AIRFLOW DAGS CLEANUP SECTION
            # ===========================================
            echo "==========================================="
            echo "üìã [CONFIG] Processing Airflow DAGs"
            echo "==========================================="

            echo "‚öôÔ∏è [PROCESS] Starting DAG deletion procedure..."

            if [ -f "/workdir/airflow_dags.yaml" ]; then
              echo "‚úÖ [SUCCESS] Airflow DAGs configuration found"
              
              echo "‚öôÔ∏è [PROCESS] Extracting DAG IDs from airflow_dags.yaml"
              mapfile -t dag_ids < <(yq -r '.dags[].dag_id' /workdir/airflow_dags.yaml)
              echo "üìã [CONFIG] Found ${#dag_ids[@]} DAG IDs to process"
              
              for dag_id in "${dag_ids[@]}"; do
                if [ -z "$dag_id" ]; then
                  echo "‚ö†Ô∏è [WARNING] Skipping empty DAG ID"
                  continue
                fi
                
                echo "‚öôÔ∏è [PROCESS] Processing DAG ID: $dag_id"
                export dag_id="$dag_id"
                
                echo "üîç [CHECK] Validating environment variables..."
                
                if [ "${PLATFORM,,}" == "composer" ]; then
                  echo "üìã [CONFIG] Platform: Composer"
                  if [[ -z "$GOOGLE_APPLICATION_CREDENTIALS" ]]; then
                    echo "‚öôÔ∏è [PROCESS] Setting up GCP credentials..."
                    echo "$GCP_SA_SECRET" | base64 --decode > /secrets/sa.json
                    gcloud auth activate-service-account --key-file /secrets/sa.json
                    gcloud config set project $GCP_PROJECT_NAME
                    export GOOGLE_APPLICATION_CREDENTIALS="/secrets/sa.json"
                  fi
                  
                  echo "üîç [CHECK] Validating required Composer environment variables..."
                  if [[ -z $AIRFLOW_URL || -z $AIRFLOW_SECRET_FILE_NAME || -z $PLATFORM || -z $project_id || -z $location || -z $composer_environment_name || -z $service_account_email ]]; then
                    echo "‚ùå [ERROR] Required variables are undefined"
                    exit 1
                  else  
                    echo "‚úÖ [SUCCESS] All variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Deleting DAG from Airflow/Composer"
                    python /usr/app/tsb-data-orchestrator-core/delete_dag_airflow_bauth.op.py || { echo "‚ùå [ERROR] Failed to delete DAG"; exit 1; }
                    echo "‚úÖ [SUCCESS] DAG deletion completed"
                  fi 
                  
                elif [ "${PLATFORM,,}" == "airflow" ]; then
                  echo "üìã [CONFIG] Platform: Airflow"
                  echo "üîç [CHECK] Validating required Airflow environment variables..."
                  if [[ -z $AIRFLOW_URL || -z $AIRFLOW_SECRET_FILE_NAME || -z $PLATFORM || -z $AIRFLOW_USER || -z $AIRFLOW_PASSWORD ]]; then
                    echo "‚ùå [ERROR] Required variables are undefined"
                    exit 1
                  else  
                    echo "‚úÖ [SUCCESS] All variables are defined"
                    echo "‚öôÔ∏è [PROCESS] Deleting DAG from Airflow"
                    python /usr/app/tsb-data-orchestrator-core/delete_dag_airflow_bauth.op.py || { echo "‚ùå [ERROR] Failed to delete DAG"; exit 1; }
                    echo "‚úÖ [SUCCESS] DAG deletion completed"
                  fi
                else
                  echo "‚ùå [ERROR] Unknown platform: ${PLATFORM,,}"
                  exit 1
                fi
              done
            else
              echo "‚ö†Ô∏è [WARNING] Airflow DAGs configuration not found"
            fi

            # ===========================================
            # CLEANUP SECTION
            # ===========================================
            echo "==========================================="
            echo "üìã [CONFIG] Final Cleanup"
            echo "==========================================="

            if [ -f "/workdir/airflow_secrets.yaml" ]; then
              echo "‚öôÔ∏è [PROCESS] Cleaning up airflow_secrets.yaml"
              rm -f /workdir/airflow_secrets.yaml
              echo "‚úÖ [SUCCESS] airflow_secrets.yaml removed"
            fi

            if [ -f "/workdir/airflow_dags.yaml" ]; then
              echo "‚öôÔ∏è [PROCESS] Cleaning up airflow_dags.yaml"
              rm -f /workdir/airflow_dags.yaml
              echo "‚úÖ [SUCCESS] airflow_dags.yaml removed"
            fi

            echo "==========================================="
            echo "‚úÖ [SUCCESS] DBT Project DAG E2E Data-Orchestrator cleanup completed"
            echo "==========================================="
        env:
          - name: DBT_API_SERVER_CONTROLLER_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DBT_API_SERVER_CONTROLLER_SECRET
          - name: DBT_TARGET
            value: "test"
          - name: PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: PLATFORM
          - name: DATA_WAREHOUSE_PLATFORM
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DATA_WAREHOUSE_PLATFORM
          - name: AIRFLOW_SECRET_FILE_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_SECRET_FILE_NAME
          - name: DAG_REPO_PUSH
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: DAG_REPO_PUSH
          - name: GIT_USER_EMAIL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GIT_USER_EMAIL
          - name: GIT_USER_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: CUSTOMER
          - name: AIRFLOW_URL
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_URL
          - name: AIRFLOW_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_USER
          - name: AIRFLOW_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: AIRFLOW_PASSWORD
          - name: GCP_PROJECT_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_PROJECT_NAME
                optional: true
          - name: GCP_SA_SECRET
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_SA_SECRET
                optional: true
          - name: GCP_BUCKET_NAME
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: GCP_BUCKET_NAME
                optional: true
          # Redshift secrets
          - name: REDSHIFT_HOST
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_HOST
                optional: true
          - name: REDSHIFT_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_DATABASE
                optional: true
          - name: REDSHIFT_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PORT
                optional: true
          - name: REDSHIFT_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_USER
                optional: true
          - name: REDSHIFT_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: REDSHIFT_PASSWORD
                optional: true
          # Snowflake secrets
          - name: SNOWFLAKE_PRIVATE_KEY
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PRIVATE_KEY
                optional: true
          - name: SNOWFLAKE_PASSPHRASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: SNOWFLAKE_PASSPHRASE
                optional: true
          # Fabric secrets
          - name: FABRIC_SERVER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_SERVER
                optional: true
          - name: FABRIC_DATABASE
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_DATABASE
                optional: true
          - name: FABRIC_PORT
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PORT
                optional: true
          - name: FABRIC_USER
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_USER
                optional: true
          - name: FABRIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_PASSWORD
                optional: true
          - name: FABRIC_AUTHENTICATION
            valueFrom:
              secretKeyRef:
                name: fastbi-argo-workflows-secrets
                key: FABRIC_AUTHENTICATION
                optional: true
        resources: {}
        volumeMounts:
          - name: workdir
            mountPath: /workdir
          - name: gcp-sa-secret-file
            mountPath: /secrets
    - name: k8s-pvc-cleanup
      inputs: {}
      outputs: {}
      metadata: {}
      container:
        name: ''
        image: bitnami/kubectl
        command:
          - sh
          - '-c'
        args:
          - |
            #!/bin/bash

            # ===========================================
            # Kubernetes PVC Cleanup Script
            # ===========================================
            # Purpose: Clean up Persistent Volume Claims
            #          for Argo Workflows in cicd-workflows namespace
            # ===========================================



            echo "üöÄ [INFO] Starting Kubernetes PVC cleanup process"
            echo "=========================================="

            # Configuration
            PVC_NAME="{{workflow.name}}-workdir"
            NAMESPACE="cicd-workflows"

            echo "üìã [CONFIG] PVC Name: $PVC_NAME"
            echo "üìã [CONFIG] Target Namespace: $NAMESPACE"
            echo ""

            echo "‚öôÔ∏è [PROCESS] Attempting to delete PVC $PVC_NAME in namespace $NAMESPACE..."

            # Execute the PVC deletion command (keeping exact same logic)
            kubectl delete pvc $PVC_NAME --namespace $NAMESPACE --ignore-not-found 2>/dev/null

            # Check the result and provide appropriate feedback
            if [ $? -eq 0 ]; then
              echo "‚úÖ [SUCCESS] Successfully deleted PVC $PVC_NAME or it did not exist"
            else
              echo "‚ùå [ERROR] Failed to delete PVC $PVC_NAME. Check logs for details"
              exit 1
            fi

            echo ""
            echo "‚úÖ [SUCCESS] Kubernetes PVC cleanup process completed"
            echo "=========================================="
        resources: {}
    - name: check-task-status
      inputs:
        parameters:
          - name: workflow_status
          - name: argo_main_address
      script:
        image: alpine
        command:
          - sh
          - '-c'
        args:
          - |
            #!/bin/sh

            # ===========================================
            # Argo Workflow Task Status Checker
            # ===========================================



            echo "üöÄ [INFO] Starting Argo Workflow Task Status Check"
            echo "=========================================="

            # ===========================================
            # Dependencies Installation
            # ===========================================

            echo "‚öôÔ∏è [PROCESS] Installing required dependencies..."
            apk add --no-cache jq >/dev/null 2>&1 || { 
              echo "‚ùå [ERROR] Failed to install jq dependency"
              exit 1
            }
            echo "‚úÖ [SUCCESS] Dependencies installed successfully"

            # ===========================================
            # Workflow Status Processing
            # ===========================================

            echo "üîç [CHECK] Parsing workflow failure information..."
            workflow_status_output='{{inputs.parameters.workflow_status}}'

            # Validate workflow status input
            if [ -z "$workflow_status_output" ] || [ "$workflow_status_output" = "null" ]; then
              echo "‚úÖ [SUCCESS] No failure information available - All tasks completed successfully"
              echo "=========================================="
              echo "üéâ [COMPLETE] Task status check completed successfully"
              exit 0
            fi

            # ===========================================
            # Argo Workflow URL Generation
            # ===========================================

            echo "üìã [CONFIG] Generating Argo Workflow URL..."
            ARGO_URL=$(echo "{{inputs.parameters.argo_main_address}}" | sed 's/:443//')
            echo "üîó [INFO] Argo Workflow URL:"
            echo "https://${ARGO_URL}/workflows/cicd-workflows/{{workflow.name}}?tab=workflow&uid={{workflow.uid}}"

            # ===========================================
            # Failure Analysis
            # ===========================================

            echo "üîç [CHECK] Analyzing task execution results..."
            failed_tasks=$(echo "$workflow_status_output" | jq -r 'fromjson | .[] | select(.phase == "Failed") | "Task: \(.displayName)\nStatus: \(.phase)\nMessage: \(.message)\nFinished at: \(.finishedAt)\n"')

            if [ -n "$failed_tasks" ]; then
              echo "‚ùå [ERROR] Failed tasks detected:"
              echo "------------------------------------------"
              echo "$failed_tasks"
              echo "------------------------------------------"
              
              failure_count=$(echo "$workflow_status_output" | jq 'fromjson | [ .[] | select(.phase == "Failed") ] | length')
              echo "‚ùå [ERROR] Pipeline failed due to $failure_count failure(s)"
              echo "=========================================="
              echo "üí• [FAILED] Task status check completed with failures"
              exit 1
            else
              echo "‚úÖ [SUCCESS] No failures detected - All tasks completed successfully"
              echo "=========================================="
              echo "üéâ [COMPLETE] Task status check completed successfully"
              exit 0
            fi
        resources: {}
  entrypoint: main
  arguments:
    parameters:
      - name: storage_class
        value: "standard"
      - name: docker_image_repository_hub
        value: docker.io
      - name: docker_image_repository
        value: 4fastbi
#fast.bi CICD Pipeline Images
      - name: data_analysis_import_core_image_name
        value: data-analysis-import-core
      - name: data_analysis_import_core_image_tag
        value: v0.1.0
      - name: data_orchestrator_core_image_name
        value: data-orchestrator-core
      - name: data_orchestrator_core_image_tag
        value: v0.1.0
      - name: dbt_workflow_core_image_name
        value: dbt-workflow-core
      - name: dbt_workflow_core_image_tag
        value: v0.1.0
      - name: repo_url
      - name: revision
      - name: branch
      - name: commit_short_sha
      - name: git_username
      - name: git_token
      - name: target_branch
      - name: ci_job_id
  volumes:
    - name: fastbi-argo-workflows-secrets
      secret:
        secretName: fastbi-argo-workflows-secrets
    - name: gcp-sa-secret-file
      emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: workdir
        creationTimestamp: null
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
        storageClassName: "{{workflow.parameters.storage_class}}"
      status: {}
  onExit: exit-handler
  ttlStrategy:
    secondsAfterCompletion: 86400
  activeDeadlineSeconds: 7200
  podGC:
    strategy: OnPodCompletion
  podSpecPatch: |
    containers:
      - name: main
        imagePullPolicy: Always
        env:
          - name: PYTHONWARNINGS
            value: "ignore"
